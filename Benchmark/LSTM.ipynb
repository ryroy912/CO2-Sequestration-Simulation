{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --------------------- Imports ---------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from google.colab import drive\n",
        "\n",
        "# --------------------- Matplotlib Setup ---------------------\n",
        "mpl.rcParams.update({\n",
        "    'font.size': 14,\n",
        "    'axes.titlesize': 15,\n",
        "    'axes.labelsize': 12,\n",
        "    'xtick.labelsize': 11,\n",
        "    'ytick.labelsize': 11,\n",
        "    'legend.fontsize': 11,\n",
        "    'figure.dpi': 300,\n",
        "    'savefig.dpi': 300,\n",
        "    'figure.autolayout': True,\n",
        "})\n",
        "\n",
        "# --------------------- Load Data ---------------------\n",
        "print(\"Mounting Google Drive and loading dataset...\")\n",
        "drive.mount('/content/drive')\n",
        "total_capture_7k = pd.read_csv('drive/My Drive/C02 project/correlation_wide.csv')\n",
        "print(f\"Loaded dataset with shape: {total_capture_7k.shape}\")\n",
        "\n",
        "# --------------------- Identify Unique Static Parameter Sets ---------------------\n",
        "static_cols = [\n",
        "    'MikeSorghum', 'Quartz', 'Plagioclase', 'Apatite', 'Ilmenite',\n",
        "    'Diopside_Mn', 'Diopside', 'Olivine', 'Alkali-feldspar',\n",
        "    'Montmorillonite', 'Glass', 'temp', 'shift', 'year'\n",
        "]\n",
        "\n",
        "# Add timestep count per file_id\n",
        "file_lengths = total_capture_7k.groupby('file_id').size().rename(\"num_timesteps\").reset_index()\n",
        "static_rows = total_capture_7k.groupby('file_id')[static_cols].first().reset_index()\n",
        "static_rows = static_rows.merge(file_lengths, on='file_id')\n",
        "\n",
        "# Filter only unique static parameter sets\n",
        "unique_static_rows = static_rows.drop_duplicates(subset=static_cols)\n",
        "unique_file_ids = unique_static_rows['file_id'].tolist()\n",
        "\n",
        "# --------------------- Extract Time Series Data ---------------------\n",
        "filtered_df = total_capture_7k[total_capture_7k['file_id'].isin(unique_file_ids)].copy()\n",
        "\n",
        "# Truncate each group to 101 timesteps\n",
        "filtered_df = filtered_df.groupby('file_id').head(101).reset_index(drop=True)\n",
        "\n",
        "# --------------------- Static Feature Table ---------------------\n",
        "Input_Link_Table = filtered_df.groupby('file_id').agg({col: 'first' for col in static_cols}).reset_index()\n",
        "print(f\"Static feature table created: Input_Link_Table.shape = {Input_Link_Table.shape}\")\n",
        "\n",
        "# --------------------- Time Series Structuring ---------------------\n",
        "result = filtered_df[['Total_CO2_capture', 'year', 'file_id']]\n",
        "file_ids = result['file_id'].unique()\n",
        "num_file_ids = len(file_ids)\n",
        "max_timesteps = 101\n",
        "relevant_data = np.zeros((num_file_ids, max_timesteps))\n",
        "file_id_order = np.zeros(num_file_ids)\n",
        "\n",
        "for i, file_id in enumerate(file_ids):\n",
        "    file_data = result[result['file_id'] == file_id]['Total_CO2_capture'].values\n",
        "    relevant_data[i, :len(file_data)] = file_data\n",
        "    file_id_order[i] = file_id\n",
        "print(f\"Time series matrix constructed: relevant_data.shape = {relevant_data.shape}\")\n",
        "\n",
        "# --------------------- Clustering ---------------------\n",
        "scaler = StandardScaler()\n",
        "normalized_data = scaler.fit_transform(relevant_data)\n",
        "kmeans = KMeans(n_clusters=8, random_state=42)\n",
        "clusters = kmeans.fit_predict(normalized_data)\n",
        "print(\"Performed KMeans clustering into 8 clusters\")\n",
        "\n",
        "# Compute boundary stats\n",
        "cluster_boundaries = []\n",
        "for cluster_id in range(8):\n",
        "    cluster_data = normalized_data[clusters == cluster_id]\n",
        "    min_v = scaler.inverse_transform(np.min(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    median_v = scaler.inverse_transform(np.median(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    mean_v = scaler.inverse_transform(np.mean(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    max_v = scaler.inverse_transform(np.max(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    cluster_boundaries.append((min_v, median_v, mean_v, max_v))\n",
        "cluster_boundaries = np.array(cluster_boundaries)\n",
        "print(f\"Cluster boundary stats calculated: cluster_boundaries.shape = {cluster_boundaries.shape}\")\n",
        "\n",
        "# --------------------- Merge Static Features with Clusters ---------------------\n",
        "Clustering_link_table = pd.DataFrame({'file_id': file_id_order.astype(int), 'cluster': clusters})\n",
        "Clustering_link_table = Clustering_link_table.sort_values(by='file_id').reset_index(drop=True)\n",
        "merged_df = pd.merge(Input_Link_Table, Clustering_link_table, on='file_id')\n",
        "print(f\"Final input features (static + cluster): merged_df.shape = {merged_df.shape}\")\n",
        "\n",
        "# --------------------- Create Output Time Series DataFrame ---------------------\n",
        "data = [[file_id_order[i].astype(int), t, relevant_data[i, t]] for i in range(len(file_id_order)) for t in range(max_timesteps)]\n",
        "df_output = pd.DataFrame(data, columns=['file_id', 'timestep', 'CO2']).sort_values(by=['file_id', 'timestep'])\n",
        "print(f\"Final output time series: df_output.shape = {df_output.shape}\")\n",
        "\n",
        "# --------------------- Summary ---------------------\n",
        "print(\"Data Preparation Summary:\")\n",
        "print(f\"Static Input Table: merged_df [{merged_df.shape[0]} rows × {merged_df.shape[1]} columns]\")\n",
        "print(f\"Time Series Output: df_output [{df_output.shape[0]} rows × 3 columns]\")\n",
        "print(f\"Cluster Boundaries: cluster_boundaries [{cluster_boundaries.shape}]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRIehVzL9_fn",
        "outputId": "332b3be8-cd28-4ad2-ebfc-957f54bbd40c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive and loading dataset...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loaded dataset with shape: (1192157, 17)\n",
            "Static feature table created: Input_Link_Table.shape = (2703, 15)\n",
            "Time series matrix constructed: relevant_data.shape = (2703, 101)\n",
            "Performed KMeans clustering into 8 clusters\n",
            "Cluster boundary stats calculated: cluster_boundaries.shape = (8, 4, 101)\n",
            "Final input features (static + cluster): merged_df.shape = (2703, 16)\n",
            "Final output time series: df_output.shape = (273003, 3)\n",
            "Data Preparation Summary:\n",
            "Static Input Table: merged_df [2703 rows × 16 columns]\n",
            "Time Series Output: df_output [273003 rows × 3 columns]\n",
            "Cluster Boundaries: cluster_boundaries [(8, 4, 101)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM**"
      ],
      "metadata": {
        "id": "gFPd5kg3Dg97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTM_MIMO(nn.Module):\n",
        "    def __init__(self, input_len, output_len, static_dim, hidden_dim=128, num_layers=2):\n",
        "        super(LSTM_MIMO, self).__init__()\n",
        "        self.input_len = input_len\n",
        "        self.output_len = output_len\n",
        "\n",
        "        self.static_fc = nn.Sequential(\n",
        "            nn.Linear(static_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=1 + hidden_dim, hidden_size=hidden_dim,\n",
        "                            num_layers=num_layers, batch_first=True)\n",
        "\n",
        "        self.output_fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_len)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_seq, x_static):\n",
        "        # x_seq: [B, T, 1]\n",
        "        # x_static: [B, static_dim]\n",
        "        batch_size, seq_len, _ = x_seq.size()\n",
        "        static_encoded = self.static_fc(x_static)  # [B, H]\n",
        "        static_expanded = static_encoded.unsqueeze(1).expand(-1, seq_len, -1)  # [B, T, H]\n",
        "        lstm_input = torch.cat([x_seq, static_expanded], dim=-1)  # [B, T, 1+H]\n",
        "        lstm_out, _ = self.lstm(lstm_input)  # [B, T, H]\n",
        "        last_hidden = lstm_out[:, -1, :]  # [B, H]\n",
        "        out = self.output_fc(last_hidden)  # [B, output_len]\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "4TiIeh11DhF7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments - Overnight duration experimentation"
      ],
      "metadata": {
        "id": "A9_s-5G2t5jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Setup\n",
        "splits = [(80, 20), (60, 40), (50, 50), (40, 60), (20, 80), (10, 90), (5, 95)]\n",
        "LSTM_mse = []\n",
        "\n",
        "# For saving models\n",
        "os.makedirs(\"drive/My Drive/DSSM-Models\", exist_ok=True)\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "for train_pct, test_pct in splits:\n",
        "    split_name = f\"{train_pct}_{test_pct}\"\n",
        "    train_ids, test_ids = train_test_split(df_output['file_id'].unique(), test_size=0.2, random_state=42)\n",
        "    df_train = df_output[df_output['file_id'].isin(train_ids)]\n",
        "    df_test = df_output[df_output['file_id'].isin(test_ids)]\n",
        "\n",
        "    train_timestep = int(train_pct / 100 * 101)\n",
        "    pred_timestep = 101 - train_timestep\n",
        "    print(f\"\\n====== Split: {split_name} | X (input): {train_timestep} AND Y (output): {pred_timestep} ======\")\n",
        "\n",
        "    # Pivot time series\n",
        "    X_train = df_train.pivot(index='file_id', columns='timestep', values='CO2').values[:, :train_timestep]\n",
        "    Y_train = df_train.pivot(index='file_id', columns='timestep', values='CO2').values[:, train_timestep:]\n",
        "    X_test = df_test.pivot(index='file_id', columns='timestep', values='CO2').values[:, :train_timestep]\n",
        "    Y_test = df_test.pivot(index='file_id', columns='timestep', values='CO2').values[:, train_timestep:]\n",
        "\n",
        "    static_train = merged_df[merged_df['file_id'].isin(train_ids)].drop(columns=['file_id', 'cluster']).values\n",
        "    static_test = merged_df[merged_df['file_id'].isin(test_ids)].drop(columns=['file_id', 'cluster']).values\n",
        "\n",
        "    # Tensors\n",
        "    X_train_tensor = torch.tensor(X_train[:, :, None], dtype=torch.float32)\n",
        "    Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
        "    static_train_tensor = torch.tensor(static_train, dtype=torch.float32)\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test[:, :, None], dtype=torch.float32)\n",
        "    Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
        "    static_test_tensor = torch.tensor(static_test, dtype=torch.float32)\n",
        "\n",
        "    print(f\"Train X shape: {X_train_tensor.shape}, Train Y shape: {Y_train_tensor.shape}, Static: {static_train_tensor.shape}\")\n",
        "    print(f\"Test  X shape: {X_test_tensor.shape}, Test  Y shape: {Y_test_tensor.shape}, Static: {static_test_tensor.shape}\")\n",
        "\n",
        "    # Dataset and loaders\n",
        "    full_train_dataset = TensorDataset(X_train_tensor, static_train_tensor, Y_train_tensor)\n",
        "    val_size = int(0.1 * len(full_train_dataset))\n",
        "    train_size = len(full_train_dataset) - val_size\n",
        "    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
        "    test_dataset = TensorDataset(X_test_tensor, static_test_tensor, Y_test_tensor)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "    # ------------------ Model and Config ------------------\n",
        "    lstm_config = {\n",
        "        \"static_dim\": static_train.shape[1],\n",
        "        \"hidden_dim\": 128,\n",
        "        \"num_layers\": 2,\n",
        "        \"dropout\": 0.0,\n",
        "        \"lr\": 0.001,\n",
        "        \"batch_size\": 64,\n",
        "        \"epochs\": 500\n",
        "    }\n",
        "\n",
        "    model = LSTM_MIMO(\n",
        "        input_len=train_timestep,\n",
        "        output_len=pred_timestep,\n",
        "        static_dim=lstm_config[\"static_dim\"],\n",
        "        hidden_dim=lstm_config[\"hidden_dim\"],\n",
        "        num_layers=lstm_config[\"num_layers\"]\n",
        "    )\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lstm_config[\"lr\"])\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # ------------------ Training ------------------\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_path = f\"drive/My Drive/DSSM-Models/LSTM_best_{split_name}.pt\"\n",
        "    print(\"Training started...\")\n",
        "\n",
        "    for epoch in range(1, lstm_config[\"epochs\"] + 1):\n",
        "        model.train()\n",
        "        for X_batch, static_batch, Y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X_batch, static_batch)\n",
        "            loss = criterion(preds, Y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_samples = 0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, static_batch, Y_batch in val_loader:\n",
        "                preds = model(X_batch, static_batch)\n",
        "                batch_loss = criterion(preds, Y_batch).item()\n",
        "                val_loss += batch_loss * X_batch.size(0)\n",
        "                val_samples += X_batch.size(0)\n",
        "        avg_val_loss = val_loss / val_samples\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "\n",
        "        print(f\"[Epoch {epoch}] Training Loss: {loss.item():.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "    # ------------------ Evaluation ------------------\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "    model.eval()\n",
        "    total_mse = 0.0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, static_batch, Y_batch in test_loader:\n",
        "            outputs = model(X_batch, static_batch)\n",
        "            batch_mse = criterion(outputs, Y_batch).item()\n",
        "            total_mse += batch_mse * X_batch.size(0)\n",
        "            total_samples += X_batch.size(0)\n",
        "\n",
        "    avg_mse = total_mse / total_samples\n",
        "    print(f\"Final Test MSE for LSTM ({split_name}): {avg_mse:.6f}\")\n",
        "    LSTM_mse.append({'Split': split_name, 'Test_MSE': avg_mse})\n",
        "\n",
        "# Save result as DataFrame\n",
        "LSTM_mse = pd.DataFrame(LSTM_mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7yv3m0nFqsh",
        "outputId": "ed26acba-abe8-4760-d7d8-2a64e041bcef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====== Split: 80_20 | X (input): 80 AND Y (output): 21 ======\n",
            "Train X shape: torch.Size([2162, 80, 1]), Train Y shape: torch.Size([2162, 21]), Static: torch.Size([2162, 14])\n",
            "Test  X shape: torch.Size([541, 80, 1]), Test  Y shape: torch.Size([541, 21]), Static: torch.Size([541, 14])\n",
            "Training started...\n",
            "[Epoch 1] Training Loss: 0.208605 | Val Loss: 0.310658\n",
            "[Epoch 2] Training Loss: 0.161890 | Val Loss: 0.187543\n",
            "[Epoch 3] Training Loss: 0.212877 | Val Loss: 0.181633\n",
            "[Epoch 4] Training Loss: 0.116815 | Val Loss: 0.185046\n",
            "[Epoch 5] Training Loss: 0.114678 | Val Loss: 0.182446\n",
            "[Epoch 6] Training Loss: 0.131637 | Val Loss: 0.183438\n",
            "[Epoch 7] Training Loss: 0.138782 | Val Loss: 0.191557\n",
            "[Epoch 8] Training Loss: 0.154229 | Val Loss: 0.187848\n",
            "[Epoch 9] Training Loss: 0.168869 | Val Loss: 0.184001\n",
            "[Epoch 10] Training Loss: 0.130706 | Val Loss: 0.184492\n",
            "[Epoch 11] Training Loss: 0.128849 | Val Loss: 0.192801\n",
            "[Epoch 12] Training Loss: 0.175763 | Val Loss: 0.193712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "meBAL_rn-6Nr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}