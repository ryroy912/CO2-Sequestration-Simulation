{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHubmQ-gpS0u"
      },
      "source": [
        "# DATA PREPERATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCxMIiA1fexX",
        "outputId": "53c5516c-914a-4ba6-9762-05ca6f457d66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounting Google Drive and loading dataset...\n",
            "Mounted at /content/drive\n",
            "Loaded dataset with shape: (1192157, 17)\n",
            "Static feature table created: Input_Link_Table.shape = (2703, 15)\n",
            "Time series matrix constructed: relevant_data.shape = (2703, 101)\n",
            "Performed KMeans clustering into 8 clusters\n",
            "Cluster boundary stats calculated: cluster_boundaries.shape = (8, 4, 101)\n",
            "Final input features (static + cluster): merged_df.shape = (2703, 16)\n",
            "Final output time series: df_output.shape = (273003, 3)\n",
            "Data Preparation Summary:\n",
            "Static Input Table: merged_df [2703 rows × 16 columns]\n",
            "Time Series Output: df_output [273003 rows × 3 columns]\n",
            "Cluster Boundaries: cluster_boundaries [(8, 4, 101)]\n"
          ]
        }
      ],
      "source": [
        "# --------------------- Imports ---------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from google.colab import drive\n",
        "\n",
        "# --------------------- Matplotlib Setup ---------------------\n",
        "mpl.rcParams.update({\n",
        "    'font.size': 14,\n",
        "    'axes.titlesize': 15,\n",
        "    'axes.labelsize': 12,\n",
        "    'xtick.labelsize': 11,\n",
        "    'ytick.labelsize': 11,\n",
        "    'legend.fontsize': 11,\n",
        "    'figure.dpi': 300,\n",
        "    'savefig.dpi': 300,\n",
        "    'figure.autolayout': True,\n",
        "})\n",
        "\n",
        "# --------------------- Load Data ---------------------\n",
        "print(\"Mounting Google Drive and loading dataset...\")\n",
        "drive.mount('/content/drive')\n",
        "total_capture_7k = pd.read_csv('drive/My Drive/correlation_wide.csv')\n",
        "print(f\"Loaded dataset with shape: {total_capture_7k.shape}\")\n",
        "\n",
        "# --------------------- Identify Unique Static Parameter Sets ---------------------\n",
        "static_cols = [\n",
        "    'MikeSorghum', 'Quartz', 'Plagioclase', 'Apatite', 'Ilmenite',\n",
        "    'Diopside_Mn', 'Diopside', 'Olivine', 'Alkali-feldspar',\n",
        "    'Montmorillonite', 'Glass', 'temp', 'shift', 'year'\n",
        "]\n",
        "\n",
        "# Add timestep count per file_id\n",
        "file_lengths = total_capture_7k.groupby('file_id').size().rename(\"num_timesteps\").reset_index()\n",
        "static_rows = total_capture_7k.groupby('file_id')[static_cols].first().reset_index()\n",
        "static_rows = static_rows.merge(file_lengths, on='file_id')\n",
        "\n",
        "# Filter only unique static parameter sets\n",
        "unique_static_rows = static_rows.drop_duplicates(subset=static_cols)\n",
        "unique_file_ids = unique_static_rows['file_id'].tolist()\n",
        "\n",
        "# --------------------- Extract Time Series Data ---------------------\n",
        "filtered_df = total_capture_7k[total_capture_7k['file_id'].isin(unique_file_ids)].copy()\n",
        "\n",
        "# Truncate each group to 101 timesteps\n",
        "filtered_df = filtered_df.groupby('file_id').head(101).reset_index(drop=True)\n",
        "\n",
        "# --------------------- Static Feature Table ---------------------\n",
        "Input_Link_Table = filtered_df.groupby('file_id').agg({col: 'first' for col in static_cols}).reset_index()\n",
        "print(f\"Static feature table created: Input_Link_Table.shape = {Input_Link_Table.shape}\")\n",
        "\n",
        "# --------------------- Time Series Structuring ---------------------\n",
        "result = filtered_df[['Total_CO2_capture', 'year', 'file_id']]\n",
        "file_ids = result['file_id'].unique()\n",
        "num_file_ids = len(file_ids)\n",
        "max_timesteps = 101\n",
        "relevant_data = np.zeros((num_file_ids, max_timesteps))\n",
        "file_id_order = np.zeros(num_file_ids)\n",
        "\n",
        "for i, file_id in enumerate(file_ids):\n",
        "    file_data = result[result['file_id'] == file_id]['Total_CO2_capture'].values\n",
        "    relevant_data[i, :len(file_data)] = file_data\n",
        "    file_id_order[i] = file_id\n",
        "print(f\"Time series matrix constructed: relevant_data.shape = {relevant_data.shape}\")\n",
        "\n",
        "# --------------------- Clustering ---------------------\n",
        "scaler = StandardScaler()\n",
        "normalized_data = scaler.fit_transform(relevant_data)\n",
        "kmeans = KMeans(n_clusters=8, random_state=42)\n",
        "clusters = kmeans.fit_predict(normalized_data)\n",
        "print(\"Performed KMeans clustering into 8 clusters\")\n",
        "\n",
        "# Compute boundary stats\n",
        "cluster_boundaries = []\n",
        "for cluster_id in range(8):\n",
        "    cluster_data = normalized_data[clusters == cluster_id]\n",
        "    min_v = scaler.inverse_transform(np.min(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    median_v = scaler.inverse_transform(np.median(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    mean_v = scaler.inverse_transform(np.mean(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    max_v = scaler.inverse_transform(np.max(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    cluster_boundaries.append((min_v, median_v, mean_v, max_v))\n",
        "cluster_boundaries = np.array(cluster_boundaries)\n",
        "print(f\"Cluster boundary stats calculated: cluster_boundaries.shape = {cluster_boundaries.shape}\")\n",
        "\n",
        "# --------------------- Merge Static Features with Clusters ---------------------\n",
        "Clustering_link_table = pd.DataFrame({'file_id': file_id_order.astype(int), 'cluster': clusters})\n",
        "Clustering_link_table = Clustering_link_table.sort_values(by='file_id').reset_index(drop=True)\n",
        "merged_df = pd.merge(Input_Link_Table, Clustering_link_table, on='file_id')\n",
        "print(f\"Final input features (static + cluster): merged_df.shape = {merged_df.shape}\")\n",
        "\n",
        "# --------------------- Create Output Time Series DataFrame ---------------------\n",
        "data = [[file_id_order[i].astype(int), t, relevant_data[i, t]] for i in range(len(file_id_order)) for t in range(max_timesteps)]\n",
        "df_output = pd.DataFrame(data, columns=['file_id', 'timestep', 'CO2']).sort_values(by=['file_id', 'timestep'])\n",
        "print(f\"Final output time series: df_output.shape = {df_output.shape}\")\n",
        "\n",
        "# --------------------- Summary ---------------------\n",
        "print(\"Data Preparation Summary:\")\n",
        "print(f\"Static Input Table: merged_df [{merged_df.shape[0]} rows × {merged_df.shape[1]} columns]\")\n",
        "print(f\"Time Series Output: df_output [{df_output.shape[0]} rows × 3 columns]\")\n",
        "print(f\"Cluster Boundaries: cluster_boundaries [{cluster_boundaries.shape}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAJUIgGhpbYG"
      },
      "source": [
        "# MODELS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9MwI8wnpgOs"
      },
      "source": [
        "**Advanced DSSM model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iyn-qdunpkLO"
      },
      "outputs": [],
      "source": [
        "class AdvancedDSSMDeepState(nn.Module):\n",
        "    def __init__(self, input_dim, static_dim, hidden_dim, output_dim):\n",
        "        super(AdvancedDSSMDeepState, self).__init__()\n",
        "\n",
        "        # Static Data Path (Fully connected layers for static features)\n",
        "        self.fc_static1 = nn.Linear(static_dim, 512)\n",
        "        self.fc_static2 = nn.Linear(512, 256)\n",
        "        self.fc_static3 = nn.Linear(256, 128)\n",
        "        self.fc_static4 = nn.Linear(128, 64)\n",
        "\n",
        "        # Time-series Path (Conv1D for feature extraction)\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Deep State Dynamics (LSTM for latent state transitions)\n",
        "        self.lstm_state = nn.LSTM(hidden_dim + 64, hidden_dim, batch_first=True)\n",
        "\n",
        "        # Observation Model (Mapping latent states to outputs)\n",
        "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, time_series_input, static_input):\n",
        "        # Static Data Path\n",
        "        static_out = self.relu(self.fc_static1(static_input))\n",
        "        static_out = self.relu(self.fc_static2(static_out))\n",
        "        static_out = self.relu(self.fc_static3(static_out))\n",
        "        static_out = self.relu(self.fc_static4(static_out))  # Shape: [batch_size, 64]\n",
        "\n",
        "        # Time-Series Data Path\n",
        "        if len(time_series_input.shape) == 2:  # [batch_size, seq_len]\n",
        "            time_series_input = time_series_input.unsqueeze(1)  # Add channel dimension: [batch_size, 1, seq_len]\n",
        "\n",
        "        conv_out = self.conv1(time_series_input)  # Conv1D layer\n",
        "        conv_out = self.relu(conv_out)\n",
        "        conv_out = conv_out.transpose(1, 2)  # Shape: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        # Expand static features to match the sequence length\n",
        "        static_expanded = static_out.unsqueeze(1).expand(-1, conv_out.size(1), -1)  # Shape: [batch_size, seq_len, 64]\n",
        "\n",
        "        # Combine Conv1D features and static features\n",
        "        lstm_input = torch.cat([conv_out, static_expanded], dim=2)  # Shape: [batch_size, seq_len, hidden_dim + 64]\n",
        "\n",
        "        # Latent State Dynamics (LSTM for state transitions)\n",
        "        lstm_out, _ = self.lstm_state(lstm_input)  # Shape: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        # Observation Model\n",
        "        lstm_out_final = lstm_out[:, -1, :]  # Use the last state for prediction\n",
        "        x = self.fc1(lstm_out_final)\n",
        "        x = self.relu(x)\n",
        "        output = self.fc2(x)  # Final prediction\n",
        "\n",
        "        return output\n",
        "\n",
        "def plot_boundary_cases_with_input(inputs, Boundary_case_actuals, Boundary_case_predicted, model_name, input_length):\n",
        "    case_names = [\"Best\", \"Average\", \"Worst\"]\n",
        "    x_range = input_length\n",
        "    y_range = Boundary_case_actuals.shape[1]\n",
        "    total_timesteps = x_range + y_range\n",
        "\n",
        "    for i in range(3):\n",
        "        plt.figure(figsize=(7.5, 3.2))\n",
        "\n",
        "        # Plot input (X)\n",
        "        plt.plot(range(x_range), inputs[i], color='black', alpha=0.5, label='Input')\n",
        "\n",
        "        # Plot output actual vs predicted (Y)\n",
        "        plt.plot(range(x_range, total_timesteps), Boundary_case_actuals[i], color='blue', alpha=0.8, label='Actual')\n",
        "        plt.plot(range(x_range, total_timesteps), Boundary_case_predicted[i], color='red', alpha=0.8, label='Predicted')\n",
        "\n",
        "        plt.xlabel(\"Time Steps\")\n",
        "        plt.ylabel(\"CO₂ Sequestration\")\n",
        "        plt.title(f\"{case_names[i]} Case – {model_name}\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout(pad=2.5)\n",
        "\n",
        "        filename = f\"drive/My Drive/DSSM-Figures1/{model_name}_{case_names[i]}.pdf\"\n",
        "        plt.savefig(filename, format='pdf', bbox_inches='tight')\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR-V3lwKt9Mt"
      },
      "source": [
        "**Advanced-DSSM-MSE-experiment (best model, saved model, hyperparameter listing, training status report)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BJjwaLJnFqkS",
        "outputId": "3374c157-bcbd-460f-9e40-1507c1abcd9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Training config: hd50_bs32_lr0.001 ===\n",
            "Epoch 10: Train Loss = 0.002272, Val Loss = 0.025849\n",
            "Epoch 20: Train Loss = 0.006224, Val Loss = 0.018834\n",
            "Epoch 30: Train Loss = 0.001257, Val Loss = 0.017925\n",
            "Epoch 40: Train Loss = 0.002396, Val Loss = 0.016167\n",
            "Epoch 50: Train Loss = 0.004867, Val Loss = 0.014591\n",
            "Epoch 60: Train Loss = 0.000927, Val Loss = 0.017997\n",
            "Epoch 70: Train Loss = 0.001794, Val Loss = 0.012556\n",
            "Epoch 80: Train Loss = 0.120569, Val Loss = 0.031388\n",
            "Epoch 90: Train Loss = 0.002861, Val Loss = 0.014293\n",
            "Epoch 100: Train Loss = 0.000354, Val Loss = 0.011990\n",
            "Epoch 110: Train Loss = 0.000236, Val Loss = 0.006642\n",
            "Epoch 120: Train Loss = 0.003129, Val Loss = 0.014372\n",
            "Epoch 130: Train Loss = 0.001541, Val Loss = 0.007804\n",
            "Epoch 140: Train Loss = 0.003383, Val Loss = 0.008697\n",
            "Epoch 150: Train Loss = 0.001954, Val Loss = 0.006914\n",
            "Epoch 160: Train Loss = 0.001556, Val Loss = 0.005949\n",
            "Epoch 170: Train Loss = 0.001834, Val Loss = 0.006129\n",
            "Epoch 180: Train Loss = 0.002474, Val Loss = 0.008606\n",
            "Epoch 190: Train Loss = 0.000464, Val Loss = 0.005092\n",
            "Epoch 200: Train Loss = 0.001317, Val Loss = 0.004709\n",
            "Test MSE for config hd50_bs32_lr0.001: 0.003900\n",
            "\n",
            "=== Training config: hd50_bs32_lr0.005 ===\n",
            "Epoch 10: Train Loss = 0.140455, Val Loss = 0.210773\n",
            "Epoch 20: Train Loss = 0.013311, Val Loss = 0.191902\n",
            "Epoch 30: Train Loss = 0.140707, Val Loss = 0.189659\n",
            "Epoch 40: Train Loss = 0.108600, Val Loss = 0.177131\n",
            "Epoch 50: Train Loss = 0.007665, Val Loss = 0.176350\n",
            "Epoch 60: Train Loss = 0.105464, Val Loss = 0.174094\n",
            "Epoch 70: Train Loss = 0.031464, Val Loss = 0.172128\n",
            "Epoch 80: Train Loss = 0.016042, Val Loss = 0.171942\n",
            "Epoch 90: Train Loss = 0.107432, Val Loss = 0.174168\n",
            "Epoch 100: Train Loss = 0.254894, Val Loss = 0.172573\n",
            "Epoch 110: Train Loss = 0.026398, Val Loss = 0.177962\n",
            "Epoch 120: Train Loss = 0.283570, Val Loss = 0.172004\n",
            "Epoch 130: Train Loss = 0.194321, Val Loss = 0.175417\n",
            "Epoch 140: Train Loss = 0.076601, Val Loss = 0.177057\n",
            "Epoch 150: Train Loss = 0.035329, Val Loss = 0.175126\n",
            "Epoch 160: Train Loss = 0.021921, Val Loss = 0.173574\n",
            "Epoch 170: Train Loss = 0.216456, Val Loss = 0.181488\n",
            "Epoch 180: Train Loss = 0.122943, Val Loss = 0.173395\n",
            "Epoch 190: Train Loss = 0.254278, Val Loss = 0.175376\n",
            "Epoch 200: Train Loss = 0.007413, Val Loss = 0.172566\n",
            "Test MSE for config hd50_bs32_lr0.005: 0.144699\n",
            "\n",
            "=== Training config: hd50_bs32_lr0.01 ===\n",
            "Epoch 10: Train Loss = 0.463049, Val Loss = 0.207610\n",
            "Epoch 20: Train Loss = 0.037150, Val Loss = 0.172821\n",
            "Epoch 30: Train Loss = 0.277268, Val Loss = 0.181503\n",
            "Epoch 40: Train Loss = 0.076433, Val Loss = 0.184228\n",
            "Epoch 50: Train Loss = 0.126775, Val Loss = 0.171943\n",
            "Epoch 60: Train Loss = 0.039016, Val Loss = 0.171884\n",
            "Epoch 70: Train Loss = 0.074898, Val Loss = 0.194297\n",
            "Epoch 80: Train Loss = 0.025216, Val Loss = 0.174262\n",
            "Epoch 90: Train Loss = 0.687887, Val Loss = 0.178559\n",
            "Epoch 100: Train Loss = 0.001292, Val Loss = 0.191572\n",
            "Epoch 110: Train Loss = 0.013932, Val Loss = 0.188169\n",
            "Epoch 120: Train Loss = 0.035629, Val Loss = 0.183104\n",
            "Epoch 130: Train Loss = 0.123880, Val Loss = 0.185239\n",
            "Epoch 140: Train Loss = 0.001772, Val Loss = 0.177423\n",
            "Epoch 150: Train Loss = 0.129956, Val Loss = 0.181410\n",
            "Epoch 160: Train Loss = 0.168627, Val Loss = 0.173227\n",
            "Epoch 170: Train Loss = 0.000910, Val Loss = 0.175974\n",
            "Epoch 180: Train Loss = 0.000273, Val Loss = 0.185282\n",
            "Epoch 190: Train Loss = 0.047082, Val Loss = 0.182503\n",
            "Epoch 200: Train Loss = 0.355802, Val Loss = 0.172617\n",
            "Test MSE for config hd50_bs32_lr0.01: 0.144610\n",
            "\n",
            "=== Training config: hd50_bs64_lr0.001 ===\n",
            "Epoch 10: Train Loss = 0.027874, Val Loss = 0.189744\n",
            "Epoch 20: Train Loss = 0.000214, Val Loss = 0.021632\n",
            "Epoch 30: Train Loss = 0.002068, Val Loss = 0.019672\n",
            "Epoch 40: Train Loss = 0.031028, Val Loss = 0.023206\n",
            "Epoch 50: Train Loss = 0.009059, Val Loss = 0.017289\n",
            "Epoch 60: Train Loss = 0.000360, Val Loss = 0.015948\n",
            "Epoch 70: Train Loss = 0.002429, Val Loss = 0.015021\n",
            "Epoch 80: Train Loss = 0.009168, Val Loss = 0.018114\n",
            "Epoch 90: Train Loss = 0.001364, Val Loss = 0.013128\n",
            "Epoch 100: Train Loss = 0.000283, Val Loss = 0.011805\n",
            "Epoch 110: Train Loss = 0.000706, Val Loss = 0.012145\n",
            "Epoch 120: Train Loss = 0.041719, Val Loss = 0.026338\n",
            "Epoch 130: Train Loss = 0.007132, Val Loss = 0.019104\n",
            "Epoch 140: Train Loss = 0.001045, Val Loss = 0.015652\n",
            "Epoch 150: Train Loss = 0.000983, Val Loss = 0.008003\n",
            "Epoch 160: Train Loss = 0.008753, Val Loss = 0.014908\n",
            "Epoch 170: Train Loss = 0.000656, Val Loss = 0.007097\n",
            "Epoch 180: Train Loss = 0.001680, Val Loss = 0.008247\n",
            "Epoch 190: Train Loss = 0.000564, Val Loss = 0.006688\n",
            "Epoch 200: Train Loss = 0.000808, Val Loss = 0.005099\n",
            "Test MSE for config hd50_bs64_lr0.001: 0.003666\n",
            "\n",
            "=== Training config: hd50_bs64_lr0.005 ===\n",
            "Epoch 10: Train Loss = 0.008087, Val Loss = 0.174516\n",
            "Epoch 20: Train Loss = 0.298827, Val Loss = 0.182231\n",
            "Epoch 30: Train Loss = 0.099174, Val Loss = 0.173810\n",
            "Epoch 40: Train Loss = 0.234949, Val Loss = 0.190293\n",
            "Epoch 50: Train Loss = 0.007742, Val Loss = 0.171381\n",
            "Epoch 60: Train Loss = 0.023950, Val Loss = 0.169977\n",
            "Epoch 70: Train Loss = 0.037476, Val Loss = 0.180442\n",
            "Epoch 80: Train Loss = 0.411967, Val Loss = 0.182133\n",
            "Epoch 90: Train Loss = 0.058044, Val Loss = 0.170578\n",
            "Epoch 100: Train Loss = 0.118568, Val Loss = 0.174973\n",
            "Epoch 110: Train Loss = 0.060869, Val Loss = 0.186200\n",
            "Epoch 120: Train Loss = 0.013369, Val Loss = 0.173814\n",
            "Epoch 130: Train Loss = 0.084768, Val Loss = 0.170160\n",
            "Epoch 140: Train Loss = 0.010223, Val Loss = 0.173728\n",
            "Epoch 150: Train Loss = 0.037368, Val Loss = 0.170964\n",
            "Epoch 160: Train Loss = 0.138383, Val Loss = 0.188215\n",
            "Epoch 170: Train Loss = 0.561430, Val Loss = 0.172530\n",
            "Epoch 180: Train Loss = 0.125670, Val Loss = 0.194886\n",
            "Epoch 190: Train Loss = 0.138082, Val Loss = 0.175519\n",
            "Epoch 200: Train Loss = 0.249566, Val Loss = 0.171102\n",
            "Test MSE for config hd50_bs64_lr0.005: 0.144666\n",
            "\n",
            "=== Training config: hd50_bs64_lr0.01 ===\n",
            "Epoch 10: Train Loss = 0.098973, Val Loss = 0.195005\n",
            "Epoch 20: Train Loss = 0.005906, Val Loss = 0.219739\n",
            "Epoch 30: Train Loss = 0.031527, Val Loss = 0.176394\n",
            "Epoch 40: Train Loss = 0.001755, Val Loss = 0.176624\n",
            "Epoch 50: Train Loss = 0.002300, Val Loss = 0.170641\n",
            "Epoch 60: Train Loss = 0.200300, Val Loss = 0.175696\n",
            "Epoch 70: Train Loss = 0.060719, Val Loss = 0.174034\n",
            "Epoch 80: Train Loss = 0.230382, Val Loss = 0.193886\n",
            "Epoch 90: Train Loss = 0.186101, Val Loss = 0.179369\n",
            "Epoch 100: Train Loss = 0.066225, Val Loss = 0.171190\n",
            "Epoch 110: Train Loss = 0.017565, Val Loss = 0.174395\n",
            "Epoch 120: Train Loss = 0.266224, Val Loss = 0.189289\n",
            "Epoch 130: Train Loss = 0.193274, Val Loss = 0.199437\n",
            "Epoch 140: Train Loss = 0.048649, Val Loss = 0.169708\n",
            "Epoch 150: Train Loss = 0.172640, Val Loss = 0.170746\n",
            "Epoch 160: Train Loss = 0.043833, Val Loss = 0.170663\n",
            "Epoch 170: Train Loss = 0.301092, Val Loss = 0.173233\n",
            "Epoch 180: Train Loss = 0.102817, Val Loss = 0.171071\n",
            "Epoch 190: Train Loss = 0.273668, Val Loss = 0.197211\n",
            "Epoch 200: Train Loss = 0.135767, Val Loss = 0.169755\n",
            "Test MSE for config hd50_bs64_lr0.01: 0.144755\n",
            "\n",
            "=== Training config: hd50_bs128_lr0.001 ===\n",
            "Epoch 10: Train Loss = 0.155171, Val Loss = 0.166716\n",
            "Epoch 20: Train Loss = 0.113272, Val Loss = 0.147983\n",
            "Epoch 30: Train Loss = 0.042477, Val Loss = 0.071078\n",
            "Epoch 40: Train Loss = 0.026714, Val Loss = 0.020914\n",
            "Epoch 50: Train Loss = 0.013649, Val Loss = 0.020190\n",
            "Epoch 60: Train Loss = 0.006361, Val Loss = 0.023132\n",
            "Epoch 70: Train Loss = 0.013994, Val Loss = 0.018199\n",
            "Epoch 80: Train Loss = 0.012793, Val Loss = 0.017404\n",
            "Epoch 90: Train Loss = 0.010004, Val Loss = 0.016664\n",
            "Epoch 100: Train Loss = 0.008471, Val Loss = 0.016453\n",
            "Epoch 110: Train Loss = 0.009527, Val Loss = 0.016640\n",
            "Epoch 120: Train Loss = 0.007252, Val Loss = 0.016951\n",
            "Epoch 130: Train Loss = 0.007686, Val Loss = 0.015676\n",
            "Epoch 140: Train Loss = 0.008134, Val Loss = 0.016436\n",
            "Epoch 150: Train Loss = 0.005722, Val Loss = 0.015247\n",
            "Epoch 160: Train Loss = 0.006227, Val Loss = 0.014597\n",
            "Epoch 170: Train Loss = 0.007579, Val Loss = 0.015013\n",
            "Epoch 180: Train Loss = 0.004214, Val Loss = 0.013485\n",
            "Epoch 190: Train Loss = 0.005987, Val Loss = 0.012259\n",
            "Epoch 200: Train Loss = 0.005753, Val Loss = 0.012381\n",
            "Test MSE for config hd50_bs128_lr0.001: 0.008009\n",
            "\n",
            "=== Training config: hd50_bs128_lr0.005 ===\n",
            "Epoch 10: Train Loss = 0.147177, Val Loss = 0.165857\n",
            "Epoch 20: Train Loss = 0.166029, Val Loss = 0.170541\n",
            "Epoch 30: Train Loss = 0.171422, Val Loss = 0.168829\n",
            "Epoch 40: Train Loss = 0.144066, Val Loss = 0.174858\n",
            "Epoch 50: Train Loss = 0.139285, Val Loss = 0.167942\n",
            "Epoch 60: Train Loss = 0.227625, Val Loss = 0.169395\n",
            "Epoch 70: Train Loss = 0.170982, Val Loss = 0.169037\n",
            "Epoch 80: Train Loss = 0.112853, Val Loss = 0.179079\n",
            "Epoch 90: Train Loss = 0.126797, Val Loss = 0.166040\n",
            "Epoch 100: Train Loss = 0.150421, Val Loss = 0.173504\n",
            "Epoch 110: Train Loss = 0.133500, Val Loss = 0.166050\n",
            "Epoch 120: Train Loss = 0.130519, Val Loss = 0.170867\n",
            "Epoch 130: Train Loss = 0.179178, Val Loss = 0.166471\n",
            "Epoch 140: Train Loss = 0.185114, Val Loss = 0.169817\n",
            "Epoch 150: Train Loss = 0.190683, Val Loss = 0.165521\n",
            "Epoch 160: Train Loss = 0.146114, Val Loss = 0.166225\n",
            "Epoch 170: Train Loss = 0.108803, Val Loss = 0.174470\n",
            "Epoch 180: Train Loss = 0.147805, Val Loss = 0.165569\n",
            "Epoch 190: Train Loss = 0.139307, Val Loss = 0.165806\n",
            "Epoch 200: Train Loss = 0.165187, Val Loss = 0.175268\n",
            "Test MSE for config hd50_bs128_lr0.005: 0.144701\n",
            "\n",
            "=== Training config: hd50_bs128_lr0.01 ===\n",
            "Epoch 10: Train Loss = 0.155630, Val Loss = 0.165524\n",
            "Epoch 20: Train Loss = 0.159187, Val Loss = 0.165793\n",
            "Epoch 30: Train Loss = 0.117619, Val Loss = 0.170640\n",
            "Epoch 40: Train Loss = 0.157238, Val Loss = 0.165493\n",
            "Epoch 50: Train Loss = 0.161068, Val Loss = 0.171516\n",
            "Epoch 60: Train Loss = 0.101642, Val Loss = 0.168215\n",
            "Epoch 70: Train Loss = 0.112751, Val Loss = 0.171547\n",
            "Epoch 80: Train Loss = 0.137542, Val Loss = 0.207260\n",
            "Epoch 90: Train Loss = 0.111801, Val Loss = 0.170296\n",
            "Epoch 100: Train Loss = 0.111419, Val Loss = 0.171339\n",
            "Epoch 110: Train Loss = 0.167593, Val Loss = 0.182667\n",
            "Epoch 120: Train Loss = 0.148617, Val Loss = 0.171079\n",
            "Epoch 130: Train Loss = 0.205389, Val Loss = 0.180247\n",
            "Epoch 140: Train Loss = 0.129617, Val Loss = 0.170641\n",
            "Epoch 150: Train Loss = 0.206036, Val Loss = 0.172816\n",
            "Epoch 160: Train Loss = 0.141294, Val Loss = 0.167263\n",
            "Epoch 170: Train Loss = 0.173204, Val Loss = 0.205975\n",
            "Epoch 180: Train Loss = 0.153801, Val Loss = 0.167928\n",
            "Epoch 190: Train Loss = 0.141566, Val Loss = 0.167345\n",
            "Epoch 200: Train Loss = 0.139844, Val Loss = 0.172849\n",
            "Test MSE for config hd50_bs128_lr0.01: 0.144744\n",
            "\n",
            "=== Training config: hd101_bs32_lr0.001 ===\n",
            "Epoch 10: Train Loss = 0.010634, Val Loss = 0.021615\n",
            "Epoch 20: Train Loss = 0.003646, Val Loss = 0.017286\n",
            "Epoch 30: Train Loss = 0.001248, Val Loss = 0.016198\n",
            "Epoch 40: Train Loss = 0.000523, Val Loss = 0.015576\n",
            "Epoch 50: Train Loss = 0.001169, Val Loss = 0.014953\n",
            "Epoch 60: Train Loss = 0.001180, Val Loss = 0.013412\n",
            "Epoch 70: Train Loss = 0.004152, Val Loss = 0.013844\n",
            "Epoch 80: Train Loss = 0.000110, Val Loss = 0.010978\n",
            "Epoch 90: Train Loss = 0.004264, Val Loss = 0.013625\n",
            "Epoch 100: Train Loss = 0.000483, Val Loss = 0.009774\n",
            "Epoch 110: Train Loss = 0.001293, Val Loss = 0.009622\n",
            "Epoch 120: Train Loss = 0.000223, Val Loss = 0.009456\n",
            "Epoch 130: Train Loss = 0.000655, Val Loss = 0.009490\n",
            "Epoch 140: Train Loss = 0.000754, Val Loss = 0.008840\n",
            "Epoch 150: Train Loss = 0.000549, Val Loss = 0.011377\n",
            "Epoch 160: Train Loss = 0.000043, Val Loss = 0.008385\n",
            "Epoch 170: Train Loss = 0.001744, Val Loss = 0.008543\n",
            "Epoch 180: Train Loss = 0.003474, Val Loss = 0.007073\n",
            "Epoch 190: Train Loss = 0.016550, Val Loss = 0.006474\n",
            "Epoch 200: Train Loss = 0.000200, Val Loss = 0.003513\n",
            "Test MSE for config hd101_bs32_lr0.001: 0.002963\n",
            "\n",
            "=== Training config: hd101_bs32_lr0.005 ===\n",
            "Epoch 10: Train Loss = 0.073986, Val Loss = 0.173421\n",
            "Epoch 20: Train Loss = 0.040657, Val Loss = 0.179224\n",
            "Epoch 30: Train Loss = 0.012225, Val Loss = 0.205357\n",
            "Epoch 40: Train Loss = 0.060609, Val Loss = 0.178071\n",
            "Epoch 50: Train Loss = 0.073447, Val Loss = 0.184094\n",
            "Epoch 60: Train Loss = 0.004010, Val Loss = 0.181114\n",
            "Epoch 70: Train Loss = 0.363153, Val Loss = 0.172585\n",
            "Epoch 80: Train Loss = 0.063288, Val Loss = 0.178278\n",
            "Epoch 90: Train Loss = 0.278429, Val Loss = 0.171950\n",
            "Epoch 100: Train Loss = 0.118401, Val Loss = 0.172038\n",
            "Epoch 110: Train Loss = 0.185604, Val Loss = 0.177133\n",
            "Epoch 120: Train Loss = 0.135026, Val Loss = 0.172988\n",
            "Epoch 130: Train Loss = 0.002779, Val Loss = 0.172456\n",
            "Epoch 140: Train Loss = 0.006165, Val Loss = 0.175574\n",
            "Epoch 150: Train Loss = 0.091521, Val Loss = 0.172047\n",
            "Epoch 160: Train Loss = 0.610895, Val Loss = 0.175325\n",
            "Epoch 170: Train Loss = 0.022755, Val Loss = 0.172707\n",
            "Epoch 180: Train Loss = 0.243168, Val Loss = 0.172736\n",
            "Epoch 190: Train Loss = 0.037490, Val Loss = 0.178002\n",
            "Epoch 200: Train Loss = 0.288297, Val Loss = 0.171834\n",
            "Test MSE for config hd101_bs32_lr0.005: 0.144709\n",
            "\n",
            "=== Training config: hd101_bs32_lr0.01 ===\n",
            "Epoch 10: Train Loss = 0.218931, Val Loss = 0.241462\n",
            "Epoch 20: Train Loss = 0.012825, Val Loss = 0.172999\n",
            "Epoch 30: Train Loss = 0.062809, Val Loss = 0.179754\n",
            "Epoch 40: Train Loss = 0.500455, Val Loss = 0.172198\n",
            "Epoch 50: Train Loss = 0.274560, Val Loss = 0.180416\n",
            "Epoch 60: Train Loss = 0.193000, Val Loss = 0.174028\n",
            "Epoch 70: Train Loss = 0.474636, Val Loss = 0.179218\n",
            "Epoch 80: Train Loss = 0.004708, Val Loss = 0.181050\n",
            "Epoch 90: Train Loss = 0.334230, Val Loss = 0.174261\n",
            "Epoch 100: Train Loss = 0.000597, Val Loss = 0.192021\n",
            "Epoch 110: Train Loss = 0.092269, Val Loss = 0.184504\n",
            "Epoch 120: Train Loss = 0.001513, Val Loss = 0.172579\n",
            "Epoch 130: Train Loss = 0.179322, Val Loss = 0.173249\n",
            "Epoch 140: Train Loss = 0.169621, Val Loss = 0.200750\n",
            "Epoch 150: Train Loss = 0.358422, Val Loss = 0.186768\n",
            "Epoch 160: Train Loss = 0.004979, Val Loss = 0.174226\n",
            "Epoch 170: Train Loss = 0.527721, Val Loss = 0.172004\n",
            "Epoch 180: Train Loss = 0.185408, Val Loss = 0.177698\n",
            "Epoch 190: Train Loss = 0.315740, Val Loss = 0.177651\n",
            "Epoch 200: Train Loss = 0.175893, Val Loss = 0.173595\n",
            "Test MSE for config hd101_bs32_lr0.01: 0.144636\n",
            "\n",
            "=== Training config: hd101_bs64_lr0.001 ===\n",
            "Epoch 10: Train Loss = 0.002104, Val Loss = 0.024519\n",
            "Epoch 20: Train Loss = 0.003539, Val Loss = 0.023756\n",
            "Epoch 30: Train Loss = 0.000616, Val Loss = 0.015564\n",
            "Epoch 40: Train Loss = 0.031604, Val Loss = 0.026731\n",
            "Epoch 50: Train Loss = 0.002485, Val Loss = 0.013625\n",
            "Epoch 60: Train Loss = 0.012584, Val Loss = 0.012999\n",
            "Epoch 70: Train Loss = 0.000562, Val Loss = 0.008865\n",
            "Epoch 80: Train Loss = 0.000156, Val Loss = 0.008957\n",
            "Epoch 90: Train Loss = 0.000349, Val Loss = 0.006849\n",
            "Epoch 100: Train Loss = 0.000520, Val Loss = 0.006952\n",
            "Epoch 110: Train Loss = 0.001366, Val Loss = 0.006747\n",
            "Epoch 120: Train Loss = 0.001699, Val Loss = 0.005497\n",
            "Epoch 130: Train Loss = 0.000140, Val Loss = 0.005910\n",
            "Epoch 140: Train Loss = 0.000866, Val Loss = 0.012910\n",
            "Epoch 150: Train Loss = 0.013163, Val Loss = 0.019626\n",
            "Epoch 160: Train Loss = 0.003997, Val Loss = 0.006279\n",
            "Epoch 170: Train Loss = 0.001634, Val Loss = 0.005474\n",
            "Epoch 180: Train Loss = 0.000106, Val Loss = 0.005501\n",
            "Epoch 190: Train Loss = 0.000431, Val Loss = 0.004792\n",
            "Epoch 200: Train Loss = 0.000308, Val Loss = 0.005199\n",
            "Test MSE for config hd101_bs64_lr0.001: 0.002537\n",
            "\n",
            "=== Training config: hd101_bs64_lr0.005 ===\n",
            "Epoch 10: Train Loss = 0.334952, Val Loss = 0.169937\n",
            "Epoch 20: Train Loss = 0.157863, Val Loss = 0.169782\n",
            "Epoch 30: Train Loss = 0.192520, Val Loss = 0.184554\n",
            "Epoch 40: Train Loss = 0.534560, Val Loss = 0.178212\n",
            "Epoch 50: Train Loss = 0.206747, Val Loss = 0.172688\n",
            "Epoch 60: Train Loss = 0.223069, Val Loss = 0.173887\n",
            "Epoch 70: Train Loss = 0.211596, Val Loss = 0.171565\n",
            "Epoch 80: Train Loss = 0.008627, Val Loss = 0.169753\n",
            "Epoch 90: Train Loss = 0.033530, Val Loss = 0.172473\n",
            "Epoch 100: Train Loss = 0.006749, Val Loss = 0.169981\n",
            "Epoch 110: Train Loss = 0.017953, Val Loss = 0.169955\n",
            "Epoch 120: Train Loss = 0.205104, Val Loss = 0.190461\n",
            "Epoch 130: Train Loss = 0.143578, Val Loss = 0.175344\n",
            "Epoch 140: Train Loss = 0.041925, Val Loss = 0.171867\n",
            "Epoch 150: Train Loss = 0.116227, Val Loss = 0.173985\n",
            "Epoch 160: Train Loss = 0.264522, Val Loss = 0.170068\n",
            "Epoch 170: Train Loss = 0.190343, Val Loss = 0.171896\n",
            "Epoch 180: Train Loss = 0.431311, Val Loss = 0.169987\n",
            "Epoch 190: Train Loss = 0.001704, Val Loss = 0.170305\n",
            "Epoch 200: Train Loss = 0.168690, Val Loss = 0.178025\n",
            "Test MSE for config hd101_bs64_lr0.005: 0.144622\n",
            "\n",
            "=== Training config: hd101_bs64_lr0.01 ===\n",
            "Epoch 10: Train Loss = 0.046849, Val Loss = 0.183098\n",
            "Epoch 20: Train Loss = 0.115585, Val Loss = 0.189165\n",
            "Epoch 30: Train Loss = 0.380102, Val Loss = 0.188436\n",
            "Epoch 40: Train Loss = 0.385356, Val Loss = 0.198557\n",
            "Epoch 50: Train Loss = 0.012269, Val Loss = 0.199379\n",
            "Epoch 60: Train Loss = 0.231391, Val Loss = 0.201328\n",
            "Epoch 70: Train Loss = 0.022227, Val Loss = 0.170344\n",
            "Epoch 80: Train Loss = 0.074275, Val Loss = 0.173055\n",
            "Epoch 90: Train Loss = 0.283180, Val Loss = 0.172053\n",
            "Epoch 100: Train Loss = 0.005666, Val Loss = 0.177507\n",
            "Epoch 110: Train Loss = 0.036387, Val Loss = 0.183126\n",
            "Epoch 120: Train Loss = 0.005522, Val Loss = 0.191840\n",
            "Epoch 130: Train Loss = 0.090644, Val Loss = 0.180961\n",
            "Epoch 140: Train Loss = 0.005087, Val Loss = 0.178152\n",
            "Epoch 150: Train Loss = 0.001711, Val Loss = 0.176910\n",
            "Epoch 160: Train Loss = 0.056003, Val Loss = 0.178326\n",
            "Epoch 170: Train Loss = 0.173127, Val Loss = 0.190069\n",
            "Epoch 180: Train Loss = 0.020340, Val Loss = 0.169656\n",
            "Epoch 190: Train Loss = 0.234983, Val Loss = 0.177254\n",
            "Epoch 200: Train Loss = 0.053926, Val Loss = 0.170242\n",
            "Test MSE for config hd101_bs64_lr0.01: 0.144740\n",
            "\n",
            "=== Training config: hd101_bs128_lr0.001 ===\n",
            "Epoch 10: Train Loss = 0.101114, Val Loss = 0.117130\n",
            "Epoch 20: Train Loss = 0.015029, Val Loss = 0.021037\n",
            "Epoch 30: Train Loss = 0.013953, Val Loss = 0.019588\n",
            "Epoch 40: Train Loss = 0.016112, Val Loss = 0.019649\n",
            "Epoch 50: Train Loss = 0.009844, Val Loss = 0.016186\n",
            "Epoch 60: Train Loss = 0.009080, Val Loss = 0.015180\n",
            "Epoch 70: Train Loss = 0.014905, Val Loss = 0.013225\n",
            "Epoch 80: Train Loss = 0.012479, Val Loss = 0.012152\n",
            "Epoch 90: Train Loss = 0.008800, Val Loss = 0.011797\n",
            "Epoch 100: Train Loss = 0.006738, Val Loss = 0.009339\n",
            "Epoch 110: Train Loss = 0.003467, Val Loss = 0.008313\n",
            "Epoch 120: Train Loss = 0.003481, Val Loss = 0.006158\n",
            "Epoch 130: Train Loss = 0.002508, Val Loss = 0.009309\n",
            "Epoch 140: Train Loss = 0.001286, Val Loss = 0.005006\n",
            "Epoch 150: Train Loss = 0.004532, Val Loss = 0.005586\n",
            "Epoch 160: Train Loss = 0.002239, Val Loss = 0.004765\n",
            "Epoch 170: Train Loss = 0.001783, Val Loss = 0.005434\n",
            "Epoch 180: Train Loss = 0.002982, Val Loss = 0.004604\n",
            "Epoch 190: Train Loss = 0.000902, Val Loss = 0.004506\n",
            "Epoch 200: Train Loss = 0.001954, Val Loss = 0.004463\n",
            "Test MSE for config hd101_bs128_lr0.001: 0.003034\n",
            "\n",
            "=== Training config: hd101_bs128_lr0.005 ===\n",
            "Epoch 10: Train Loss = 0.184498, Val Loss = 0.165534\n",
            "Epoch 20: Train Loss = 0.125508, Val Loss = 0.177723\n",
            "Epoch 30: Train Loss = 0.155324, Val Loss = 0.166270\n",
            "Epoch 40: Train Loss = 0.154375, Val Loss = 0.224267\n",
            "Epoch 50: Train Loss = 0.136349, Val Loss = 0.168295\n",
            "Epoch 60: Train Loss = 0.176337, Val Loss = 0.165929\n",
            "Epoch 70: Train Loss = 0.131040, Val Loss = 0.190141\n",
            "Epoch 80: Train Loss = 0.129663, Val Loss = 0.166754\n",
            "Epoch 90: Train Loss = 0.143257, Val Loss = 0.167364\n",
            "Epoch 100: Train Loss = 0.161527, Val Loss = 0.168484\n",
            "Epoch 110: Train Loss = 0.208977, Val Loss = 0.165691\n",
            "Epoch 120: Train Loss = 0.185165, Val Loss = 0.169810\n",
            "Epoch 130: Train Loss = 0.128418, Val Loss = 0.167032\n",
            "Epoch 140: Train Loss = 0.142244, Val Loss = 0.165937\n",
            "Epoch 150: Train Loss = 0.111602, Val Loss = 0.165895\n",
            "Epoch 160: Train Loss = 0.175425, Val Loss = 0.168138\n",
            "Epoch 170: Train Loss = 0.185893, Val Loss = 0.166515\n",
            "Epoch 180: Train Loss = 0.211466, Val Loss = 0.169543\n",
            "Epoch 190: Train Loss = 0.140453, Val Loss = 0.167361\n",
            "Epoch 200: Train Loss = 0.170031, Val Loss = 0.169646\n",
            "Test MSE for config hd101_bs128_lr0.005: 0.144739\n",
            "\n",
            "=== Training config: hd101_bs128_lr0.01 ===\n",
            "Epoch 10: Train Loss = 0.138040, Val Loss = 0.174241\n",
            "Epoch 20: Train Loss = 0.136945, Val Loss = 0.189179\n",
            "Epoch 30: Train Loss = 0.196614, Val Loss = 0.171642\n",
            "Epoch 40: Train Loss = 0.169991, Val Loss = 0.176547\n",
            "Epoch 50: Train Loss = 0.145072, Val Loss = 0.178738\n",
            "Epoch 60: Train Loss = 0.179286, Val Loss = 0.165633\n",
            "Epoch 70: Train Loss = 0.131033, Val Loss = 0.178644\n",
            "Epoch 80: Train Loss = 0.159590, Val Loss = 0.165621\n",
            "Epoch 90: Train Loss = 0.151908, Val Loss = 0.168368\n",
            "Epoch 100: Train Loss = 0.209392, Val Loss = 0.169640\n",
            "Epoch 110: Train Loss = 0.173164, Val Loss = 0.189468\n",
            "Epoch 120: Train Loss = 0.148818, Val Loss = 0.178888\n",
            "Epoch 130: Train Loss = 0.124883, Val Loss = 0.173559\n",
            "Epoch 140: Train Loss = 0.233184, Val Loss = 0.172342\n",
            "Epoch 150: Train Loss = 0.142642, Val Loss = 0.166305\n",
            "Epoch 160: Train Loss = 0.147359, Val Loss = 0.165487\n",
            "Epoch 170: Train Loss = 0.128534, Val Loss = 0.165504\n",
            "Epoch 180: Train Loss = 0.125640, Val Loss = 0.166412\n",
            "Epoch 190: Train Loss = 0.168260, Val Loss = 0.166210\n",
            "Epoch 200: Train Loss = 0.107813, Val Loss = 0.166989\n",
            "Test MSE for config hd101_bs128_lr0.01: 0.144719\n",
            "\n",
            "=== Training config: hd200_bs32_lr0.001 ===\n",
            "Epoch 10: Train Loss = 0.001869, Val Loss = 0.019742\n",
            "Epoch 20: Train Loss = 0.001363, Val Loss = 0.011850\n",
            "Epoch 30: Train Loss = 0.006297, Val Loss = 0.009615\n",
            "Epoch 40: Train Loss = 0.000383, Val Loss = 0.008563\n",
            "Epoch 50: Train Loss = 0.001746, Val Loss = 0.008120\n",
            "Epoch 60: Train Loss = 0.005668, Val Loss = 0.009711\n",
            "Epoch 70: Train Loss = 0.015128, Val Loss = 0.029825\n",
            "Epoch 80: Train Loss = 0.002550, Val Loss = 0.006003\n",
            "Epoch 90: Train Loss = 0.004820, Val Loss = 0.007851\n",
            "Epoch 100: Train Loss = 0.003164, Val Loss = 0.006642\n",
            "Epoch 110: Train Loss = 0.000537, Val Loss = 0.006963\n",
            "Epoch 120: Train Loss = 0.029518, Val Loss = 0.039962\n",
            "Epoch 130: Train Loss = 0.000669, Val Loss = 0.004296\n",
            "Epoch 140: Train Loss = 0.003883, Val Loss = 0.008208\n",
            "Epoch 150: Train Loss = 0.000906, Val Loss = 0.004753\n",
            "Epoch 160: Train Loss = 0.001049, Val Loss = 0.004494\n",
            "Epoch 170: Train Loss = 0.002283, Val Loss = 0.003976\n",
            "Epoch 180: Train Loss = 0.000392, Val Loss = 0.004094\n",
            "Epoch 190: Train Loss = 0.002160, Val Loss = 0.009149\n",
            "Epoch 200: Train Loss = 0.001417, Val Loss = 0.008588\n",
            "Test MSE for config hd200_bs32_lr0.001: 0.002138\n",
            "\n",
            "=== Training config: hd200_bs32_lr0.005 ===\n",
            "Epoch 10: Train Loss = 0.007260, Val Loss = 0.178327\n",
            "Epoch 20: Train Loss = 0.029247, Val Loss = 0.172476\n",
            "Epoch 30: Train Loss = 0.301089, Val Loss = 0.188354\n",
            "Epoch 40: Train Loss = 0.054133, Val Loss = 0.173642\n",
            "Epoch 50: Train Loss = 0.031514, Val Loss = 0.171783\n",
            "Epoch 60: Train Loss = 0.080903, Val Loss = 0.171769\n",
            "Epoch 70: Train Loss = 0.121224, Val Loss = 0.183218\n",
            "Epoch 80: Train Loss = 0.369510, Val Loss = 0.172898\n",
            "Epoch 90: Train Loss = 0.061603, Val Loss = 0.173435\n",
            "Epoch 100: Train Loss = 0.368243, Val Loss = 0.190044\n",
            "Epoch 110: Train Loss = 0.260075, Val Loss = 0.179490\n",
            "Epoch 120: Train Loss = 0.194682, Val Loss = 0.172099\n",
            "Epoch 130: Train Loss = 0.046144, Val Loss = 0.182965\n",
            "Epoch 140: Train Loss = 0.189125, Val Loss = 0.173665\n",
            "Epoch 150: Train Loss = 0.156842, Val Loss = 0.179438\n",
            "Epoch 160: Train Loss = 0.005401, Val Loss = 0.178415\n",
            "Epoch 170: Train Loss = 0.029736, Val Loss = 0.185987\n",
            "Epoch 180: Train Loss = 0.022692, Val Loss = 0.179892\n",
            "Epoch 190: Train Loss = 0.000367, Val Loss = 0.175930\n",
            "Epoch 200: Train Loss = 0.395821, Val Loss = 0.173754\n",
            "Test MSE for config hd200_bs32_lr0.005: 0.144646\n",
            "\n",
            "=== Training config: hd200_bs32_lr0.01 ===\n",
            "Epoch 10: Train Loss = 0.331570, Val Loss = 0.300543\n",
            "Epoch 20: Train Loss = 0.449609, Val Loss = 0.225166\n",
            "Epoch 30: Train Loss = 0.206421, Val Loss = 0.180274\n",
            "Epoch 40: Train Loss = 0.028236, Val Loss = 0.186170\n",
            "Epoch 50: Train Loss = 0.228541, Val Loss = 0.177746\n",
            "Epoch 60: Train Loss = 0.172784, Val Loss = 0.172357\n",
            "Epoch 70: Train Loss = 0.005690, Val Loss = 0.176372\n",
            "Epoch 80: Train Loss = 0.152649, Val Loss = 0.184515\n",
            "Epoch 90: Train Loss = 0.463710, Val Loss = 0.172201\n",
            "Epoch 100: Train Loss = 0.026202, Val Loss = 0.173624\n",
            "Epoch 110: Train Loss = 0.076016, Val Loss = 0.172812\n",
            "Epoch 120: Train Loss = 0.051091, Val Loss = 0.173931\n",
            "Epoch 130: Train Loss = 0.122250, Val Loss = 0.184035\n",
            "Epoch 140: Train Loss = 1.406108, Val Loss = 0.185186\n",
            "Epoch 150: Train Loss = 0.209494, Val Loss = 0.172024\n",
            "Epoch 160: Train Loss = 0.151211, Val Loss = 0.173098\n",
            "Epoch 170: Train Loss = 0.017282, Val Loss = 0.172212\n",
            "Epoch 180: Train Loss = 0.047362, Val Loss = 0.172490\n",
            "Epoch 190: Train Loss = 0.101570, Val Loss = 0.173152\n",
            "Epoch 200: Train Loss = 0.043899, Val Loss = 0.173869\n",
            "Test MSE for config hd200_bs32_lr0.01: 0.144598\n",
            "\n",
            "=== Training config: hd200_bs64_lr0.001 ===\n",
            "Epoch 10: Train Loss = 0.054159, Val Loss = 0.055763\n",
            "Epoch 20: Train Loss = 0.003938, Val Loss = 0.029001\n",
            "Epoch 30: Train Loss = 0.001477, Val Loss = 0.019522\n",
            "Epoch 40: Train Loss = 0.001956, Val Loss = 0.022555\n",
            "Epoch 50: Train Loss = 0.000410, Val Loss = 0.012139\n",
            "Epoch 60: Train Loss = 0.002277, Val Loss = 0.010289\n",
            "Epoch 70: Train Loss = 0.060774, Val Loss = 0.069621\n",
            "Epoch 80: Train Loss = 0.000398, Val Loss = 0.008442\n",
            "Epoch 90: Train Loss = 0.000652, Val Loss = 0.007670\n",
            "Epoch 100: Train Loss = 0.001020, Val Loss = 0.006649\n",
            "Epoch 110: Train Loss = 0.000137, Val Loss = 0.011605\n",
            "Epoch 120: Train Loss = 0.000485, Val Loss = 0.007544\n",
            "Epoch 130: Train Loss = 0.001125, Val Loss = 0.009129\n",
            "Epoch 140: Train Loss = 0.000656, Val Loss = 0.006208\n",
            "Epoch 150: Train Loss = 0.002743, Val Loss = 0.008818\n",
            "Epoch 160: Train Loss = 0.000168, Val Loss = 0.004843\n",
            "Epoch 170: Train Loss = 0.000099, Val Loss = 0.004586\n",
            "Epoch 180: Train Loss = 0.001672, Val Loss = 0.009137\n",
            "Epoch 190: Train Loss = 0.007567, Val Loss = 0.006359\n",
            "Epoch 200: Train Loss = 0.002813, Val Loss = 0.006082\n",
            "Test MSE for config hd200_bs64_lr0.001: 0.002441\n",
            "\n",
            "=== Training config: hd200_bs64_lr0.005 ===\n",
            "Epoch 10: Train Loss = 0.212809, Val Loss = 0.235818\n",
            "Epoch 20: Train Loss = 0.007283, Val Loss = 0.214417\n",
            "Epoch 30: Train Loss = 0.111919, Val Loss = 0.185972\n",
            "Epoch 40: Train Loss = 0.005537, Val Loss = 0.261569\n",
            "Epoch 50: Train Loss = 0.065041, Val Loss = 0.169754\n",
            "Epoch 60: Train Loss = 0.069125, Val Loss = 0.178277\n",
            "Epoch 70: Train Loss = 0.454954, Val Loss = 0.177608\n",
            "Epoch 80: Train Loss = 0.062405, Val Loss = 0.170836\n",
            "Epoch 90: Train Loss = 0.056195, Val Loss = 0.177588\n",
            "Epoch 100: Train Loss = 0.002675, Val Loss = 0.179111\n",
            "Epoch 110: Train Loss = 0.223079, Val Loss = 0.173851\n",
            "Epoch 120: Train Loss = 0.052892, Val Loss = 0.169949\n",
            "Epoch 130: Train Loss = 0.271247, Val Loss = 0.179088\n",
            "Epoch 140: Train Loss = 0.076998, Val Loss = 0.170059\n",
            "Epoch 150: Train Loss = 0.047805, Val Loss = 0.170871\n",
            "Epoch 160: Train Loss = 0.218481, Val Loss = 0.176319\n",
            "Epoch 170: Train Loss = 0.008612, Val Loss = 0.179009\n",
            "Epoch 180: Train Loss = 0.339847, Val Loss = 0.169919\n",
            "Epoch 190: Train Loss = 0.055450, Val Loss = 0.170288\n",
            "Epoch 200: Train Loss = 0.010897, Val Loss = 0.176829\n",
            "Test MSE for config hd200_bs64_lr0.005: 0.144585\n",
            "\n",
            "=== Training config: hd200_bs64_lr0.01 ===\n",
            "Epoch 10: Train Loss = 0.042889, Val Loss = 0.170097\n",
            "Epoch 20: Train Loss = 0.012482, Val Loss = 0.170046\n",
            "Epoch 30: Train Loss = 0.036247, Val Loss = 0.201989\n",
            "Epoch 40: Train Loss = 0.127581, Val Loss = 0.179948\n",
            "Epoch 50: Train Loss = 0.013728, Val Loss = 0.200212\n",
            "Epoch 60: Train Loss = 0.330486, Val Loss = 0.178249\n",
            "Epoch 70: Train Loss = 0.017906, Val Loss = 0.172034\n",
            "Epoch 80: Train Loss = 0.227441, Val Loss = 0.210209\n",
            "Epoch 90: Train Loss = 0.281591, Val Loss = 0.183117\n",
            "Epoch 100: Train Loss = 0.106994, Val Loss = 0.169901\n",
            "Epoch 110: Train Loss = 0.018274, Val Loss = 0.170327\n",
            "Epoch 120: Train Loss = 0.014731, Val Loss = 0.187816\n",
            "Epoch 130: Train Loss = 0.040888, Val Loss = 0.172153\n",
            "Epoch 140: Train Loss = 0.002439, Val Loss = 0.204087\n",
            "Epoch 150: Train Loss = 0.004699, Val Loss = 0.170212\n",
            "Epoch 160: Train Loss = 0.319904, Val Loss = 0.173205\n",
            "Epoch 170: Train Loss = 0.053002, Val Loss = 0.173415\n",
            "Epoch 180: Train Loss = 0.162463, Val Loss = 0.186076\n",
            "Epoch 190: Train Loss = 0.163167, Val Loss = 0.174109\n",
            "Epoch 200: Train Loss = 0.014030, Val Loss = 0.170922\n",
            "Test MSE for config hd200_bs64_lr0.01: 0.144677\n",
            "\n",
            "=== Training config: hd200_bs128_lr0.001 ===\n",
            "Epoch 10: Train Loss = 0.011983, Val Loss = 0.021884\n",
            "Epoch 20: Train Loss = 0.026369, Val Loss = 0.019485\n",
            "Epoch 30: Train Loss = 0.009056, Val Loss = 0.016993\n",
            "Epoch 40: Train Loss = 0.014021, Val Loss = 0.014231\n",
            "Epoch 50: Train Loss = 0.006110, Val Loss = 0.011056\n",
            "Epoch 60: Train Loss = 0.008445, Val Loss = 0.007786\n",
            "Epoch 70: Train Loss = 0.005220, Val Loss = 0.007708\n",
            "Epoch 80: Train Loss = 0.003317, Val Loss = 0.005165\n",
            "Epoch 90: Train Loss = 0.002195, Val Loss = 0.005440\n",
            "Epoch 100: Train Loss = 0.002204, Val Loss = 0.004452\n",
            "Epoch 110: Train Loss = 0.001988, Val Loss = 0.004202\n",
            "Epoch 120: Train Loss = 0.004526, Val Loss = 0.006061\n",
            "Epoch 130: Train Loss = 0.001613, Val Loss = 0.004897\n",
            "Epoch 140: Train Loss = 0.002707, Val Loss = 0.005655\n",
            "Epoch 150: Train Loss = 0.001908, Val Loss = 0.004832\n",
            "Epoch 160: Train Loss = 0.001084, Val Loss = 0.004072\n",
            "Epoch 170: Train Loss = 0.001148, Val Loss = 0.003609\n",
            "Epoch 180: Train Loss = 0.004240, Val Loss = 0.004221\n",
            "Epoch 190: Train Loss = 0.001871, Val Loss = 0.003946\n",
            "Epoch 200: Train Loss = 0.000839, Val Loss = 0.003471\n",
            "Test MSE for config hd200_bs128_lr0.001: 0.002678\n",
            "\n",
            "=== Training config: hd200_bs128_lr0.005 ===\n",
            "Epoch 10: Train Loss = 0.172455, Val Loss = 0.169753\n",
            "Epoch 20: Train Loss = 0.144482, Val Loss = 0.197653\n",
            "Epoch 30: Train Loss = 0.141732, Val Loss = 0.165685\n",
            "Epoch 40: Train Loss = 0.209075, Val Loss = 0.166657\n",
            "Epoch 50: Train Loss = 0.176761, Val Loss = 0.166592\n",
            "Epoch 60: Train Loss = 0.136875, Val Loss = 0.166909\n",
            "Epoch 70: Train Loss = 0.150351, Val Loss = 0.166649\n",
            "Epoch 80: Train Loss = 0.131700, Val Loss = 0.168274\n",
            "Epoch 90: Train Loss = 0.183840, Val Loss = 0.167561\n",
            "Epoch 100: Train Loss = 0.149729, Val Loss = 0.166780\n",
            "Epoch 110: Train Loss = 0.156874, Val Loss = 0.169901\n",
            "Epoch 120: Train Loss = 0.164687, Val Loss = 0.170900\n",
            "Epoch 130: Train Loss = 0.166487, Val Loss = 0.185161\n",
            "Epoch 140: Train Loss = 0.145833, Val Loss = 0.165793\n",
            "Epoch 150: Train Loss = 0.156771, Val Loss = 0.165608\n",
            "Epoch 160: Train Loss = 0.158305, Val Loss = 0.192019\n",
            "Epoch 170: Train Loss = 0.175915, Val Loss = 0.174190\n",
            "Epoch 180: Train Loss = 0.179989, Val Loss = 0.168120\n",
            "Epoch 190: Train Loss = 0.130296, Val Loss = 0.165527\n",
            "Epoch 200: Train Loss = 0.150478, Val Loss = 0.168769\n",
            "Test MSE for config hd200_bs128_lr0.005: 0.144723\n",
            "\n",
            "=== Training config: hd200_bs128_lr0.01 ===\n",
            "Epoch 10: Train Loss = 0.134904, Val Loss = 0.183034\n",
            "Epoch 20: Train Loss = 0.175072, Val Loss = 0.166377\n",
            "Epoch 30: Train Loss = 0.135478, Val Loss = 0.168966\n",
            "Epoch 40: Train Loss = 0.174953, Val Loss = 0.165751\n",
            "Epoch 50: Train Loss = 0.177123, Val Loss = 0.185942\n",
            "Epoch 60: Train Loss = 0.175239, Val Loss = 0.170702\n",
            "Epoch 70: Train Loss = 0.146756, Val Loss = 0.167944\n",
            "Epoch 80: Train Loss = 0.131328, Val Loss = 0.165495\n",
            "Epoch 90: Train Loss = 0.158318, Val Loss = 0.169458\n",
            "Epoch 100: Train Loss = 0.179003, Val Loss = 0.165655\n",
            "Epoch 110: Train Loss = 0.174217, Val Loss = 0.171456\n",
            "Epoch 120: Train Loss = 0.163757, Val Loss = 0.165770\n",
            "Epoch 130: Train Loss = 0.148816, Val Loss = 0.166636\n",
            "Epoch 140: Train Loss = 0.152530, Val Loss = 0.172781\n",
            "Epoch 150: Train Loss = 0.144954, Val Loss = 0.179485\n",
            "Epoch 160: Train Loss = 0.163529, Val Loss = 0.167025\n",
            "Epoch 170: Train Loss = 0.149186, Val Loss = 0.165554\n",
            "Epoch 180: Train Loss = 0.240004, Val Loss = 0.183081\n",
            "Epoch 190: Train Loss = 0.183395, Val Loss = 0.167031\n",
            "Epoch 200: Train Loss = 0.173664, Val Loss = 0.179113\n",
            "Test MSE for config hd200_bs128_lr0.01: 0.144597\n",
            "\n",
            "✅ All results saved to DSSM_Advanced_gridsearch_results.csv\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from itertools import product\n",
        "\n",
        "# Output path\n",
        "drive_path = '/content/drive/MyDrive/DSSM-Figures'\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "# Grid search hyperparameters\n",
        "hidden_dims = [50, 101, 200]\n",
        "batch_sizes = [32, 64, 128]\n",
        "learning_rates = [0.001, 0.005, 0.01]\n",
        "\n",
        "# Result storage\n",
        "DSSM_Advanced_mse = pd.DataFrame(columns=['hidden_dim', 'batch_size', 'learning_rate', 'Test_MSE'])\n",
        "\n",
        "# Data split setup (only one split used here)\n",
        "splits = [(20, 80)]\n",
        "\n",
        "for train_pct, test_pct in splits:\n",
        "    file_ids = df_output['file_id'].unique()\n",
        "    trainval_ids, test_ids = train_test_split(file_ids, test_size=0.2, random_state=42)\n",
        "    train_ids, val_ids = train_test_split(trainval_ids, test_size=0.2, random_state=42)\n",
        "\n",
        "    def extract_X_Y(ids, pct):\n",
        "        df_subset = df_output[df_output['file_id'].isin(ids)]\n",
        "        pivoted = df_subset.pivot(index='file_id', columns='timestep', values='CO2').values\n",
        "        split_idx = int(pct / 100 * 101)\n",
        "        X = pivoted[:, :split_idx]\n",
        "        Y = pivoted[:, split_idx:]\n",
        "        return X, Y\n",
        "\n",
        "    X_train, Y_train = extract_X_Y(train_ids, train_pct)\n",
        "    X_val, Y_val = extract_X_Y(val_ids, train_pct)\n",
        "    X_test, Y_test = extract_X_Y(test_ids, train_pct)\n",
        "\n",
        "    static_train = merged_df[merged_df['file_id'].isin(train_ids)].drop(columns=['file_id', 'cluster']).values\n",
        "    static_val = merged_df[merged_df['file_id'].isin(val_ids)].drop(columns=['file_id', 'cluster']).values\n",
        "    static_test = merged_df[merged_df['file_id'].isin(test_ids)].drop(columns=['file_id', 'cluster']).values\n",
        "\n",
        "    # Convert to tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
        "    static_train_tensor = torch.tensor(static_train, dtype=torch.float32)\n",
        "\n",
        "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "    Y_val_tensor = torch.tensor(Y_val, dtype=torch.float32)\n",
        "    static_val_tensor = torch.tensor(static_val, dtype=torch.float32)\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
        "    static_test_tensor = torch.tensor(static_test, dtype=torch.float32)\n",
        "\n",
        "    # Grid search loop\n",
        "    for hidden_dim, batch_size, learning_rate in product(hidden_dims, batch_sizes, learning_rates):\n",
        "        config_name = f\"hd{hidden_dim}_bs{batch_size}_lr{learning_rate}\"\n",
        "        print(f\"\\n=== Training config: {config_name} ===\")\n",
        "\n",
        "        model = AdvancedDSSMDeepState(\n",
        "            input_dim=X_train.shape[1],\n",
        "            static_dim=static_train.shape[1],\n",
        "            hidden_dim=hidden_dim,\n",
        "            output_dim=Y_train.shape[1]\n",
        "        )\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "        train_loader = DataLoader(TensorDataset(X_train_tensor, static_train_tensor, Y_train_tensor),\n",
        "                                  batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(TensorDataset(X_val_tensor, static_val_tensor, Y_val_tensor),\n",
        "                                batch_size=batch_size)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_model_state = None\n",
        "\n",
        "        for epoch in range(200):\n",
        "            model.train()\n",
        "            for X_batch, static_batch, Y_batch in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                preds = model(X_batch, static_batch)\n",
        "                loss = criterion(preds, Y_batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for X_batch, static_batch, Y_batch in val_loader:\n",
        "                    preds = model(X_batch, static_batch)\n",
        "                    val_loss += criterion(preds, Y_batch).item()\n",
        "                val_loss /= len(val_loader)\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model_state = model.state_dict()\n",
        "                torch.save(best_model_state, os.path.join(drive_path, f\"best_model_{config_name}.pt\"))\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}: Train Loss = {loss.item():.6f}, Val Loss = {val_loss:.6f}\")\n",
        "\n",
        "        # Final test loss\n",
        "        model.load_state_dict(torch.load(os.path.join(drive_path, f\"best_model_{config_name}.pt\")))\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_preds = model(X_test_tensor, static_test_tensor)\n",
        "            test_loss = criterion(test_preds, Y_test_tensor).item()\n",
        "\n",
        "        DSSM_Advanced_mse.loc[len(DSSM_Advanced_mse)] = [hidden_dim, batch_size, learning_rate, test_loss]\n",
        "        print(f\"Test MSE for config {config_name}: {test_loss:.6f}\")\n",
        "\n",
        "# Save all results\n",
        "DSSM_Advanced_mse.to_csv(os.path.join(drive_path, \"DSSM_Advanced_gridsearch_results.csv\"), index=False)\n",
        "print(f\"\\n✅ All results saved to DSSM_Advanced_gridsearch_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1ICMuQZl5bv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}