{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data preperation"
      ],
      "metadata": {
        "id": "51TiCzzrTQsy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vLWAmLUHkWOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3d0665-bd69-4d6c-ca15-0d21aa86d67a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive and loading dataset...\n",
            "Mounted at /content/drive\n",
            "Loaded dataset with shape: (1192157, 17)\n",
            "Static feature table created: Input_Link_Table.shape = (2703, 15)\n",
            "Time series matrix constructed: relevant_data.shape = (2703, 101)\n",
            "Performed KMeans clustering into 8 clusters\n",
            "Cluster boundary stats calculated: cluster_boundaries.shape = (8, 4, 101)\n",
            "Final input features (static + cluster): merged_df.shape = (2703, 16)\n",
            "Final output time series: df_output.shape = (273003, 3)\n",
            "Data Preparation Summary:\n",
            "Static Input Table: merged_df [2703 rows × 16 columns]\n",
            "Time Series Output: df_output [273003 rows × 3 columns]\n",
            "Cluster Boundaries: cluster_boundaries [(8, 4, 101)]\n"
          ]
        }
      ],
      "source": [
        "# --------------------- Imports ---------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from google.colab import drive\n",
        "\n",
        "# --------------------- Matplotlib Setup ---------------------\n",
        "mpl.rcParams.update({\n",
        "    'font.size': 14,\n",
        "    'axes.titlesize': 15,\n",
        "    'axes.labelsize': 12,\n",
        "    'xtick.labelsize': 11,\n",
        "    'ytick.labelsize': 11,\n",
        "    'legend.fontsize': 11,\n",
        "    'figure.dpi': 300,\n",
        "    'savefig.dpi': 300,\n",
        "    'figure.autolayout': True,\n",
        "})\n",
        "\n",
        "# --------------------- Load Data ---------------------\n",
        "print(\"Mounting Google Drive and loading dataset...\")\n",
        "drive.mount('/content/drive')\n",
        "total_capture_7k = pd.read_csv('drive/My Drive/correlation_wide.csv')\n",
        "print(f\"Loaded dataset with shape: {total_capture_7k.shape}\")\n",
        "\n",
        "# --------------------- Identify Unique Static Parameter Sets ---------------------\n",
        "static_cols = [\n",
        "    'MikeSorghum', 'Quartz', 'Plagioclase', 'Apatite', 'Ilmenite',\n",
        "    'Diopside_Mn', 'Diopside', 'Olivine', 'Alkali-feldspar',\n",
        "    'Montmorillonite', 'Glass', 'temp', 'shift', 'year'\n",
        "]\n",
        "\n",
        "# Add timestep count per file_id\n",
        "file_lengths = total_capture_7k.groupby('file_id').size().rename(\"num_timesteps\").reset_index()\n",
        "static_rows = total_capture_7k.groupby('file_id')[static_cols].first().reset_index()\n",
        "static_rows = static_rows.merge(file_lengths, on='file_id')\n",
        "\n",
        "# Filter only unique static parameter sets\n",
        "unique_static_rows = static_rows.drop_duplicates(subset=static_cols)\n",
        "unique_file_ids = unique_static_rows['file_id'].tolist()\n",
        "\n",
        "# --------------------- Extract Time Series Data ---------------------\n",
        "filtered_df = total_capture_7k[total_capture_7k['file_id'].isin(unique_file_ids)].copy()\n",
        "\n",
        "# Truncate each group to 101 timesteps\n",
        "filtered_df = filtered_df.groupby('file_id').head(101).reset_index(drop=True)\n",
        "\n",
        "# --------------------- Static Feature Table ---------------------\n",
        "Input_Link_Table = filtered_df.groupby('file_id').agg({col: 'first' for col in static_cols}).reset_index()\n",
        "print(f\"Static feature table created: Input_Link_Table.shape = {Input_Link_Table.shape}\")\n",
        "\n",
        "# --------------------- Time Series Structuring ---------------------\n",
        "result = filtered_df[['Total_CO2_capture', 'year', 'file_id']]\n",
        "file_ids = result['file_id'].unique()\n",
        "num_file_ids = len(file_ids)\n",
        "max_timesteps = 101\n",
        "relevant_data = np.zeros((num_file_ids, max_timesteps))\n",
        "file_id_order = np.zeros(num_file_ids)\n",
        "\n",
        "for i, file_id in enumerate(file_ids):\n",
        "    file_data = result[result['file_id'] == file_id]['Total_CO2_capture'].values\n",
        "    relevant_data[i, :len(file_data)] = file_data\n",
        "    file_id_order[i] = file_id\n",
        "print(f\"Time series matrix constructed: relevant_data.shape = {relevant_data.shape}\")\n",
        "\n",
        "# --------------------- Clustering ---------------------\n",
        "scaler = StandardScaler()\n",
        "normalized_data = scaler.fit_transform(relevant_data)\n",
        "kmeans = KMeans(n_clusters=8, random_state=42)\n",
        "clusters = kmeans.fit_predict(normalized_data)\n",
        "print(\"Performed KMeans clustering into 8 clusters\")\n",
        "\n",
        "# Compute boundary stats\n",
        "cluster_boundaries = []\n",
        "for cluster_id in range(8):\n",
        "    cluster_data = normalized_data[clusters == cluster_id]\n",
        "    min_v = scaler.inverse_transform(np.min(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    median_v = scaler.inverse_transform(np.median(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    mean_v = scaler.inverse_transform(np.mean(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    max_v = scaler.inverse_transform(np.max(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    cluster_boundaries.append((min_v, median_v, mean_v, max_v))\n",
        "cluster_boundaries = np.array(cluster_boundaries)\n",
        "print(f\"Cluster boundary stats calculated: cluster_boundaries.shape = {cluster_boundaries.shape}\")\n",
        "\n",
        "# --------------------- Merge Static Features with Clusters ---------------------\n",
        "Clustering_link_table = pd.DataFrame({'file_id': file_id_order.astype(int), 'cluster': clusters})\n",
        "Clustering_link_table = Clustering_link_table.sort_values(by='file_id').reset_index(drop=True)\n",
        "merged_df = pd.merge(Input_Link_Table, Clustering_link_table, on='file_id')\n",
        "print(f\"Final input features (static + cluster): merged_df.shape = {merged_df.shape}\")\n",
        "\n",
        "# --------------------- Create Output Time Series DataFrame ---------------------\n",
        "data = [[file_id_order[i].astype(int), t, relevant_data[i, t]] for i in range(len(file_id_order)) for t in range(max_timesteps)]\n",
        "df_output = pd.DataFrame(data, columns=['file_id', 'timestep', 'CO2']).sort_values(by=['file_id', 'timestep'])\n",
        "print(f\"Final output time series: df_output.shape = {df_output.shape}\")\n",
        "\n",
        "# --------------------- Summary ---------------------\n",
        "print(\"Data Preparation Summary:\")\n",
        "print(f\"Static Input Table: merged_df [{merged_df.shape[0]} rows × {merged_df.shape[1]} columns]\")\n",
        "print(f\"Time Series Output: df_output [{df_output.shape[0]} rows × 3 columns]\")\n",
        "print(f\"Cluster Boundaries: cluster_boundaries [{cluster_boundaries.shape}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model definition"
      ],
      "metadata": {
        "id": "566fz9VtnDNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Definition\n",
        "class AdvancedDSSMDeepState(nn.Module):\n",
        "    def __init__(self, input_dim, static_dim, hidden_dim, output_dim):\n",
        "        super(AdvancedDSSMDeepState, self).__init__()\n",
        "\n",
        "        # Static Data Path (Fully connected layers for static features)\n",
        "        self.fc_static1 = nn.Linear(static_dim, 512)\n",
        "        self.fc_static2 = nn.Linear(512, 256)\n",
        "        self.fc_static3 = nn.Linear(256, 128)\n",
        "        self.fc_static4 = nn.Linear(128, 64)\n",
        "\n",
        "        # Time-series Path (Conv1D for feature extraction)\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Deep State Dynamics (LSTM for latent state transitions)\n",
        "        self.lstm_state = nn.LSTM(hidden_dim + 64, hidden_dim, batch_first=True)\n",
        "\n",
        "        # Observation Model (Mapping latent states to outputs)\n",
        "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, time_series_input, static_input):\n",
        "        # Static Data Path\n",
        "        static_out = self.relu(self.fc_static1(static_input))\n",
        "        static_out = self.relu(self.fc_static2(static_out))\n",
        "        static_out = self.relu(self.fc_static3(static_out))\n",
        "        static_out = self.relu(self.fc_static4(static_out))  # Shape: [batch_size, 64]\n",
        "\n",
        "        # Time-Series Data Path\n",
        "        if len(time_series_input.shape) == 2:  # [batch_size, seq_len]\n",
        "            time_series_input = time_series_input.unsqueeze(1)  # Add channel dimension: [batch_size, 1, seq_len]\n",
        "\n",
        "        conv_out = self.conv1(time_series_input)  # Conv1D layer\n",
        "        conv_out = self.relu(conv_out)\n",
        "        conv_out = conv_out.transpose(1, 2)  # Shape: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        # Expand static features to match the sequence length\n",
        "        static_expanded = static_out.unsqueeze(1).expand(-1, conv_out.size(1), -1)  # Shape: [batch_size, seq_len, 64]\n",
        "\n",
        "        # Combine Conv1D features and static features\n",
        "        lstm_input = torch.cat([conv_out, static_expanded], dim=2)  # Shape: [batch_size, seq_len, hidden_dim + 64]\n",
        "\n",
        "        # Latent State Dynamics (LSTM for state transitions)\n",
        "        lstm_out, _ = self.lstm_state(lstm_input)  # Shape: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        # Observation Model\n",
        "        lstm_out_final = lstm_out[:, -1, :]  # Use the last state for prediction\n",
        "        x = self.fc1(lstm_out_final)\n",
        "        x = self.relu(x)\n",
        "        output = self.fc2(x)  # Final prediction\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "-L_ExHUVmyg0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "WK9vQKkunUtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_boundary_cases_with_input(inputs, Boundary_case_actuals, Boundary_case_predicted, model_name, input_length):\n",
        "    case_names = [\"Best\", \"Average\", \"Worst\"]\n",
        "    x_range = input_length\n",
        "    y_range = Boundary_case_actuals.shape[1]\n",
        "    total_timesteps = x_range + y_range\n",
        "\n",
        "    for i in range(3):\n",
        "        plt.figure(figsize=(7.5, 3.2))\n",
        "\n",
        "        # Plot input (X)\n",
        "        plt.plot(range(x_range), inputs[i], color='black', alpha=0.5, label='Input')\n",
        "\n",
        "        # Plot output actual vs predicted (Y)\n",
        "        plt.plot(range(x_range, total_timesteps), Boundary_case_actuals[i], color='blue', alpha=0.8, label='Actual')\n",
        "        plt.plot(range(x_range, total_timesteps), Boundary_case_predicted[i], color='red', alpha=0.8, label='Predicted')\n",
        "\n",
        "        plt.xlabel(\"Time Steps\")\n",
        "        plt.ylabel(\"CO₂ Sequestration\")\n",
        "        plt.title(f\"{case_names[i]} Case – {model_name}\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout(pad=2.5)\n",
        "\n",
        "        filename = f\"drive/My Drive/DSSM-Figures-final/{model_name}_{case_names[i]}.pdf\"\n",
        "        plt.savefig(filename, format='pdf', bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "def calculate_metrics(actuals, predictions,model_name):\n",
        "    # Mean Absolute Error (MAE)\n",
        "    mae = np.mean(np.abs(actuals - predictions), axis=1)\n",
        "    mae_mean = np.mean(mae)\n",
        "    mae_std = np.std(mae)\n",
        "\n",
        "    # Mean Squared Error (MSE)\n",
        "    mse = np.mean((actuals - predictions) ** 2, axis=1)\n",
        "    mse_mean = np.mean(mse)\n",
        "    mse_std = np.std(mse)\n",
        "\n",
        "    # Symmetric Mean Absolute Percentage Error (SMAPE)\n",
        "    smape = np.mean(2 * np.abs(actuals - predictions) / (np.abs(actuals) + np.abs(predictions) + 1e-8), axis=1) * 100\n",
        "    smape_mean = np.mean(smape)\n",
        "    smape_std = np.std(smape)\n",
        "\n",
        "    # Root Mean Squared Error (RMSE)\n",
        "    rmse = np.sqrt(mse)\n",
        "    rmse_mean = np.mean(rmse)\n",
        "    rmse_std = np.std(rmse)\n",
        "\n",
        "    # R-squared (R²)\n",
        "    ss_res = np.sum((actuals - predictions) ** 2, axis=1)\n",
        "    ss_tot = np.sum((actuals - np.mean(actuals, axis=1, keepdims=True)) ** 2, axis=1)\n",
        "    r2 = 1 - (ss_res / ss_tot)\n",
        "    r2_mean = np.mean(r2)\n",
        "    r2_std = np.std(r2)\n",
        "\n",
        "    # Finding indices for the lowest, average, and highest RMSE\n",
        "    min_rmse_index = np.argmin(rmse)\n",
        "    max_rmse_index = np.argmax(rmse)\n",
        "    avg_rmse_index = np.argsort(rmse)[len(rmse) // 2]  # median RMSE as the average case\n",
        "\n",
        "    # Boundary case actuals and predictions\n",
        "    Boundary_case_actuals = np.vstack([\n",
        "        actuals[min_rmse_index],\n",
        "        actuals[avg_rmse_index],\n",
        "        actuals[max_rmse_index]\n",
        "    ])\n",
        "\n",
        "    Boundary_case_predicted = np.vstack([\n",
        "        predictions[min_rmse_index],\n",
        "        predictions[avg_rmse_index],\n",
        "        predictions[max_rmse_index]\n",
        "    ])\n",
        "\n",
        "    # Return metrics and their standard deviations\n",
        "    return {\n",
        "        f'{model_name} MAE': mae_mean,\n",
        "        f'{model_name} MAE_std': mae_std,\n",
        "        f'{model_name} MSE': mse_mean,\n",
        "        f'{model_name} MSE_std': mse_std,\n",
        "        f'{model_name} SMAPE': smape_mean,\n",
        "        f'{model_name} SMAPE_std': smape_std,\n",
        "        f'{model_name} RMSE': rmse_mean,\n",
        "        f'{model_name} RMSE_std': rmse_std,\n",
        "        f'{model_name} R2': r2_mean,\n",
        "        f'{model_name} R2_std': r2_std,\n",
        "        f'Boundary_case_actuals': Boundary_case_actuals,\n",
        "        f'Boundary_case_predicted': Boundary_case_predicted\n",
        "    }\n",
        "\n",
        "# Plotting Function (PDF & LaTeX-Ready)\n",
        "def plot_discretized_validation(inputs, actuals, predictions, clusters, cluster_boundaries, model_name):\n",
        "    file_id_to_index = {fid: idx for idx, fid in enumerate(Clustering_link_table['file_id'])}\n",
        "    test_file_ids = list(Clustering_link_table['file_id'][Clustering_link_table['file_id'].isin(test_ids)])\n",
        "    cluster_ids = np.array(Clustering_link_table['cluster'][Clustering_link_table['file_id'].isin(test_ids)])\n",
        "    unique_clusters = np.unique(cluster_ids)\n",
        "    y_length = predictions.shape[1]\n",
        "    x_length = inputs.shape[1]\n",
        "    num_timesteps = x_length + y_length\n",
        "\n",
        "    for cluster_id in unique_clusters:\n",
        "        cluster_file_ids = [fid for i, fid in enumerate(test_file_ids) if cluster_ids[i] == cluster_id]\n",
        "        test_pos_in_preds = [i for i, fid in enumerate(test_file_ids) if cluster_ids[i] == cluster_id]\n",
        "\n",
        "        plt.figure(figsize=(7.5, 3.2))\n",
        "        min_x = cluster_boundaries[cluster_id, 0, :x_length]\n",
        "        max_x = cluster_boundaries[cluster_id, 3, :x_length]\n",
        "        min_y = cluster_boundaries[cluster_id, 0, -y_length:]\n",
        "        max_y = cluster_boundaries[cluster_id, 3, -y_length:]\n",
        "\n",
        "        for i, pred_idx in enumerate(test_pos_in_preds):\n",
        "            plt.plot(range(x_length), inputs[pred_idx], color='black', alpha=0.3, label='Input' if i == 0 else \"\")\n",
        "            plt.plot(range(x_length, num_timesteps), actuals[pred_idx], color='blue', alpha=0.5, label='Actual' if i == 0 else \"\")\n",
        "            plt.plot(range(x_length, num_timesteps), predictions[pred_idx], color='red', alpha=0.5, label='Predicted' if i == 0 else \"\")\n",
        "\n",
        "        plt.fill_between(range(x_length), min_x, max_x, color='orange', alpha=0.2, label='Input Boundary')\n",
        "        plt.fill_between(range(x_length, num_timesteps), min_y, max_y, color='grey', alpha=0.3, label='Output Boundary')\n",
        "\n",
        "        plt.xlabel('Time Steps')\n",
        "        plt.ylabel('CO₂ Sequestration')\n",
        "        plt.title(f'{model_name} – Cluster {cluster_id}')\n",
        "        plt.legend()\n",
        "        plt.tight_layout(pad=2.5)\n",
        "        filename = f\"drive/My Drive/DSSM-Figures-final/{model_name}_cluster_{cluster_id}.pdf\"\n",
        "        plt.savefig(filename, format='pdf', bbox_inches='tight')\n",
        "        plt.close()"
      ],
      "metadata": {
        "id": "FT0HrQSPnXP6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment"
      ],
      "metadata": {
        "id": "6PQv2pwpoB_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "splits = [(20, 80)]\n",
        "for train_pct, test_pct in splits:\n",
        "    split_name = f\"{train_pct}_{test_pct}\"\n",
        "    train_ids, test_ids = train_test_split(df_output['file_id'].unique(), test_size=0.2, random_state=42)\n",
        "    df_train = df_output[df_output['file_id'].isin(train_ids)]\n",
        "    df_test = df_output[df_output['file_id'].isin(test_ids)]\n",
        "    train_timestep = int(train_pct / 100 * 101)\n",
        "    X_train = df_train.pivot(index='file_id', columns='timestep', values='CO2').values[:, :train_timestep]\n",
        "    Y_train = df_train.pivot(index='file_id', columns='timestep', values='CO2').values[:, train_timestep:]\n",
        "    X_test = df_test.pivot(index='file_id', columns='timestep', values='CO2').values[:, :train_timestep]\n",
        "    Y_test = df_test.pivot(index='file_id', columns='timestep', values='CO2').values[:, train_timestep:]\n",
        "    static_train = merged_df[merged_df['file_id'].isin(train_ids)].drop(columns=['file_id', 'cluster']).values\n",
        "    static_test = merged_df[merged_df['file_id'].isin(test_ids)].drop(columns=['file_id', 'cluster']).values\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
        "    static_train_tensor = torch.tensor(static_train, dtype=torch.float32)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
        "    static_test_tensor = torch.tensor(static_test, dtype=torch.float32)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train_tensor, static_train_tensor, Y_train_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, static_test_tensor, Y_test_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "    model = AdvancedDSSMDeepState(input_dim=train_timestep, static_dim=static_train.shape[1], hidden_dim=101, output_dim=Y_train.shape[1])\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    model.train()\n",
        "    print(f\"Training for {split_name}\")\n",
        "    for epoch in range(500):\n",
        "        for X_batch, static_batch, Y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X_batch, static_batch)\n",
        "            loss = criterion(preds, Y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = []\n",
        "        actuals = []\n",
        "        inputs = []\n",
        "        for X_batch, static_batch, Y_batch in test_loader:\n",
        "            outputs = model(X_batch, static_batch)\n",
        "            predictions.append(outputs.numpy())\n",
        "            actuals.append(Y_batch.numpy())\n",
        "            inputs.append(X_batch.numpy())\n",
        "\n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    actuals = np.concatenate(actuals, axis=0)\n",
        "    inputs = np.concatenate(inputs, axis=0)\n",
        "    model_name = f\"DSSM_{split_name}\"\n",
        "\n",
        "    plot_discretized_validation(inputs, actuals, predictions, clusters, cluster_boundaries, model_name)\n",
        "    metrics = calculate_metrics(actuals, predictions, model_name)\n",
        "    Boundary_case_actuals = metrics['Boundary_case_actuals']\n",
        "    Boundary_case_predicted = metrics['Boundary_case_predicted']\n",
        "\n",
        "\n",
        "    input_length = inputs.shape[1]\n",
        "    rmse = np.sqrt(np.mean((actuals - predictions) ** 2, axis=1))\n",
        "    min_rmse_index = np.argmin(rmse)\n",
        "    max_rmse_index = np.argmax(rmse)\n",
        "    avg_rmse_index = np.argsort(rmse)[len(rmse) // 2]\n",
        "\n",
        "    selected_inputs = np.vstack([\n",
        "        inputs[min_rmse_index],\n",
        "        inputs[avg_rmse_index],\n",
        "        inputs[max_rmse_index]\n",
        "    ])\n",
        "\n",
        "    plot_boundary_cases_with_input(selected_inputs, Boundary_case_actuals, Boundary_case_predicted, model_name, input_length)\n",
        "     # --- New Visualization 1: Timestep-based Error Plot ---\n",
        "    timestep_mse = np.mean((actuals - predictions) ** 2, axis=0)\n",
        "\n",
        "    plt.figure(figsize=(7.5, 3.2))\n",
        "    plt.plot(range(1, len(timestep_mse) + 1), timestep_mse, color='purple', marker='o')\n",
        "    plt.xlabel(\"Predicted Timestep (t)\")\n",
        "    plt.ylabel(\"Average MSE\")\n",
        "    plt.title(f\"Timestep-based-error – {model_name}\")\n",
        "    plt.tight_layout(pad=2.5)\n",
        "    filename = f\"drive/My Drive/DSSM-Figures-final/{model_name}_timestep_error.pdf\"\n",
        "    plt.savefig(filename, format='pdf', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # --- New Visualization 2: Input-based Error Plot + CSV Table ---\n",
        "    test_mse = np.mean((actuals - predictions) ** 2, axis=1)\n",
        "\n",
        "    # --- Sort the test MSEs in ascending order ---\n",
        "    sorted_indices = np.argsort(test_mse)\n",
        "    sorted_mse = np.array(test_mse)[sorted_indices]\n",
        "\n",
        "    # --- Plot MSE per test case, sorted ---\n",
        "    plt.figure(figsize=(7.5, 3.2))\n",
        "    plt.plot(range(len(sorted_mse)), sorted_mse, color='darkgreen', marker='.')\n",
        "    plt.xlabel(\"Renumbered Test Case Index (Sorted by Error)\")\n",
        "    plt.ylabel(\"MSE\")\n",
        "    plt.title(f\"Input-based-error – {model_name}\")\n",
        "    plt.tight_layout(pad=2.5)\n",
        "\n",
        "    # --- Save to Drive ---\n",
        "    filename = f\"drive/My Drive/DSSM-Figures-final/{model_name}_input_error.pdf\"\n",
        "    plt.savefig(filename, format='pdf', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Save static features and MSE with new indices\n",
        "    test_static_df = pd.DataFrame(static_test, columns=merged_df.drop(columns=['file_id', 'cluster']).columns)\n",
        "    test_static_df.insert(0, \"Test_Case_Index\", np.arange(len(test_static_df)))\n",
        "    test_static_df.insert(1, \"MSE\", test_mse)\n",
        "    test_static_df.to_csv(f\"drive/My Drive/DSSM-Figures-final/{model_name}_test_input_features.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Dr0gbAhoES6",
        "outputId": "3d2e8766-ef83-4e6f-dee5-e5617de04027"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 20_80\n",
            "Epoch 1, Loss: 0.15828680992126465\n",
            "Epoch 2, Loss: 0.13744263350963593\n",
            "Epoch 3, Loss: 0.10406629741191864\n",
            "Epoch 4, Loss: 0.05085765942931175\n",
            "Epoch 5, Loss: 0.025389686226844788\n",
            "Epoch 6, Loss: 0.025762517005205154\n",
            "Epoch 7, Loss: 0.025303257629275322\n",
            "Epoch 8, Loss: 0.01531542930752039\n",
            "Epoch 9, Loss: 0.026277482509613037\n",
            "Epoch 10, Loss: 0.03640454262495041\n",
            "Epoch 11, Loss: 0.01880188286304474\n",
            "Epoch 12, Loss: 0.030176423490047455\n",
            "Epoch 13, Loss: 0.012344791553914547\n",
            "Epoch 14, Loss: 0.01922675222158432\n",
            "Epoch 15, Loss: 0.020823560655117035\n",
            "Epoch 16, Loss: 0.014164267107844353\n",
            "Epoch 17, Loss: 0.015640851110219955\n",
            "Epoch 18, Loss: 0.00956644769757986\n",
            "Epoch 19, Loss: 0.02409621886909008\n",
            "Epoch 20, Loss: 0.026111586019396782\n",
            "Epoch 21, Loss: 0.01685100980103016\n",
            "Epoch 22, Loss: 0.01571664772927761\n",
            "Epoch 23, Loss: 0.010084589943289757\n",
            "Epoch 24, Loss: 0.016782918944954872\n",
            "Epoch 25, Loss: 0.030276738107204437\n",
            "Epoch 26, Loss: 0.015180359594523907\n",
            "Epoch 27, Loss: 0.0226997509598732\n",
            "Epoch 28, Loss: 0.028151700273156166\n",
            "Epoch 29, Loss: 0.015043770894408226\n",
            "Epoch 30, Loss: 0.01233555655926466\n",
            "Epoch 31, Loss: 0.021016495302319527\n",
            "Epoch 32, Loss: 0.013196670450270176\n",
            "Epoch 33, Loss: 0.007713969796895981\n",
            "Epoch 34, Loss: 0.010160683654248714\n",
            "Epoch 35, Loss: 0.007458751555532217\n",
            "Epoch 36, Loss: 0.01248497236520052\n",
            "Epoch 37, Loss: 0.014352988451719284\n",
            "Epoch 38, Loss: 0.008037727326154709\n",
            "Epoch 39, Loss: 0.008786848746240139\n",
            "Epoch 40, Loss: 0.01089189201593399\n",
            "Epoch 41, Loss: 0.010581973008811474\n",
            "Epoch 42, Loss: 0.010746000334620476\n",
            "Epoch 43, Loss: 0.005206588190048933\n",
            "Epoch 44, Loss: 0.014745011925697327\n",
            "Epoch 45, Loss: 0.022700510919094086\n",
            "Epoch 46, Loss: 0.008168497122824192\n",
            "Epoch 47, Loss: 0.009095408022403717\n",
            "Epoch 48, Loss: 0.013747885823249817\n",
            "Epoch 49, Loss: 0.003896745154634118\n",
            "Epoch 50, Loss: 0.0105176055803895\n",
            "Epoch 51, Loss: 0.006645772606134415\n",
            "Epoch 52, Loss: 0.006929763592779636\n",
            "Epoch 53, Loss: 0.005028235260397196\n",
            "Epoch 54, Loss: 0.0043288893066346645\n",
            "Epoch 55, Loss: 0.0054603018797934055\n",
            "Epoch 56, Loss: 0.004355576820671558\n",
            "Epoch 57, Loss: 0.008418434299528599\n",
            "Epoch 58, Loss: 0.006287760101258755\n",
            "Epoch 59, Loss: 0.0036913822405040264\n",
            "Epoch 60, Loss: 0.007411243859678507\n",
            "Epoch 61, Loss: 0.00501672737300396\n",
            "Epoch 62, Loss: 0.003009510226547718\n",
            "Epoch 63, Loss: 0.00674811378121376\n",
            "Epoch 64, Loss: 0.008069141767919064\n",
            "Epoch 65, Loss: 0.003870077896863222\n",
            "Epoch 66, Loss: 0.002202547388151288\n",
            "Epoch 67, Loss: 0.008297359570860863\n",
            "Epoch 68, Loss: 0.005841404665261507\n",
            "Epoch 69, Loss: 0.0036870476324111223\n",
            "Epoch 70, Loss: 0.00887727364897728\n",
            "Epoch 71, Loss: 0.009259926155209541\n",
            "Epoch 72, Loss: 0.004365801345556974\n",
            "Epoch 73, Loss: 0.005107095930725336\n",
            "Epoch 74, Loss: 0.003971561323851347\n",
            "Epoch 75, Loss: 0.0012939783046022058\n",
            "Epoch 76, Loss: 0.0020716183353215456\n",
            "Epoch 77, Loss: 0.003876239527016878\n",
            "Epoch 78, Loss: 0.0021170068066567183\n",
            "Epoch 79, Loss: 0.004879317246377468\n",
            "Epoch 80, Loss: 0.009113543666899204\n",
            "Epoch 81, Loss: 0.00358694139868021\n",
            "Epoch 82, Loss: 0.004290673416107893\n",
            "Epoch 83, Loss: 0.0042159114964306355\n",
            "Epoch 84, Loss: 0.0029472410678863525\n",
            "Epoch 85, Loss: 0.0025298690889030695\n",
            "Epoch 86, Loss: 0.003195772413164377\n",
            "Epoch 87, Loss: 0.0028191879391670227\n",
            "Epoch 88, Loss: 0.0043722232803702354\n",
            "Epoch 89, Loss: 0.003197989659383893\n",
            "Epoch 90, Loss: 0.0018878770060837269\n",
            "Epoch 91, Loss: 0.003562688594684005\n",
            "Epoch 92, Loss: 0.0022228045854717493\n",
            "Epoch 93, Loss: 0.0034725992009043694\n",
            "Epoch 94, Loss: 0.001473379204981029\n",
            "Epoch 95, Loss: 0.003155785845592618\n",
            "Epoch 96, Loss: 0.0034911560360342264\n",
            "Epoch 97, Loss: 0.002826802432537079\n",
            "Epoch 98, Loss: 0.0029083585832268\n",
            "Epoch 99, Loss: 0.005322672426700592\n",
            "Epoch 100, Loss: 0.005385659635066986\n",
            "Epoch 101, Loss: 0.003862552810460329\n",
            "Epoch 102, Loss: 0.002228895202279091\n",
            "Epoch 103, Loss: 0.005094810388982296\n",
            "Epoch 104, Loss: 0.001575979171320796\n",
            "Epoch 105, Loss: 0.0016893589636310935\n",
            "Epoch 106, Loss: 0.0031350520439445972\n",
            "Epoch 107, Loss: 0.0014573318185284734\n",
            "Epoch 108, Loss: 0.004759314935654402\n",
            "Epoch 109, Loss: 0.00247885100543499\n",
            "Epoch 110, Loss: 0.002800748683512211\n",
            "Epoch 111, Loss: 0.004333075135946274\n",
            "Epoch 112, Loss: 0.003518730169162154\n",
            "Epoch 113, Loss: 0.00344495614990592\n",
            "Epoch 114, Loss: 0.003995660226792097\n",
            "Epoch 115, Loss: 0.0018691138830035925\n",
            "Epoch 116, Loss: 0.0027078643906861544\n",
            "Epoch 117, Loss: 0.0018894788809120655\n",
            "Epoch 118, Loss: 0.004244996700435877\n",
            "Epoch 119, Loss: 0.004407335072755814\n",
            "Epoch 120, Loss: 0.002590914024040103\n",
            "Epoch 121, Loss: 0.006842104718089104\n",
            "Epoch 122, Loss: 0.004460018593817949\n",
            "Epoch 123, Loss: 0.00327960099093616\n",
            "Epoch 124, Loss: 0.003407827578485012\n",
            "Epoch 125, Loss: 0.0022521342616528273\n",
            "Epoch 126, Loss: 0.001496779383160174\n",
            "Epoch 127, Loss: 0.0036424673162400723\n",
            "Epoch 128, Loss: 0.002830721437931061\n",
            "Epoch 129, Loss: 0.001831002184189856\n",
            "Epoch 130, Loss: 0.00340913119725883\n",
            "Epoch 131, Loss: 0.00314912386238575\n",
            "Epoch 132, Loss: 0.004858298692852259\n",
            "Epoch 133, Loss: 0.004222058225423098\n",
            "Epoch 134, Loss: 0.0031831583473831415\n",
            "Epoch 135, Loss: 0.004321423824876547\n",
            "Epoch 136, Loss: 0.0020496174693107605\n",
            "Epoch 137, Loss: 0.0024388241581618786\n",
            "Epoch 138, Loss: 0.002772875828668475\n",
            "Epoch 139, Loss: 0.0023663954343646765\n",
            "Epoch 140, Loss: 0.0033434873912483454\n",
            "Epoch 141, Loss: 0.0017157775582745671\n",
            "Epoch 142, Loss: 0.0038144721183925867\n",
            "Epoch 143, Loss: 0.0013033590512350202\n",
            "Epoch 144, Loss: 0.0011420411756262183\n",
            "Epoch 145, Loss: 0.0037648864090442657\n",
            "Epoch 146, Loss: 0.0010313320672139525\n",
            "Epoch 147, Loss: 0.0016918184701353312\n",
            "Epoch 148, Loss: 0.0014793180162087083\n",
            "Epoch 149, Loss: 0.0017361573409289122\n",
            "Epoch 150, Loss: 0.0010044312803074718\n",
            "Epoch 151, Loss: 0.0015129904495552182\n",
            "Epoch 152, Loss: 0.001068025710992515\n",
            "Epoch 153, Loss: 0.0014556943206116557\n",
            "Epoch 154, Loss: 0.0009765680879354477\n",
            "Epoch 155, Loss: 0.0015283523825928569\n",
            "Epoch 156, Loss: 0.002561309142038226\n",
            "Epoch 157, Loss: 0.004325782880187035\n",
            "Epoch 158, Loss: 0.0024331817403435707\n",
            "Epoch 159, Loss: 0.0020635852124542\n",
            "Epoch 160, Loss: 0.0024683435913175344\n",
            "Epoch 161, Loss: 0.002622838132083416\n",
            "Epoch 162, Loss: 0.0017913351766765118\n",
            "Epoch 163, Loss: 0.001143219298683107\n",
            "Epoch 164, Loss: 0.0018078562570735812\n",
            "Epoch 165, Loss: 0.001294532441534102\n",
            "Epoch 166, Loss: 0.002200234914198518\n",
            "Epoch 167, Loss: 0.0010093658929690719\n",
            "Epoch 168, Loss: 0.0010884955991059542\n",
            "Epoch 169, Loss: 0.00186137855052948\n",
            "Epoch 170, Loss: 0.003151373704895377\n",
            "Epoch 171, Loss: 0.0013091175351291895\n",
            "Epoch 172, Loss: 0.0017848066054284573\n",
            "Epoch 173, Loss: 0.00231834570877254\n",
            "Epoch 174, Loss: 0.0024417731910943985\n",
            "Epoch 175, Loss: 0.0011629491345956922\n",
            "Epoch 176, Loss: 0.0028941622003912926\n",
            "Epoch 177, Loss: 0.0009971974650397897\n",
            "Epoch 178, Loss: 0.000984472455456853\n",
            "Epoch 179, Loss: 0.0020923151168972254\n",
            "Epoch 180, Loss: 0.002708952408283949\n",
            "Epoch 181, Loss: 0.0012154537253081799\n",
            "Epoch 182, Loss: 0.0012429894413799047\n",
            "Epoch 183, Loss: 0.0035086972638964653\n",
            "Epoch 184, Loss: 0.001305857440456748\n",
            "Epoch 185, Loss: 0.0023872824385762215\n",
            "Epoch 186, Loss: 0.003146114991977811\n",
            "Epoch 187, Loss: 0.0017994262743741274\n",
            "Epoch 188, Loss: 0.0015444380696862936\n",
            "Epoch 189, Loss: 0.002058678539469838\n",
            "Epoch 190, Loss: 0.0016554445028305054\n",
            "Epoch 191, Loss: 0.0015788713935762644\n",
            "Epoch 192, Loss: 0.0012757214717566967\n",
            "Epoch 193, Loss: 0.0024915600661188364\n",
            "Epoch 194, Loss: 0.001528315362520516\n",
            "Epoch 195, Loss: 0.0009785890579223633\n",
            "Epoch 196, Loss: 0.0012583089992403984\n",
            "Epoch 197, Loss: 0.001903439755551517\n",
            "Epoch 198, Loss: 0.0012878953712061048\n",
            "Epoch 199, Loss: 0.0012085761409252882\n",
            "Epoch 200, Loss: 0.0012512877583503723\n",
            "Epoch 201, Loss: 0.001367860590107739\n",
            "Epoch 202, Loss: 0.001469805953092873\n",
            "Epoch 203, Loss: 0.001169812516309321\n",
            "Epoch 204, Loss: 0.0021334663033485413\n",
            "Epoch 205, Loss: 0.003418870735913515\n",
            "Epoch 206, Loss: 0.002146314363926649\n",
            "Epoch 207, Loss: 0.00416764710098505\n",
            "Epoch 208, Loss: 0.0036201614420861006\n",
            "Epoch 209, Loss: 0.0012561791809275746\n",
            "Epoch 210, Loss: 0.0009869972709566355\n",
            "Epoch 211, Loss: 0.00148907455150038\n",
            "Epoch 212, Loss: 0.0007837566663511097\n",
            "Epoch 213, Loss: 0.002052928786724806\n",
            "Epoch 214, Loss: 0.0024691964499652386\n",
            "Epoch 215, Loss: 0.0019814250990748405\n",
            "Epoch 216, Loss: 0.000826117699034512\n",
            "Epoch 217, Loss: 0.001745393150486052\n",
            "Epoch 218, Loss: 0.0014923653798177838\n",
            "Epoch 219, Loss: 0.0009174137376248837\n",
            "Epoch 220, Loss: 0.00445649353787303\n",
            "Epoch 221, Loss: 0.0018965270137414336\n",
            "Epoch 222, Loss: 0.0015030920039862394\n",
            "Epoch 223, Loss: 0.0017646761843934655\n",
            "Epoch 224, Loss: 0.0018502343446016312\n",
            "Epoch 225, Loss: 0.0008322546491399407\n",
            "Epoch 226, Loss: 0.0010723453015089035\n",
            "Epoch 227, Loss: 0.001452943542972207\n",
            "Epoch 228, Loss: 0.0016851549735292792\n",
            "Epoch 229, Loss: 0.0022184003610163927\n",
            "Epoch 230, Loss: 0.0028920716140419245\n",
            "Epoch 231, Loss: 0.0011967300670221448\n",
            "Epoch 232, Loss: 0.00045998531277291477\n",
            "Epoch 233, Loss: 0.0010754992254078388\n",
            "Epoch 234, Loss: 0.0013563777320086956\n",
            "Epoch 235, Loss: 0.0012363711139187217\n",
            "Epoch 236, Loss: 0.0011874056654050946\n",
            "Epoch 237, Loss: 0.0008379911887459457\n",
            "Epoch 238, Loss: 0.0013813673285767436\n",
            "Epoch 239, Loss: 0.0033519528806209564\n",
            "Epoch 240, Loss: 0.0014117805985733867\n",
            "Epoch 241, Loss: 0.0007385346107184887\n",
            "Epoch 242, Loss: 0.0010442750062793493\n",
            "Epoch 243, Loss: 0.0016003533964976668\n",
            "Epoch 244, Loss: 0.001357038738206029\n",
            "Epoch 245, Loss: 0.0011011817259714007\n",
            "Epoch 246, Loss: 0.001262818113900721\n",
            "Epoch 247, Loss: 0.0006271203747019172\n",
            "Epoch 248, Loss: 0.0008370773284696043\n",
            "Epoch 249, Loss: 0.0009562945924699306\n",
            "Epoch 250, Loss: 0.0006788413156755269\n",
            "Epoch 251, Loss: 0.0006282622925937176\n",
            "Epoch 252, Loss: 0.0014812255976721644\n",
            "Epoch 253, Loss: 0.0007841520709916949\n",
            "Epoch 254, Loss: 0.0006613011937588453\n",
            "Epoch 255, Loss: 0.005603591445833445\n",
            "Epoch 256, Loss: 0.003079637885093689\n",
            "Epoch 257, Loss: 0.0046552331186831\n",
            "Epoch 258, Loss: 0.0035883772652596235\n",
            "Epoch 259, Loss: 0.0027346641290932894\n",
            "Epoch 260, Loss: 0.0007021831115707755\n",
            "Epoch 261, Loss: 0.0012439301935955882\n",
            "Epoch 262, Loss: 0.0007086569676175714\n",
            "Epoch 263, Loss: 0.0010165945859625936\n",
            "Epoch 264, Loss: 0.0009937492432072759\n",
            "Epoch 265, Loss: 0.0006307830335572362\n",
            "Epoch 266, Loss: 0.001481355051510036\n",
            "Epoch 267, Loss: 0.0005594036774709821\n",
            "Epoch 268, Loss: 0.00030181885813362896\n",
            "Epoch 269, Loss: 0.0012285263510420918\n",
            "Epoch 270, Loss: 0.00032347923843190074\n",
            "Epoch 271, Loss: 0.0011747871758416295\n",
            "Epoch 272, Loss: 0.0013950057327747345\n",
            "Epoch 273, Loss: 0.0010955673642456532\n",
            "Epoch 274, Loss: 0.002907905960455537\n",
            "Epoch 275, Loss: 0.001377012929879129\n",
            "Epoch 276, Loss: 0.0005481880507431924\n",
            "Epoch 277, Loss: 0.0009843402076512575\n",
            "Epoch 278, Loss: 0.0006975330761633813\n",
            "Epoch 279, Loss: 0.0007572938338853419\n",
            "Epoch 280, Loss: 0.000899270991794765\n",
            "Epoch 281, Loss: 0.0007275070529431105\n",
            "Epoch 282, Loss: 0.0012767011066898704\n",
            "Epoch 283, Loss: 0.000594984507188201\n",
            "Epoch 284, Loss: 0.000707681872881949\n",
            "Epoch 285, Loss: 0.0005385854747146368\n",
            "Epoch 286, Loss: 0.0009735591011121869\n",
            "Epoch 287, Loss: 0.0008679043385200202\n",
            "Epoch 288, Loss: 0.001004671212285757\n",
            "Epoch 289, Loss: 0.0009381158743053675\n",
            "Epoch 290, Loss: 0.0005660061724483967\n",
            "Epoch 291, Loss: 0.0012825634330511093\n",
            "Epoch 292, Loss: 0.0007633030181750655\n",
            "Epoch 293, Loss: 0.0012171934358775616\n",
            "Epoch 294, Loss: 0.0005592346424236894\n",
            "Epoch 295, Loss: 0.0015892949886620045\n",
            "Epoch 296, Loss: 0.0004053673183079809\n",
            "Epoch 297, Loss: 0.0012767453445121646\n",
            "Epoch 298, Loss: 0.001059377333149314\n",
            "Epoch 299, Loss: 0.0013720181304961443\n",
            "Epoch 300, Loss: 0.0011337342439219356\n",
            "Epoch 301, Loss: 0.0006112770061008632\n",
            "Epoch 302, Loss: 0.0018702626693993807\n",
            "Epoch 303, Loss: 0.0009134307038038969\n",
            "Epoch 304, Loss: 0.0009560961625538766\n",
            "Epoch 305, Loss: 0.00040792772779241204\n",
            "Epoch 306, Loss: 0.0006471307133324444\n",
            "Epoch 307, Loss: 0.0012038224376738071\n",
            "Epoch 308, Loss: 0.0010450639529153705\n",
            "Epoch 309, Loss: 0.0011400998337194324\n",
            "Epoch 310, Loss: 0.0006642817170359194\n",
            "Epoch 311, Loss: 0.0013766566989943385\n",
            "Epoch 312, Loss: 0.0009776139631867409\n",
            "Epoch 313, Loss: 0.000688481202814728\n",
            "Epoch 314, Loss: 0.00047808769159018993\n",
            "Epoch 315, Loss: 0.0009463559836149216\n",
            "Epoch 316, Loss: 0.0002922707935795188\n",
            "Epoch 317, Loss: 0.0011275585275143385\n",
            "Epoch 318, Loss: 0.0006809384794905782\n",
            "Epoch 319, Loss: 0.001985992304980755\n",
            "Epoch 320, Loss: 0.0008277670131064951\n",
            "Epoch 321, Loss: 0.0011771897552534938\n",
            "Epoch 322, Loss: 0.0012359542306512594\n",
            "Epoch 323, Loss: 0.00038349672104232013\n",
            "Epoch 324, Loss: 0.001506326487287879\n",
            "Epoch 325, Loss: 0.0016115629114210606\n",
            "Epoch 326, Loss: 0.0008370299474336207\n",
            "Epoch 327, Loss: 0.0009398267138749361\n",
            "Epoch 328, Loss: 0.000673749833367765\n",
            "Epoch 329, Loss: 0.0011281654005870223\n",
            "Epoch 330, Loss: 0.0011510703479871154\n",
            "Epoch 331, Loss: 0.001760314917191863\n",
            "Epoch 332, Loss: 0.001223570085130632\n",
            "Epoch 333, Loss: 0.0005644423654302955\n",
            "Epoch 334, Loss: 0.0008129464695230126\n",
            "Epoch 335, Loss: 0.0008178245043382049\n",
            "Epoch 336, Loss: 0.000726542086340487\n",
            "Epoch 337, Loss: 0.0011564659653231502\n",
            "Epoch 338, Loss: 0.0009028491913340986\n",
            "Epoch 339, Loss: 0.0012551563559100032\n",
            "Epoch 340, Loss: 0.0013563540996983647\n",
            "Epoch 341, Loss: 0.000608143862336874\n",
            "Epoch 342, Loss: 0.0006076223799027503\n",
            "Epoch 343, Loss: 0.0006099494639784098\n",
            "Epoch 344, Loss: 0.0012059640139341354\n",
            "Epoch 345, Loss: 0.0006239437498152256\n",
            "Epoch 346, Loss: 0.0006215578760020435\n",
            "Epoch 347, Loss: 0.0005303716752678156\n",
            "Epoch 348, Loss: 0.0015797633677721024\n",
            "Epoch 349, Loss: 0.0029856886249035597\n",
            "Epoch 350, Loss: 0.0018621054477989674\n",
            "Epoch 351, Loss: 0.001971174031496048\n",
            "Epoch 352, Loss: 0.0014655588893219829\n",
            "Epoch 353, Loss: 0.0013273749500513077\n",
            "Epoch 354, Loss: 0.0005372040905058384\n",
            "Epoch 355, Loss: 0.0012149411486461759\n",
            "Epoch 356, Loss: 0.0015434731030836701\n",
            "Epoch 357, Loss: 0.0007703426526859403\n",
            "Epoch 358, Loss: 0.0009288230212405324\n",
            "Epoch 359, Loss: 0.0009912068489938974\n",
            "Epoch 360, Loss: 0.0003637023619376123\n",
            "Epoch 361, Loss: 0.001061779330484569\n",
            "Epoch 362, Loss: 0.0008510429761372507\n",
            "Epoch 363, Loss: 0.000698735355399549\n",
            "Epoch 364, Loss: 0.0011406720150262117\n",
            "Epoch 365, Loss: 0.0007936004549264908\n",
            "Epoch 366, Loss: 0.0008744189399294555\n",
            "Epoch 367, Loss: 0.001397778862155974\n",
            "Epoch 368, Loss: 0.0007424938958138227\n",
            "Epoch 369, Loss: 0.0011432691244408488\n",
            "Epoch 370, Loss: 0.0006636022590100765\n",
            "Epoch 371, Loss: 0.0005042258417233825\n",
            "Epoch 372, Loss: 0.0010226430604234338\n",
            "Epoch 373, Loss: 0.0006879817228764296\n",
            "Epoch 374, Loss: 0.0009090019157156348\n",
            "Epoch 375, Loss: 0.0007977396599017084\n",
            "Epoch 376, Loss: 0.0006102427141740918\n",
            "Epoch 377, Loss: 0.0006372035713866353\n",
            "Epoch 378, Loss: 0.0008038104278966784\n",
            "Epoch 379, Loss: 0.0005098932306282222\n",
            "Epoch 380, Loss: 0.000991568434983492\n",
            "Epoch 381, Loss: 0.0004235213855281472\n",
            "Epoch 382, Loss: 0.0004605235590133816\n",
            "Epoch 383, Loss: 0.0007333859684877098\n",
            "Epoch 384, Loss: 0.0005875793285667896\n",
            "Epoch 385, Loss: 0.0006059793522581458\n",
            "Epoch 386, Loss: 0.000643656007014215\n",
            "Epoch 387, Loss: 0.0008158670971170068\n",
            "Epoch 388, Loss: 0.001667656353674829\n",
            "Epoch 389, Loss: 0.0007775465492159128\n",
            "Epoch 390, Loss: 0.0004128526197746396\n",
            "Epoch 391, Loss: 0.0010803904151543975\n",
            "Epoch 392, Loss: 0.000667962827719748\n",
            "Epoch 393, Loss: 0.0010758943390101194\n",
            "Epoch 394, Loss: 0.0005013655172660947\n",
            "Epoch 395, Loss: 0.0010119155049324036\n",
            "Epoch 396, Loss: 0.0009389658225700259\n",
            "Epoch 397, Loss: 0.0008735821465961635\n",
            "Epoch 398, Loss: 0.0006013787933625281\n",
            "Epoch 399, Loss: 0.0008125971653498709\n",
            "Epoch 400, Loss: 0.0004883260116912425\n",
            "Epoch 401, Loss: 0.0008875012863427401\n",
            "Epoch 402, Loss: 0.00032357408781535923\n",
            "Epoch 403, Loss: 0.000837669475004077\n",
            "Epoch 404, Loss: 0.0010961038060486317\n",
            "Epoch 405, Loss: 0.0005309368716552854\n",
            "Epoch 406, Loss: 0.0011009799782186747\n",
            "Epoch 407, Loss: 0.0013909946428611875\n",
            "Epoch 408, Loss: 0.002230973681434989\n",
            "Epoch 409, Loss: 0.0036196259316056967\n",
            "Epoch 410, Loss: 0.0031661884859204292\n",
            "Epoch 411, Loss: 0.0014033699408173561\n",
            "Epoch 412, Loss: 0.001343855052255094\n",
            "Epoch 413, Loss: 0.0008556992979720235\n",
            "Epoch 414, Loss: 0.0007691560313105583\n",
            "Epoch 415, Loss: 0.0006415923708118498\n",
            "Epoch 416, Loss: 0.0010336360428482294\n",
            "Epoch 417, Loss: 0.0003223009698558599\n",
            "Epoch 418, Loss: 0.0004062497755512595\n",
            "Epoch 419, Loss: 0.0005256767035461962\n",
            "Epoch 420, Loss: 0.0005947112804278731\n",
            "Epoch 421, Loss: 0.0009063468314707279\n",
            "Epoch 422, Loss: 0.0005209524533711374\n",
            "Epoch 423, Loss: 0.0006112656556069851\n",
            "Epoch 424, Loss: 0.0005351165309548378\n",
            "Epoch 425, Loss: 0.0006638314807787538\n",
            "Epoch 426, Loss: 0.00046383001608774066\n",
            "Epoch 427, Loss: 0.0007962284726090729\n",
            "Epoch 428, Loss: 0.0007026908569969237\n",
            "Epoch 429, Loss: 0.0012036124244332314\n",
            "Epoch 430, Loss: 0.0006896678823977709\n",
            "Epoch 431, Loss: 0.00042296512401662767\n",
            "Epoch 432, Loss: 0.0005120820133015513\n",
            "Epoch 433, Loss: 0.0004199617833364755\n",
            "Epoch 434, Loss: 0.0006883926689624786\n",
            "Epoch 435, Loss: 0.000353583280229941\n",
            "Epoch 436, Loss: 0.0007094321772456169\n",
            "Epoch 437, Loss: 0.002835078164935112\n",
            "Epoch 438, Loss: 0.000819728069473058\n",
            "Epoch 439, Loss: 0.0009179709013551474\n",
            "Epoch 440, Loss: 0.0005901156691834331\n",
            "Epoch 441, Loss: 0.00032489339355379343\n",
            "Epoch 442, Loss: 0.00029182873549871147\n",
            "Epoch 443, Loss: 0.0006251134327612817\n",
            "Epoch 444, Loss: 0.0005067672464065254\n",
            "Epoch 445, Loss: 0.00046211894368752837\n",
            "Epoch 446, Loss: 0.0007775413687340915\n",
            "Epoch 447, Loss: 0.0006446280749514699\n",
            "Epoch 448, Loss: 0.0007561205420643091\n",
            "Epoch 449, Loss: 0.0005828700959682465\n",
            "Epoch 450, Loss: 0.00043456119601614773\n",
            "Epoch 451, Loss: 0.0010771467350423336\n",
            "Epoch 452, Loss: 0.0004984806873835623\n",
            "Epoch 453, Loss: 0.000534297083504498\n",
            "Epoch 454, Loss: 0.0003211579460185021\n",
            "Epoch 455, Loss: 0.00031182210659608245\n",
            "Epoch 456, Loss: 0.00040222928510047495\n",
            "Epoch 457, Loss: 0.0008187393541447818\n",
            "Epoch 458, Loss: 0.0009987435769289732\n",
            "Epoch 459, Loss: 0.0005343177472241223\n",
            "Epoch 460, Loss: 0.0005763632943853736\n",
            "Epoch 461, Loss: 0.0007743326714262366\n",
            "Epoch 462, Loss: 0.0008488967432640493\n",
            "Epoch 463, Loss: 0.0009411296923644841\n",
            "Epoch 464, Loss: 0.0008592043304815888\n",
            "Epoch 465, Loss: 0.000542150461114943\n",
            "Epoch 466, Loss: 0.0011586464243009686\n",
            "Epoch 467, Loss: 0.0005943015567027032\n",
            "Epoch 468, Loss: 0.0006590546108782291\n",
            "Epoch 469, Loss: 0.0032561947591602802\n",
            "Epoch 470, Loss: 0.001053981832228601\n",
            "Epoch 471, Loss: 0.0009423168958164752\n",
            "Epoch 472, Loss: 0.00042854901403188705\n",
            "Epoch 473, Loss: 0.00045674043940380216\n",
            "Epoch 474, Loss: 0.0005143059534020722\n",
            "Epoch 475, Loss: 0.00048210466047748923\n",
            "Epoch 476, Loss: 0.0005087374011054635\n",
            "Epoch 477, Loss: 0.0007110597798600793\n",
            "Epoch 478, Loss: 0.00037723054992966354\n",
            "Epoch 479, Loss: 0.0008450881578028202\n",
            "Epoch 480, Loss: 0.00039387636934407055\n",
            "Epoch 481, Loss: 0.00023619913554284722\n",
            "Epoch 482, Loss: 0.00036502614966593683\n",
            "Epoch 483, Loss: 0.00032649392960593104\n",
            "Epoch 484, Loss: 0.00020301504991948605\n",
            "Epoch 485, Loss: 0.0012154901633039117\n",
            "Epoch 486, Loss: 0.0005498069222085178\n",
            "Epoch 487, Loss: 0.00044645153684541583\n",
            "Epoch 488, Loss: 0.0003232594463042915\n",
            "Epoch 489, Loss: 0.0006465107435360551\n",
            "Epoch 490, Loss: 0.0002411972964182496\n",
            "Epoch 491, Loss: 0.0007369061349891126\n",
            "Epoch 492, Loss: 0.0005709638353437185\n",
            "Epoch 493, Loss: 0.0006851993384771049\n",
            "Epoch 494, Loss: 0.00034989730920642614\n",
            "Epoch 495, Loss: 0.00027683950611390173\n",
            "Epoch 496, Loss: 0.0004549941513687372\n",
            "Epoch 497, Loss: 0.0004308722272980958\n",
            "Epoch 498, Loss: 0.000536243780516088\n",
            "Epoch 499, Loss: 0.0005008462467230856\n",
            "Epoch 500, Loss: 0.0019402520265430212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # --- Sort the test MSEs in ascending order ---\n",
        "    sorted_indices = np.argsort(test_mse)\n",
        "    sorted_mse = np.array(test_mse)[sorted_indices]\n",
        "\n",
        "    # --- Plot MSE per test case, sorted ---\n",
        "    plt.figure(figsize=(7.5, 3.2))\n",
        "    plt.plot(range(len(sorted_mse)), sorted_mse, color='darkgreen', marker='.')\n",
        "    plt.xlabel(\"Sorted Test set error (per timeseries)\")\n",
        "    plt.ylabel(\"MSE\")\n",
        "    plt.title(f\"Input-based-error – {model_name}\")\n",
        "    plt.tight_layout(pad=2.5)\n",
        "\n",
        "    # --- Save to Drive ---\n",
        "    filename = f\"drive/My Drive/DSSM-Figures-final/{model_name}_input_error.pdf\"\n",
        "    plt.savefig(filename, format='pdf', bbox_inches='tight')\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "g4eaUkU8p9A3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yFH5xi8bsHM8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}