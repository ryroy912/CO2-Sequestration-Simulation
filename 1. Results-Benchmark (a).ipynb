{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PREPERATION"
      ],
      "metadata": {
        "id": "uHubmQ-gpS0u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCxMIiA1fexX",
        "outputId": "c16b38fa-1e82-41a6-9dea-a687f13e93b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive and loading dataset...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loaded dataset with shape: (1192157, 17)\n",
            "Static feature table created: Input_Link_Table.shape = (2703, 15)\n",
            "Time series matrix constructed: relevant_data.shape = (2703, 101)\n",
            "Performed KMeans clustering into 8 clusters\n",
            "Cluster boundary stats calculated: cluster_boundaries.shape = (8, 4, 101)\n",
            "Final input features (static + cluster): merged_df.shape = (2703, 16)\n",
            "Final output time series: df_output.shape = (273003, 3)\n",
            "Data Preparation Summary:\n",
            "Static Input Table: merged_df [2703 rows × 16 columns]\n",
            "Time Series Output: df_output [273003 rows × 3 columns]\n",
            "Cluster Boundaries: cluster_boundaries [(8, 4, 101)]\n"
          ]
        }
      ],
      "source": [
        "# --------------------- Imports ---------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from google.colab import drive\n",
        "\n",
        "# --------------------- Matplotlib Setup ---------------------\n",
        "mpl.rcParams.update({\n",
        "    'font.size': 14,\n",
        "    'axes.titlesize': 15,\n",
        "    'axes.labelsize': 12,\n",
        "    'xtick.labelsize': 11,\n",
        "    'ytick.labelsize': 11,\n",
        "    'legend.fontsize': 11,\n",
        "    'figure.dpi': 300,\n",
        "    'savefig.dpi': 300,\n",
        "    'figure.autolayout': True,\n",
        "})\n",
        "\n",
        "# --------------------- Load Data ---------------------\n",
        "print(\"Mounting Google Drive and loading dataset...\")\n",
        "drive.mount('/content/drive')\n",
        "total_capture_7k = pd.read_csv('drive/My Drive/correlation_wide.csv')\n",
        "print(f\"Loaded dataset with shape: {total_capture_7k.shape}\")\n",
        "\n",
        "# --------------------- Identify Unique Static Parameter Sets ---------------------\n",
        "static_cols = [\n",
        "    'MikeSorghum', 'Quartz', 'Plagioclase', 'Apatite', 'Ilmenite',\n",
        "    'Diopside_Mn', 'Diopside', 'Olivine', 'Alkali-feldspar',\n",
        "    'Montmorillonite', 'Glass', 'temp', 'shift', 'year'\n",
        "]\n",
        "\n",
        "# Add timestep count per file_id\n",
        "file_lengths = total_capture_7k.groupby('file_id').size().rename(\"num_timesteps\").reset_index()\n",
        "static_rows = total_capture_7k.groupby('file_id')[static_cols].first().reset_index()\n",
        "static_rows = static_rows.merge(file_lengths, on='file_id')\n",
        "\n",
        "# Filter only unique static parameter sets\n",
        "unique_static_rows = static_rows.drop_duplicates(subset=static_cols)\n",
        "unique_file_ids = unique_static_rows['file_id'].tolist()\n",
        "\n",
        "# --------------------- Extract Time Series Data ---------------------\n",
        "filtered_df = total_capture_7k[total_capture_7k['file_id'].isin(unique_file_ids)].copy()\n",
        "\n",
        "# Truncate each group to 101 timesteps\n",
        "filtered_df = filtered_df.groupby('file_id').head(101).reset_index(drop=True)\n",
        "\n",
        "# --------------------- Static Feature Table ---------------------\n",
        "Input_Link_Table = filtered_df.groupby('file_id').agg({col: 'first' for col in static_cols}).reset_index()\n",
        "print(f\"Static feature table created: Input_Link_Table.shape = {Input_Link_Table.shape}\")\n",
        "\n",
        "# --------------------- Time Series Structuring ---------------------\n",
        "result = filtered_df[['Total_CO2_capture', 'year', 'file_id']]\n",
        "file_ids = result['file_id'].unique()\n",
        "num_file_ids = len(file_ids)\n",
        "max_timesteps = 101\n",
        "relevant_data = np.zeros((num_file_ids, max_timesteps))\n",
        "file_id_order = np.zeros(num_file_ids)\n",
        "\n",
        "for i, file_id in enumerate(file_ids):\n",
        "    file_data = result[result['file_id'] == file_id]['Total_CO2_capture'].values\n",
        "    relevant_data[i, :len(file_data)] = file_data\n",
        "    file_id_order[i] = file_id\n",
        "print(f\"Time series matrix constructed: relevant_data.shape = {relevant_data.shape}\")\n",
        "\n",
        "# --------------------- Clustering ---------------------\n",
        "scaler = StandardScaler()\n",
        "normalized_data = scaler.fit_transform(relevant_data)\n",
        "kmeans = KMeans(n_clusters=8, random_state=42)\n",
        "clusters = kmeans.fit_predict(normalized_data)\n",
        "print(\"Performed KMeans clustering into 8 clusters\")\n",
        "\n",
        "# Compute boundary stats\n",
        "cluster_boundaries = []\n",
        "for cluster_id in range(8):\n",
        "    cluster_data = normalized_data[clusters == cluster_id]\n",
        "    min_v = scaler.inverse_transform(np.min(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    median_v = scaler.inverse_transform(np.median(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    mean_v = scaler.inverse_transform(np.mean(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    max_v = scaler.inverse_transform(np.max(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    cluster_boundaries.append((min_v, median_v, mean_v, max_v))\n",
        "cluster_boundaries = np.array(cluster_boundaries)\n",
        "print(f\"Cluster boundary stats calculated: cluster_boundaries.shape = {cluster_boundaries.shape}\")\n",
        "\n",
        "# --------------------- Merge Static Features with Clusters ---------------------\n",
        "Clustering_link_table = pd.DataFrame({'file_id': file_id_order.astype(int), 'cluster': clusters})\n",
        "Clustering_link_table = Clustering_link_table.sort_values(by='file_id').reset_index(drop=True)\n",
        "merged_df = pd.merge(Input_Link_Table, Clustering_link_table, on='file_id')\n",
        "print(f\"Final input features (static + cluster): merged_df.shape = {merged_df.shape}\")\n",
        "\n",
        "# --------------------- Create Output Time Series DataFrame ---------------------\n",
        "data = [[file_id_order[i].astype(int), t, relevant_data[i, t]] for i in range(len(file_id_order)) for t in range(max_timesteps)]\n",
        "df_output = pd.DataFrame(data, columns=['file_id', 'timestep', 'CO2']).sort_values(by=['file_id', 'timestep'])\n",
        "print(f\"Final output time series: df_output.shape = {df_output.shape}\")\n",
        "\n",
        "# --------------------- Summary ---------------------\n",
        "print(\"Data Preparation Summary:\")\n",
        "print(f\"Static Input Table: merged_df [{merged_df.shape[0]} rows × {merged_df.shape[1]} columns]\")\n",
        "print(f\"Time Series Output: df_output [{df_output.shape[0]} rows × 3 columns]\")\n",
        "print(f\"Cluster Boundaries: cluster_boundaries [{cluster_boundaries.shape}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODELS"
      ],
      "metadata": {
        "id": "XAJUIgGhpbYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advanced DSSM model**"
      ],
      "metadata": {
        "id": "u9MwI8wnpgOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedDSSMDeepState(nn.Module):\n",
        "    def __init__(self, input_dim, static_dim, hidden_dim, output_dim):\n",
        "        super(AdvancedDSSMDeepState, self).__init__()\n",
        "\n",
        "        # Static Data Path (Fully connected layers for static features)\n",
        "        self.fc_static1 = nn.Linear(static_dim, 512)\n",
        "        self.fc_static2 = nn.Linear(512, 256)\n",
        "        self.fc_static3 = nn.Linear(256, 128)\n",
        "        self.fc_static4 = nn.Linear(128, 64)\n",
        "\n",
        "        # Time-series Path (Conv1D for feature extraction)\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Deep State Dynamics (LSTM for latent state transitions)\n",
        "        self.lstm_state = nn.LSTM(hidden_dim + 64, hidden_dim, batch_first=True)\n",
        "\n",
        "        # Observation Model (Mapping latent states to outputs)\n",
        "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, time_series_input, static_input):\n",
        "        # Static Data Path\n",
        "        static_out = self.relu(self.fc_static1(static_input))\n",
        "        static_out = self.relu(self.fc_static2(static_out))\n",
        "        static_out = self.relu(self.fc_static3(static_out))\n",
        "        static_out = self.relu(self.fc_static4(static_out))  # Shape: [batch_size, 64]\n",
        "\n",
        "        # Time-Series Data Path\n",
        "        if len(time_series_input.shape) == 2:  # [batch_size, seq_len]\n",
        "            time_series_input = time_series_input.unsqueeze(1)  # Add channel dimension: [batch_size, 1, seq_len]\n",
        "\n",
        "        conv_out = self.conv1(time_series_input)  # Conv1D layer\n",
        "        conv_out = self.relu(conv_out)\n",
        "        conv_out = conv_out.transpose(1, 2)  # Shape: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        # Expand static features to match the sequence length\n",
        "        static_expanded = static_out.unsqueeze(1).expand(-1, conv_out.size(1), -1)  # Shape: [batch_size, seq_len, 64]\n",
        "\n",
        "        # Combine Conv1D features and static features\n",
        "        lstm_input = torch.cat([conv_out, static_expanded], dim=2)  # Shape: [batch_size, seq_len, hidden_dim + 64]\n",
        "\n",
        "        # Latent State Dynamics (LSTM for state transitions)\n",
        "        lstm_out, _ = self.lstm_state(lstm_input)  # Shape: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        # Observation Model\n",
        "        lstm_out_final = lstm_out[:, -1, :]  # Use the last state for prediction\n",
        "        x = self.fc1(lstm_out_final)\n",
        "        x = self.relu(x)\n",
        "        output = self.fc2(x)  # Final prediction\n",
        "\n",
        "        return output\n",
        "\n",
        "def plot_boundary_cases_with_input(inputs, Boundary_case_actuals, Boundary_case_predicted, model_name, input_length):\n",
        "    case_names = [\"Best\", \"Average\", \"Worst\"]\n",
        "    x_range = input_length\n",
        "    y_range = Boundary_case_actuals.shape[1]\n",
        "    total_timesteps = x_range + y_range\n",
        "\n",
        "    for i in range(3):\n",
        "        plt.figure(figsize=(7.5, 3.2))\n",
        "\n",
        "        # Plot input (X)\n",
        "        plt.plot(range(x_range), inputs[i], color='black', alpha=0.5, label='Input')\n",
        "\n",
        "        # Plot output actual vs predicted (Y)\n",
        "        plt.plot(range(x_range, total_timesteps), Boundary_case_actuals[i], color='blue', alpha=0.8, label='Actual')\n",
        "        plt.plot(range(x_range, total_timesteps), Boundary_case_predicted[i], color='red', alpha=0.8, label='Predicted')\n",
        "\n",
        "        plt.xlabel(\"Time Steps\")\n",
        "        plt.ylabel(\"CO₂ Sequestration\")\n",
        "        plt.title(f\"{case_names[i]} Case – {model_name}\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout(pad=2.5)\n",
        "\n",
        "        filename = f\"drive/My Drive/DSSM-Figures1/{model_name}_{case_names[i]}.pdf\"\n",
        "        plt.savefig(filename, format='pdf', bbox_inches='tight')\n",
        "        plt.close()"
      ],
      "metadata": {
        "id": "Iyn-qdunpkLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DSSM basic**"
      ],
      "metadata": {
        "id": "GEpGGxlvwKHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Define the Deep State Space Model\n",
        "class DeepStateSpaceModel(nn.Module):\n",
        "    def __init__(self, input_dim, static_dim, hidden_dim, output_dim):\n",
        "        super(DeepStateSpaceModel, self).__init__()\n",
        "\n",
        "        # State Transition Model (LSTM for latent state evolution)\n",
        "        self.state_transition = nn.LSTM(input_dim + static_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "        # Observation Model (Fully connected layers for mapping latent state to output)\n",
        "        self.fc_obs1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc_obs2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        # ReLU activation for nonlinearity\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, time_series_input, static_input):\n",
        "        # Validate and adjust time_series_input shape if needed\n",
        "        if len(time_series_input.shape) == 2:  # [batch_size, seq_len]\n",
        "            time_series_input = time_series_input.unsqueeze(-1)  # Add feature dimension: [batch_size, seq_len, 1]\n",
        "\n",
        "        # Expand static features to match sequence length\n",
        "        static_expanded = static_input.unsqueeze(1).expand(-1, time_series_input.size(1), -1)  # [batch_size, seq_len, static_dim]\n",
        "\n",
        "        # Concatenate static and time-series inputs\n",
        "        lstm_input = torch.cat([time_series_input, static_expanded], dim=2)  # [batch_size, seq_len, input_dim + static_dim]\n",
        "\n",
        "        # State Transition Dynamics\n",
        "        state_output, _ = self.state_transition(lstm_input)  # [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        # Observation Model (map latent states to predictions)\n",
        "        obs_out = self.relu(self.fc_obs1(state_output))  # [batch_size, seq_len, hidden_dim]\n",
        "        predictions = self.fc_obs2(obs_out)  # [batch_size, seq_len, output_dim]\n",
        "\n",
        "        # Return only the prediction for the last time step\n",
        "        return predictions[:, -1, :]  # [batch_size, output_dim]"
      ],
      "metadata": {
        "id": "-5emWXatwI0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TFT**"
      ],
      "metadata": {
        "id": "I0pU-Oc22TKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GatedResidualNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim=None, dropout=0.1):\n",
        "        super().__init__()\n",
        "        if output_dim is None:\n",
        "            output_dim = input_dim\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.elu = nn.ELU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.gate = nn.Linear(output_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.skip = nn.Linear(input_dim, output_dim) if input_dim != output_dim else None\n",
        "        self.norm = nn.LayerNorm(output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x if self.skip is None else self.skip(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.elu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        gate = self.sigmoid(self.gate(x))\n",
        "        x = gate * x\n",
        "        x = x + residual\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TemporalFusionTransformer(nn.Module):\n",
        "    def __init__(self, input_size, output_size, static_size, hidden_size=64, num_heads=4, dropout=0.1, num_lstm_layers=1):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(1, hidden_size)\n",
        "        self.static_encoder = nn.Sequential(\n",
        "            nn.Linear(static_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size)\n",
        "        )\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=hidden_size + hidden_size,\n",
        "                            hidden_size=hidden_size,\n",
        "                            num_layers=num_lstm_layers,\n",
        "                            batch_first=True)\n",
        "\n",
        "        self.post_lstm_grn = GatedResidualNetwork(hidden_size, hidden_size, hidden_size, dropout)\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
        "        self.attn_grn = GatedResidualNetwork(hidden_size, hidden_size, hidden_size, dropout)\n",
        "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x_seq, x_static):\n",
        "        # x_seq: [B, T_in, 1]\n",
        "        # x_static: [B, static_size]\n",
        "        batch_size, seq_len, _ = x_seq.size()\n",
        "\n",
        "        x_seq_emb = self.input_proj(x_seq)  # [B, T_in, H]\n",
        "        x_static_emb = self.static_encoder(x_static)  # [B, H]\n",
        "        x_static_expanded = x_static_emb.unsqueeze(1).expand(-1, seq_len, -1)  # [B, T_in, H]\n",
        "\n",
        "        x_combined = torch.cat([x_seq_emb, x_static_expanded], dim=-1)  # [B, T_in, 2H]\n",
        "        lstm_out, _ = self.lstm(x_combined)  # [B, T_in, H]\n",
        "        lstm_out = self.post_lstm_grn(lstm_out)  # GRN layer\n",
        "\n",
        "        # Self-attention\n",
        "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
        "        attn_out = self.attn_grn(attn_out)  # GRN layer again\n",
        "\n",
        "        final_out = self.output_layer(attn_out[:, -1, :])  # final timestep only\n",
        "        return final_out\n"
      ],
      "metadata": {
        "id": "TshK3cIW2VpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLinear**"
      ],
      "metadata": {
        "id": "TToXRA993wzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NLinear(nn.Module):\n",
        "    def __init__(self, seq_len, pred_len, individual=False):\n",
        "        super(NLinear, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.individual = individual\n",
        "\n",
        "        if self.individual:\n",
        "            self.Linear = nn.ModuleList()\n",
        "            for i in range(1):  # Univariate case\n",
        "                self.Linear.append(nn.Linear(self.seq_len, self.pred_len))\n",
        "        else:\n",
        "            self.Linear = nn.Linear(self.seq_len, self.pred_len)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) == 3:\n",
        "            x = x.squeeze(-1)\n",
        "\n",
        "        seq_last = x[:, -1:].detach()\n",
        "        x = x - seq_last\n",
        "\n",
        "        if self.individual:\n",
        "            out = torch.zeros([x.size(0), self.pred_len], dtype=x.dtype).to(x.device)\n",
        "            for i in range(1):\n",
        "                out[:, :] = self.Linear[i](x)\n",
        "        else:\n",
        "            out = self.Linear(x)\n",
        "\n",
        "        out = out + seq_last\n",
        "        return out.unsqueeze(-1)"
      ],
      "metadata": {
        "id": "GRtDheQx3zj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TCN**"
      ],
      "metadata": {
        "id": "uD6dzLzfB57H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Chomp1d(nn.Module):\n",
        "    def __init__(self, chomp_size):\n",
        "        super().__init__()\n",
        "        self.chomp_size = chomp_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :-self.chomp_size].contiguous()\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, dilation, padding, dropout):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                               stride=stride, padding=padding, dilation=dilation)\n",
        "        self.chomp1 = Chomp1d(padding)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size,\n",
        "                               stride=stride, padding=padding, dilation=dilation)\n",
        "        self.chomp2 = Chomp1d(padding)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
        "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
        "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        return self.relu(out + res)\n",
        "\n",
        "class TCN_MIMO(nn.Module):\n",
        "    def __init__(self, input_len, output_len, static_dim, num_channels, kernel_size=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.static_fc = nn.Sequential(\n",
        "            nn.Linear(static_dim, num_channels[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(num_channels[0], num_channels[0])\n",
        "        )\n",
        "\n",
        "        layers = []\n",
        "        num_levels = len(num_channels)\n",
        "        for i in range(num_levels):\n",
        "            dilation_size = 2 ** i\n",
        "            in_ch = 1 if i == 0 else num_channels[i-1]\n",
        "            out_ch = num_channels[i]\n",
        "            layers += [TemporalBlock(in_ch, out_ch, kernel_size, stride=1,\n",
        "                                     dilation=dilation_size, padding=(kernel_size-1)*dilation_size, dropout=dropout)]\n",
        "        self.tcn = nn.Sequential(*layers)\n",
        "\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(num_channels[-1] + num_channels[0], 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_len)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_seq, x_static):\n",
        "        # x_seq: [B, T, 1], x_static: [B, static_dim]\n",
        "        x_seq = x_seq.transpose(1, 2)  # [B, 1, T]\n",
        "        tcn_out = self.tcn(x_seq)     # [B, C, T]\n",
        "        tcn_out = tcn_out[:, :, -1]   # [B, C] final time step only\n",
        "\n",
        "        static_out = self.static_fc(x_static)  # [B, C]\n",
        "        combined = torch.cat([tcn_out, static_out], dim=1)  # [B, 2C]\n",
        "        return self.output_layer(combined)  # [B, output_len]\n"
      ],
      "metadata": {
        "id": "Zf4YiyKuB6Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM**"
      ],
      "metadata": {
        "id": "gFPd5kg3Dg97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTM_MIMO(nn.Module):\n",
        "    def __init__(self, input_len, output_len, static_dim, hidden_dim=128, num_layers=2):\n",
        "        super(LSTM_MIMO, self).__init__()\n",
        "        self.input_len = input_len\n",
        "        self.output_len = output_len\n",
        "\n",
        "        self.static_fc = nn.Sequential(\n",
        "            nn.Linear(static_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=1 + hidden_dim, hidden_size=hidden_dim,\n",
        "                            num_layers=num_layers, batch_first=True)\n",
        "\n",
        "        self.output_fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_len)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_seq, x_static):\n",
        "        # x_seq: [B, T, 1]\n",
        "        # x_static: [B, static_dim]\n",
        "        batch_size, seq_len, _ = x_seq.size()\n",
        "        static_encoded = self.static_fc(x_static)  # [B, H]\n",
        "        static_expanded = static_encoded.unsqueeze(1).expand(-1, seq_len, -1)  # [B, T, H]\n",
        "        lstm_input = torch.cat([x_seq, static_expanded], dim=-1)  # [B, T, 1+H]\n",
        "        lstm_out, _ = self.lstm(lstm_input)  # [B, T, H]\n",
        "        last_hidden = lstm_out[:, -1, :]  # [B, H]\n",
        "        out = self.output_fc(last_hidden)  # [B, output_len]\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "4TiIeh11DhF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments - Overnight duration experimentation"
      ],
      "metadata": {
        "id": "A9_s-5G2t5jb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advanced-DSSM-MSE-experiment (best model, saved model, hyperparameter listing, training status report)**"
      ],
      "metadata": {
        "id": "zR-V3lwKt9Mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define fixed hyperparameters (excluding input/output dimensions)\n",
        "dssm_advanced_config = {\n",
        "    'hidden_dim': 101,\n",
        "    'epochs': 500,\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 0.001,\n",
        "    'model_name': 'DSSM_Advanced'\n",
        "}\n",
        "\n",
        "# DataFrame to store results\n",
        "DSSM_Advanced_mse = pd.DataFrame(columns=['Split', 'Test_MSE'])\n",
        "\n",
        "# Output path\n",
        "drive_path = '/content/drive/MyDrive/DSSM-Figures'\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "# Define splits\n",
        "splits = [ (80, 20),(60, 40), (50, 50), (40, 60), (20, 80), (10, 90), (5, 95), (3, 97), (1, 99)]\n",
        "\n",
        "for train_pct, test_pct in splits:\n",
        "    split_name = f\"{train_pct}_{test_pct}\"\n",
        "    print(f\"\\n==== Running Split: {split_name} ====\")\n",
        "\n",
        "    file_ids = df_output['file_id'].unique()\n",
        "    trainval_ids, test_ids = train_test_split(file_ids, test_size=0.2, random_state=42)\n",
        "    train_ids, val_ids = train_test_split(trainval_ids, test_size=0.2, random_state=42)\n",
        "\n",
        "    def extract_X_Y(ids, pct):\n",
        "        df_subset = df_output[df_output['file_id'].isin(ids)]\n",
        "        pivoted = df_subset.pivot(index='file_id', columns='timestep', values='CO2').values\n",
        "        split_idx = int(pct / 100 * 101)\n",
        "        X = pivoted[:, :split_idx]\n",
        "        Y = pivoted[:, split_idx:]\n",
        "        return X, Y\n",
        "\n",
        "    X_train, Y_train = extract_X_Y(train_ids, train_pct)\n",
        "    X_val, Y_val = extract_X_Y(val_ids, train_pct)\n",
        "    X_test, Y_test = extract_X_Y(test_ids, train_pct)\n",
        "\n",
        "    static_train = merged_df[merged_df['file_id'].isin(train_ids)].drop(columns=['file_id', 'cluster']).values\n",
        "    static_val = merged_df[merged_df['file_id'].isin(val_ids)].drop(columns=['file_id', 'cluster']).values\n",
        "    static_test = merged_df[merged_df['file_id'].isin(test_ids)].drop(columns=['file_id', 'cluster']).values\n",
        "\n",
        "    print(f\"Split {split_name} — X (INPUT): {X_train.shape[1]}, Y (OUTPUT): {Y_train.shape[1]}\")\n",
        "    print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "    # Convert to tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
        "    static_train_tensor = torch.tensor(static_train, dtype=torch.float32)\n",
        "\n",
        "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "    Y_val_tensor = torch.tensor(Y_val, dtype=torch.float32)\n",
        "    static_val_tensor = torch.tensor(static_val, dtype=torch.float32)\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
        "    static_test_tensor = torch.tensor(static_test, dtype=torch.float32)\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(TensorDataset(X_train_tensor, static_train_tensor, Y_train_tensor),\n",
        "                              batch_size=dssm_advanced_config['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(TensorDataset(X_val_tensor, static_val_tensor, Y_val_tensor),\n",
        "                            batch_size=dssm_advanced_config['batch_size'])\n",
        "\n",
        "    # Model setup\n",
        "    model = AdvancedDSSMDeepState(\n",
        "        input_dim=X_train.shape[1],\n",
        "        static_dim=static_train.shape[1],\n",
        "        hidden_dim=dssm_advanced_config['hidden_dim'],\n",
        "        output_dim=Y_train.shape[1]\n",
        "    )\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=dssm_advanced_config['learning_rate'])\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "\n",
        "    print(\"Training started...\")\n",
        "    for epoch in range(dssm_advanced_config['epochs']):\n",
        "        model.train()\n",
        "        for X_batch, static_batch, Y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X_batch, static_batch)\n",
        "            loss = criterion(preds, Y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, static_batch, Y_batch in val_loader:\n",
        "                preds = model(X_batch, static_batch)\n",
        "                val_loss += criterion(preds, Y_batch).item()\n",
        "            val_loss /= len(val_loader)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch + 1}: Train Loss = {loss.item():.6f}, Val Loss = {val_loss:.6f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict()\n",
        "            #print(f\"New best model found at epoch {epoch + 1} with val loss {val_loss:.6f}\")\n",
        "            torch.save(best_model_state, os.path.join(drive_path, f\"best_model_{split_name}.pt\"))\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load(os.path.join(drive_path, f\"best_model_{split_name}.pt\")))\n",
        "    model.eval()\n",
        "\n",
        "    # Final test loss\n",
        "    with torch.no_grad():\n",
        "        test_preds = model(X_test_tensor, static_test_tensor)\n",
        "        test_loss = criterion(test_preds, Y_test_tensor).item()\n",
        "\n",
        "    DSSM_Advanced_mse.loc[len(DSSM_Advanced_mse)] = [split_name, test_loss]\n",
        "    print(f\"Split {split_name} — Final Test MSE: {test_loss:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJjwaLJnFqkS",
        "outputId": "5b157997-b25a-4f5a-b876-967b7afd18f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Running Split: 80_20 ====\n",
            "Split 80_20 — X (INPUT): 80, Y (OUTPUT): 21\n",
            "Train: (1729, 80), Val: (433, 80), Test: (541, 80)\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.000259, Val Loss = 0.003496\n",
            "Epoch 20: Train Loss = 0.002615, Val Loss = 0.007881\n",
            "Epoch 30: Train Loss = 0.000061, Val Loss = 0.000755\n",
            "Epoch 40: Train Loss = 0.000161, Val Loss = 0.000293\n",
            "Epoch 50: Train Loss = 0.000015, Val Loss = 0.000746\n",
            "Epoch 60: Train Loss = 0.000051, Val Loss = 0.000164\n",
            "Epoch 70: Train Loss = 0.000059, Val Loss = 0.000160\n",
            "Epoch 80: Train Loss = 0.000551, Val Loss = 0.001038\n",
            "Epoch 90: Train Loss = 0.000009, Val Loss = 0.000293\n",
            "Epoch 100: Train Loss = 0.000015, Val Loss = 0.000111\n",
            "Epoch 110: Train Loss = 0.000047, Val Loss = 0.000075\n",
            "Epoch 120: Train Loss = 0.000028, Val Loss = 0.000098\n",
            "Epoch 130: Train Loss = 0.000366, Val Loss = 0.000365\n",
            "Epoch 140: Train Loss = 0.000185, Val Loss = 0.000829\n",
            "Epoch 150: Train Loss = 0.000041, Val Loss = 0.000240\n",
            "Epoch 160: Train Loss = 0.000002, Val Loss = 0.000061\n",
            "Epoch 170: Train Loss = 0.000018, Val Loss = 0.000485\n",
            "Epoch 180: Train Loss = 0.001042, Val Loss = 0.000597\n",
            "Epoch 190: Train Loss = 0.000021, Val Loss = 0.000199\n",
            "Epoch 200: Train Loss = 0.000232, Val Loss = 0.000774\n",
            "Epoch 210: Train Loss = 0.000178, Val Loss = 0.000069\n",
            "Epoch 220: Train Loss = 0.000058, Val Loss = 0.000152\n",
            "Epoch 230: Train Loss = 0.000074, Val Loss = 0.000085\n",
            "Epoch 240: Train Loss = 0.000208, Val Loss = 0.000586\n",
            "Epoch 250: Train Loss = 0.000320, Val Loss = 0.000089\n",
            "Epoch 260: Train Loss = 0.000189, Val Loss = 0.000320\n",
            "Epoch 270: Train Loss = 0.000037, Val Loss = 0.000043\n",
            "Epoch 280: Train Loss = 0.000140, Val Loss = 0.000437\n",
            "Epoch 290: Train Loss = 0.000238, Val Loss = 0.000128\n",
            "Epoch 300: Train Loss = 0.000005, Val Loss = 0.000090\n",
            "Epoch 310: Train Loss = 0.000264, Val Loss = 0.000681\n",
            "Epoch 320: Train Loss = 0.000145, Val Loss = 0.000902\n",
            "Epoch 330: Train Loss = 0.000014, Val Loss = 0.000051\n",
            "Epoch 340: Train Loss = 0.000179, Val Loss = 0.000813\n",
            "Epoch 350: Train Loss = 0.000008, Val Loss = 0.000051\n",
            "Epoch 360: Train Loss = 0.000067, Val Loss = 0.000064\n",
            "Epoch 370: Train Loss = 0.000059, Val Loss = 0.000321\n",
            "Epoch 380: Train Loss = 0.000154, Val Loss = 0.000785\n",
            "Epoch 390: Train Loss = 0.000004, Val Loss = 0.000134\n",
            "Epoch 400: Train Loss = 0.000025, Val Loss = 0.000123\n",
            "Epoch 410: Train Loss = 0.000143, Val Loss = 0.000088\n",
            "Epoch 420: Train Loss = 0.000001, Val Loss = 0.000050\n",
            "Epoch 430: Train Loss = 0.000010, Val Loss = 0.000100\n",
            "Epoch 440: Train Loss = 0.000014, Val Loss = 0.000046\n",
            "Epoch 450: Train Loss = 0.000073, Val Loss = 0.000185\n",
            "Epoch 460: Train Loss = 0.000030, Val Loss = 0.000037\n",
            "Epoch 470: Train Loss = 0.000193, Val Loss = 0.001953\n",
            "Epoch 480: Train Loss = 0.000009, Val Loss = 0.000038\n",
            "Epoch 490: Train Loss = 0.000007, Val Loss = 0.000047\n",
            "Epoch 500: Train Loss = 0.000005, Val Loss = 0.000114\n",
            "Split 80_20 — Final Test MSE: 0.000020\n",
            "\n",
            "==== Running Split: 60_40 ====\n",
            "Split 60_40 — X (INPUT): 60, Y (OUTPUT): 41\n",
            "Train: (1729, 60), Val: (433, 60), Test: (541, 60)\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.000653, Val Loss = 0.008897\n",
            "Epoch 20: Train Loss = 0.001422, Val Loss = 0.006674\n",
            "Epoch 30: Train Loss = 0.000508, Val Loss = 0.003961\n",
            "Epoch 40: Train Loss = 0.000819, Val Loss = 0.004248\n",
            "Epoch 50: Train Loss = 0.000890, Val Loss = 0.004142\n",
            "Epoch 60: Train Loss = 0.000460, Val Loss = 0.003106\n",
            "Epoch 70: Train Loss = 0.000727, Val Loss = 0.002386\n",
            "Epoch 80: Train Loss = 0.000200, Val Loss = 0.002068\n",
            "Epoch 90: Train Loss = 0.000305, Val Loss = 0.001704\n",
            "Epoch 100: Train Loss = 0.001547, Val Loss = 0.001563\n",
            "Epoch 110: Train Loss = 0.000078, Val Loss = 0.000916\n",
            "Epoch 120: Train Loss = 0.000305, Val Loss = 0.001121\n",
            "Epoch 130: Train Loss = 0.000036, Val Loss = 0.000727\n",
            "Epoch 140: Train Loss = 0.000230, Val Loss = 0.001601\n",
            "Epoch 150: Train Loss = 0.000065, Val Loss = 0.000595\n",
            "Epoch 160: Train Loss = 0.000373, Val Loss = 0.000926\n",
            "Epoch 170: Train Loss = 0.000050, Val Loss = 0.000571\n",
            "Epoch 180: Train Loss = 0.000189, Val Loss = 0.000832\n",
            "Epoch 190: Train Loss = 0.000353, Val Loss = 0.000680\n",
            "Epoch 200: Train Loss = 0.000029, Val Loss = 0.000451\n",
            "Epoch 210: Train Loss = 0.000020, Val Loss = 0.000656\n",
            "Epoch 220: Train Loss = 0.000134, Val Loss = 0.001076\n",
            "Epoch 230: Train Loss = 0.000032, Val Loss = 0.000451\n",
            "Epoch 240: Train Loss = 0.000019, Val Loss = 0.000424\n",
            "Epoch 250: Train Loss = 0.000029, Val Loss = 0.000464\n",
            "Epoch 260: Train Loss = 0.000451, Val Loss = 0.001380\n",
            "Epoch 270: Train Loss = 0.000165, Val Loss = 0.003472\n",
            "Epoch 280: Train Loss = 0.000092, Val Loss = 0.000429\n",
            "Epoch 290: Train Loss = 0.000132, Val Loss = 0.000379\n",
            "Epoch 300: Train Loss = 0.000088, Val Loss = 0.000439\n",
            "Epoch 310: Train Loss = 0.000024, Val Loss = 0.000779\n",
            "Epoch 320: Train Loss = 0.000103, Val Loss = 0.000409\n",
            "Epoch 330: Train Loss = 0.000025, Val Loss = 0.000913\n",
            "Epoch 340: Train Loss = 0.000134, Val Loss = 0.000708\n",
            "Epoch 350: Train Loss = 0.000348, Val Loss = 0.000800\n",
            "Epoch 360: Train Loss = 0.000771, Val Loss = 0.001108\n",
            "Epoch 370: Train Loss = 0.000091, Val Loss = 0.000315\n",
            "Epoch 380: Train Loss = 0.000320, Val Loss = 0.000847\n",
            "Epoch 390: Train Loss = 0.000930, Val Loss = 0.004592\n",
            "Epoch 400: Train Loss = 0.000231, Val Loss = 0.000352\n",
            "Epoch 410: Train Loss = 0.000006, Val Loss = 0.000253\n",
            "Epoch 420: Train Loss = 0.000016, Val Loss = 0.000267\n",
            "Epoch 430: Train Loss = 0.000020, Val Loss = 0.000552\n",
            "Epoch 440: Train Loss = 0.000017, Val Loss = 0.000330\n",
            "Epoch 450: Train Loss = 0.000006, Val Loss = 0.000400\n",
            "Epoch 460: Train Loss = 0.000039, Val Loss = 0.000277\n",
            "Epoch 470: Train Loss = 0.000581, Val Loss = 0.002172\n",
            "Epoch 480: Train Loss = 0.000177, Val Loss = 0.000273\n",
            "Epoch 490: Train Loss = 0.000113, Val Loss = 0.001530\n",
            "Epoch 500: Train Loss = 0.000091, Val Loss = 0.000662\n",
            "Split 60_40 — Final Test MSE: 0.000158\n",
            "\n",
            "==== Running Split: 50_50 ====\n",
            "Split 50_50 — X (INPUT): 50, Y (OUTPUT): 51\n",
            "Train: (1729, 50), Val: (433, 50), Test: (541, 50)\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.001387, Val Loss = 0.008929\n",
            "Epoch 20: Train Loss = 0.000764, Val Loss = 0.007128\n",
            "Epoch 30: Train Loss = 0.001315, Val Loss = 0.009902\n",
            "Epoch 40: Train Loss = 0.023745, Val Loss = 0.039362\n",
            "Epoch 50: Train Loss = 0.000112, Val Loss = 0.002994\n",
            "Epoch 60: Train Loss = 0.014670, Val Loss = 0.019757\n",
            "Epoch 70: Train Loss = 0.000294, Val Loss = 0.001362\n",
            "Epoch 80: Train Loss = 0.000544, Val Loss = 0.001499\n",
            "Epoch 90: Train Loss = 0.001118, Val Loss = 0.002665\n",
            "Epoch 100: Train Loss = 0.000021, Val Loss = 0.001819\n",
            "Epoch 110: Train Loss = 0.000089, Val Loss = 0.001294\n",
            "Epoch 120: Train Loss = 0.000180, Val Loss = 0.002324\n",
            "Epoch 130: Train Loss = 0.000487, Val Loss = 0.001436\n",
            "Epoch 140: Train Loss = 0.000357, Val Loss = 0.001185\n",
            "Epoch 150: Train Loss = 0.000135, Val Loss = 0.001720\n",
            "Epoch 160: Train Loss = 0.000062, Val Loss = 0.000918\n",
            "Epoch 170: Train Loss = 0.000876, Val Loss = 0.001495\n",
            "Epoch 180: Train Loss = 0.000513, Val Loss = 0.003994\n",
            "Epoch 190: Train Loss = 0.000031, Val Loss = 0.000864\n",
            "Epoch 200: Train Loss = 0.000297, Val Loss = 0.000981\n",
            "Epoch 210: Train Loss = 0.000249, Val Loss = 0.000944\n",
            "Epoch 220: Train Loss = 0.000950, Val Loss = 0.002783\n",
            "Epoch 230: Train Loss = 0.000083, Val Loss = 0.000843\n",
            "Epoch 240: Train Loss = 0.000026, Val Loss = 0.000846\n",
            "Epoch 250: Train Loss = 0.000115, Val Loss = 0.000832\n",
            "Epoch 260: Train Loss = 0.000109, Val Loss = 0.000914\n",
            "Epoch 270: Train Loss = 0.000019, Val Loss = 0.000863\n",
            "Epoch 280: Train Loss = 0.000361, Val Loss = 0.000920\n",
            "Epoch 290: Train Loss = 0.000434, Val Loss = 0.000890\n",
            "Epoch 300: Train Loss = 0.000092, Val Loss = 0.000859\n",
            "Epoch 310: Train Loss = 0.000029, Val Loss = 0.000831\n",
            "Epoch 320: Train Loss = 0.000609, Val Loss = 0.001673\n",
            "Epoch 330: Train Loss = 0.000013, Val Loss = 0.000649\n",
            "Epoch 340: Train Loss = 0.000036, Val Loss = 0.000499\n",
            "Epoch 350: Train Loss = 0.000068, Val Loss = 0.000570\n",
            "Epoch 360: Train Loss = 0.001022, Val Loss = 0.000654\n",
            "Epoch 370: Train Loss = 0.000135, Val Loss = 0.000543\n",
            "Epoch 380: Train Loss = 0.000016, Val Loss = 0.000554\n",
            "Epoch 390: Train Loss = 0.000048, Val Loss = 0.000574\n",
            "Epoch 400: Train Loss = 0.000024, Val Loss = 0.001128\n",
            "Epoch 410: Train Loss = 0.000159, Val Loss = 0.000873\n",
            "Epoch 420: Train Loss = 0.000028, Val Loss = 0.000851\n",
            "Epoch 430: Train Loss = 0.000131, Val Loss = 0.000599\n",
            "Epoch 440: Train Loss = 0.000070, Val Loss = 0.000606\n",
            "Epoch 450: Train Loss = 0.000011, Val Loss = 0.000646\n",
            "Epoch 460: Train Loss = 0.000086, Val Loss = 0.002075\n",
            "Epoch 470: Train Loss = 0.000061, Val Loss = 0.000523\n",
            "Epoch 480: Train Loss = 0.000127, Val Loss = 0.000714\n",
            "Epoch 490: Train Loss = 0.000347, Val Loss = 0.001837\n",
            "Epoch 500: Train Loss = 0.000081, Val Loss = 0.000926\n",
            "Split 50_50 — Final Test MSE: 0.000543\n",
            "\n",
            "==== Running Split: 40_60 ====\n",
            "Split 40_60 — X (INPUT): 40, Y (OUTPUT): 61\n",
            "Train: (1729, 40), Val: (433, 40), Test: (541, 40)\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.001993, Val Loss = 0.019105\n",
            "Epoch 20: Train Loss = 0.003164, Val Loss = 0.009843\n",
            "Epoch 30: Train Loss = 0.000905, Val Loss = 0.010189\n",
            "Epoch 40: Train Loss = 0.000806, Val Loss = 0.007124\n",
            "Epoch 50: Train Loss = 0.002681, Val Loss = 0.010635\n",
            "Epoch 60: Train Loss = 0.000346, Val Loss = 0.004014\n",
            "Epoch 70: Train Loss = 0.001017, Val Loss = 0.002907\n",
            "Epoch 80: Train Loss = 0.000156, Val Loss = 0.002542\n",
            "Epoch 90: Train Loss = 0.000322, Val Loss = 0.002276\n",
            "Epoch 100: Train Loss = 0.002181, Val Loss = 0.002897\n",
            "Epoch 110: Train Loss = 0.000708, Val Loss = 0.002339\n",
            "Epoch 120: Train Loss = 0.005875, Val Loss = 0.004194\n",
            "Epoch 130: Train Loss = 0.000974, Val Loss = 0.002004\n",
            "Epoch 140: Train Loss = 0.000385, Val Loss = 0.002212\n",
            "Epoch 150: Train Loss = 0.001716, Val Loss = 0.002879\n",
            "Epoch 160: Train Loss = 0.000445, Val Loss = 0.002224\n",
            "Epoch 170: Train Loss = 0.000985, Val Loss = 0.002238\n",
            "Epoch 180: Train Loss = 0.001032, Val Loss = 0.002317\n",
            "Epoch 190: Train Loss = 0.000290, Val Loss = 0.001986\n",
            "Epoch 200: Train Loss = 0.012192, Val Loss = 0.055544\n",
            "Epoch 210: Train Loss = 0.000064, Val Loss = 0.001614\n",
            "Epoch 220: Train Loss = 0.000870, Val Loss = 0.001532\n",
            "Epoch 230: Train Loss = 0.002150, Val Loss = 0.008360\n",
            "Epoch 240: Train Loss = 0.000742, Val Loss = 0.001589\n",
            "Epoch 250: Train Loss = 0.000952, Val Loss = 0.004213\n",
            "Epoch 260: Train Loss = 0.000162, Val Loss = 0.001830\n",
            "Epoch 270: Train Loss = 0.000420, Val Loss = 0.002624\n",
            "Epoch 280: Train Loss = 0.000057, Val Loss = 0.002031\n",
            "Epoch 290: Train Loss = 0.003393, Val Loss = 0.004982\n",
            "Epoch 300: Train Loss = 0.000102, Val Loss = 0.002607\n",
            "Epoch 310: Train Loss = 0.000082, Val Loss = 0.001262\n",
            "Epoch 320: Train Loss = 0.000031, Val Loss = 0.001535\n",
            "Epoch 330: Train Loss = 0.000448, Val Loss = 0.001253\n",
            "Epoch 340: Train Loss = 0.000260, Val Loss = 0.001197\n",
            "Epoch 350: Train Loss = 0.000753, Val Loss = 0.002940\n",
            "Epoch 360: Train Loss = 0.000318, Val Loss = 0.001396\n",
            "Epoch 370: Train Loss = 0.000288, Val Loss = 0.001498\n",
            "Epoch 380: Train Loss = 0.001763, Val Loss = 0.006647\n",
            "Epoch 390: Train Loss = 0.000024, Val Loss = 0.001860\n",
            "Epoch 400: Train Loss = 0.000475, Val Loss = 0.001129\n",
            "Epoch 410: Train Loss = 0.000008, Val Loss = 0.001068\n",
            "Epoch 420: Train Loss = 0.000012, Val Loss = 0.000979\n",
            "Epoch 430: Train Loss = 0.000120, Val Loss = 0.001033\n",
            "Epoch 440: Train Loss = 0.000068, Val Loss = 0.001224\n",
            "Epoch 450: Train Loss = 0.000035, Val Loss = 0.002645\n",
            "Epoch 460: Train Loss = 0.000523, Val Loss = 0.001759\n",
            "Epoch 470: Train Loss = 0.000095, Val Loss = 0.000977\n",
            "Epoch 480: Train Loss = 0.001040, Val Loss = 0.002439\n",
            "Epoch 490: Train Loss = 0.000013, Val Loss = 0.001148\n",
            "Epoch 500: Train Loss = 0.000163, Val Loss = 0.001104\n",
            "Split 40_60 — Final Test MSE: 0.000761\n",
            "\n",
            "==== Running Split: 20_80 ====\n",
            "Split 20_80 — X (INPUT): 20, Y (OUTPUT): 81\n",
            "Train: (1729, 20), Val: (433, 20), Test: (541, 20)\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.002239, Val Loss = 0.074572\n",
            "Epoch 20: Train Loss = 0.001565, Val Loss = 0.020542\n",
            "Epoch 30: Train Loss = 0.030891, Val Loss = 0.024055\n",
            "Epoch 40: Train Loss = 0.000191, Val Loss = 0.016985\n",
            "Epoch 50: Train Loss = 0.004695, Val Loss = 0.019326\n",
            "Epoch 60: Train Loss = 0.000094, Val Loss = 0.014624\n",
            "Epoch 70: Train Loss = 0.002478, Val Loss = 0.011673\n",
            "Epoch 80: Train Loss = 0.020692, Val Loss = 0.019758\n",
            "Epoch 90: Train Loss = 0.000495, Val Loss = 0.010252\n",
            "Epoch 100: Train Loss = 0.003068, Val Loss = 0.010307\n",
            "Epoch 110: Train Loss = 0.003335, Val Loss = 0.012579\n",
            "Epoch 120: Train Loss = 0.007002, Val Loss = 0.016027\n",
            "Epoch 130: Train Loss = 0.000630, Val Loss = 0.008523\n",
            "Epoch 140: Train Loss = 0.002090, Val Loss = 0.005916\n",
            "Epoch 150: Train Loss = 0.009289, Val Loss = 0.010898\n",
            "Epoch 160: Train Loss = 0.026689, Val Loss = 0.017908\n",
            "Epoch 170: Train Loss = 0.000928, Val Loss = 0.005117\n",
            "Epoch 180: Train Loss = 0.000217, Val Loss = 0.004987\n",
            "Epoch 190: Train Loss = 0.000825, Val Loss = 0.006477\n",
            "Epoch 200: Train Loss = 0.002468, Val Loss = 0.007161\n",
            "Epoch 210: Train Loss = 0.000450, Val Loss = 0.005544\n",
            "Epoch 220: Train Loss = 0.001165, Val Loss = 0.006020\n",
            "Epoch 230: Train Loss = 0.000672, Val Loss = 0.005143\n",
            "Epoch 240: Train Loss = 0.000791, Val Loss = 0.005448\n",
            "Epoch 250: Train Loss = 0.001329, Val Loss = 0.005346\n",
            "Epoch 260: Train Loss = 0.000283, Val Loss = 0.005082\n",
            "Epoch 270: Train Loss = 0.008138, Val Loss = 0.011763\n",
            "Epoch 280: Train Loss = 0.000496, Val Loss = 0.005845\n",
            "Epoch 290: Train Loss = 0.001101, Val Loss = 0.005020\n",
            "Epoch 300: Train Loss = 0.000507, Val Loss = 0.005148\n",
            "Epoch 310: Train Loss = 0.000187, Val Loss = 0.005500\n",
            "Epoch 320: Train Loss = 0.015713, Val Loss = 0.022551\n",
            "Epoch 330: Train Loss = 0.000090, Val Loss = 0.003852\n",
            "Epoch 340: Train Loss = 0.001230, Val Loss = 0.005037\n",
            "Epoch 350: Train Loss = 0.000761, Val Loss = 0.005817\n",
            "Epoch 360: Train Loss = 0.000701, Val Loss = 0.003669\n",
            "Epoch 370: Train Loss = 0.000027, Val Loss = 0.003862\n",
            "Epoch 380: Train Loss = 0.000456, Val Loss = 0.004775\n",
            "Epoch 390: Train Loss = 0.000476, Val Loss = 0.006927\n",
            "Epoch 400: Train Loss = 0.000157, Val Loss = 0.003699\n",
            "Epoch 410: Train Loss = 0.000661, Val Loss = 0.003782\n",
            "Epoch 420: Train Loss = 0.000096, Val Loss = 0.004411\n",
            "Epoch 430: Train Loss = 0.000573, Val Loss = 0.003695\n",
            "Epoch 440: Train Loss = 0.000064, Val Loss = 0.003183\n",
            "Epoch 450: Train Loss = 0.000046, Val Loss = 0.003042\n",
            "Epoch 460: Train Loss = 0.000614, Val Loss = 0.003184\n",
            "Epoch 470: Train Loss = 0.002308, Val Loss = 0.004436\n",
            "Epoch 480: Train Loss = 0.000219, Val Loss = 0.003789\n",
            "Epoch 490: Train Loss = 0.000026, Val Loss = 0.003734\n",
            "Epoch 500: Train Loss = 0.000273, Val Loss = 0.003074\n",
            "Split 20_80 — Final Test MSE: 0.002454\n",
            "\n",
            "==== Running Split: 10_90 ====\n",
            "Split 10_90 — X (INPUT): 10, Y (OUTPUT): 91\n",
            "Train: (1729, 10), Val: (433, 10), Test: (541, 10)\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.027356, Val Loss = 0.038495\n",
            "Epoch 20: Train Loss = 0.023789, Val Loss = 0.030361\n",
            "Epoch 30: Train Loss = 0.073538, Val Loss = 0.036344\n",
            "Epoch 40: Train Loss = 0.001584, Val Loss = 0.025633\n",
            "Epoch 50: Train Loss = 0.002019, Val Loss = 0.023519\n",
            "Epoch 60: Train Loss = 0.001103, Val Loss = 0.023851\n",
            "Epoch 70: Train Loss = 0.008304, Val Loss = 0.020883\n",
            "Epoch 80: Train Loss = 0.001266, Val Loss = 0.019884\n",
            "Epoch 90: Train Loss = 0.001679, Val Loss = 0.017254\n",
            "Epoch 100: Train Loss = 0.003651, Val Loss = 0.018593\n",
            "Epoch 110: Train Loss = 0.000081, Val Loss = 0.016816\n",
            "Epoch 120: Train Loss = 0.000923, Val Loss = 0.013435\n",
            "Epoch 130: Train Loss = 0.006318, Val Loss = 0.012923\n",
            "Epoch 140: Train Loss = 0.004454, Val Loss = 0.011070\n",
            "Epoch 150: Train Loss = 0.004946, Val Loss = 0.013812\n",
            "Epoch 160: Train Loss = 0.001358, Val Loss = 0.012857\n",
            "Epoch 170: Train Loss = 0.006253, Val Loss = 0.008536\n",
            "Epoch 180: Train Loss = 0.000237, Val Loss = 0.011273\n",
            "Epoch 190: Train Loss = 0.000321, Val Loss = 0.008510\n",
            "Epoch 200: Train Loss = 0.000148, Val Loss = 0.007676\n",
            "Epoch 210: Train Loss = 0.000465, Val Loss = 0.008864\n",
            "Epoch 220: Train Loss = 0.000164, Val Loss = 0.007946\n",
            "Epoch 230: Train Loss = 0.000283, Val Loss = 0.007667\n",
            "Epoch 240: Train Loss = 0.000734, Val Loss = 0.009091\n",
            "Epoch 250: Train Loss = 0.000291, Val Loss = 0.007173\n",
            "Epoch 260: Train Loss = 0.000863, Val Loss = 0.008906\n",
            "Epoch 270: Train Loss = 0.000180, Val Loss = 0.007587\n",
            "Epoch 280: Train Loss = 0.000178, Val Loss = 0.008829\n",
            "Epoch 290: Train Loss = 0.000365, Val Loss = 0.008744\n",
            "Epoch 300: Train Loss = 0.056533, Val Loss = 0.021126\n",
            "Epoch 310: Train Loss = 0.000049, Val Loss = 0.007226\n",
            "Epoch 320: Train Loss = 0.000729, Val Loss = 0.006675\n",
            "Epoch 330: Train Loss = 0.004505, Val Loss = 0.009166\n",
            "Epoch 340: Train Loss = 0.002812, Val Loss = 0.007161\n",
            "Epoch 350: Train Loss = 0.001534, Val Loss = 0.006364\n",
            "Epoch 360: Train Loss = 0.003351, Val Loss = 0.006471\n",
            "Epoch 370: Train Loss = 0.000737, Val Loss = 0.006173\n",
            "Epoch 380: Train Loss = 0.000430, Val Loss = 0.006667\n",
            "Epoch 390: Train Loss = 0.002003, Val Loss = 0.010718\n",
            "Epoch 400: Train Loss = 0.000869, Val Loss = 0.006216\n",
            "Epoch 410: Train Loss = 0.054309, Val Loss = 0.029139\n",
            "Epoch 420: Train Loss = 0.000765, Val Loss = 0.007375\n",
            "Epoch 430: Train Loss = 0.000273, Val Loss = 0.007798\n",
            "Epoch 440: Train Loss = 0.005121, Val Loss = 0.008075\n",
            "Epoch 450: Train Loss = 0.002532, Val Loss = 0.006355\n",
            "Epoch 460: Train Loss = 0.002568, Val Loss = 0.009021\n",
            "Epoch 470: Train Loss = 0.000133, Val Loss = 0.007343\n",
            "Epoch 480: Train Loss = 0.000173, Val Loss = 0.006957\n",
            "Epoch 490: Train Loss = 0.000517, Val Loss = 0.006724\n",
            "Epoch 500: Train Loss = 0.000521, Val Loss = 0.007196\n",
            "Split 10_90 — Final Test MSE: 0.004517\n",
            "\n",
            "==== Running Split: 5_95 ====\n",
            "Split 5_95 — X (INPUT): 5, Y (OUTPUT): 96\n",
            "Train: (1729, 5), Val: (433, 5), Test: (541, 5)\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.019729, Val Loss = 0.040907\n",
            "Epoch 20: Train Loss = 0.018612, Val Loss = 0.041594\n",
            "Epoch 30: Train Loss = 0.001641, Val Loss = 0.040338\n",
            "Epoch 40: Train Loss = 0.006671, Val Loss = 0.043918\n",
            "Epoch 50: Train Loss = 0.047563, Val Loss = 0.047357\n",
            "Epoch 60: Train Loss = 0.004408, Val Loss = 0.029245\n",
            "Epoch 70: Train Loss = 0.007812, Val Loss = 0.030925\n",
            "Epoch 80: Train Loss = 0.001020, Val Loss = 0.026336\n",
            "Epoch 90: Train Loss = 0.000844, Val Loss = 0.022865\n",
            "Epoch 100: Train Loss = 0.001370, Val Loss = 0.023418\n",
            "Epoch 110: Train Loss = 0.019325, Val Loss = 0.023514\n",
            "Epoch 120: Train Loss = 0.008010, Val Loss = 0.020311\n",
            "Epoch 130: Train Loss = 0.001435, Val Loss = 0.017913\n",
            "Epoch 140: Train Loss = 0.000641, Val Loss = 0.017540\n",
            "Epoch 150: Train Loss = 0.001829, Val Loss = 0.020543\n",
            "Epoch 160: Train Loss = 0.003322, Val Loss = 0.021335\n",
            "Epoch 170: Train Loss = 0.000839, Val Loss = 0.020698\n",
            "Epoch 180: Train Loss = 0.141773, Val Loss = 0.074642\n",
            "Epoch 190: Train Loss = 0.001057, Val Loss = 0.018725\n",
            "Epoch 200: Train Loss = 0.001963, Val Loss = 0.016679\n",
            "Epoch 210: Train Loss = 0.006218, Val Loss = 0.019395\n",
            "Epoch 220: Train Loss = 0.000511, Val Loss = 0.016577\n",
            "Epoch 230: Train Loss = 0.040874, Val Loss = 0.028743\n",
            "Epoch 240: Train Loss = 0.010322, Val Loss = 0.015573\n",
            "Epoch 250: Train Loss = 0.000496, Val Loss = 0.018028\n",
            "Epoch 260: Train Loss = 0.000557, Val Loss = 0.017308\n",
            "Epoch 270: Train Loss = 0.002390, Val Loss = 0.014932\n",
            "Epoch 280: Train Loss = 0.004972, Val Loss = 0.015221\n",
            "Epoch 290: Train Loss = 0.001482, Val Loss = 0.013378\n",
            "Epoch 300: Train Loss = 0.020208, Val Loss = 0.029668\n",
            "Epoch 310: Train Loss = 0.000997, Val Loss = 0.014476\n",
            "Epoch 320: Train Loss = 0.170588, Val Loss = 0.078319\n",
            "Epoch 330: Train Loss = 0.000229, Val Loss = 0.013718\n",
            "Epoch 340: Train Loss = 0.000099, Val Loss = 0.013216\n",
            "Epoch 350: Train Loss = 0.000092, Val Loss = 0.012113\n",
            "Epoch 360: Train Loss = 0.004322, Val Loss = 0.013369\n",
            "Epoch 370: Train Loss = 0.004987, Val Loss = 0.016104\n",
            "Epoch 380: Train Loss = 0.000684, Val Loss = 0.013798\n",
            "Epoch 390: Train Loss = 0.146065, Val Loss = 0.063585\n",
            "Epoch 400: Train Loss = 0.000981, Val Loss = 0.012221\n",
            "Epoch 410: Train Loss = 0.000050, Val Loss = 0.012657\n",
            "Epoch 420: Train Loss = 0.000397, Val Loss = 0.012419\n",
            "Epoch 430: Train Loss = 0.000655, Val Loss = 0.011340\n",
            "Epoch 440: Train Loss = 0.000513, Val Loss = 0.014548\n",
            "Epoch 450: Train Loss = 0.000117, Val Loss = 0.011806\n",
            "Epoch 460: Train Loss = 0.003330, Val Loss = 0.011833\n",
            "Epoch 470: Train Loss = 0.001595, Val Loss = 0.012577\n",
            "Epoch 480: Train Loss = 0.000151, Val Loss = 0.012513\n",
            "Epoch 490: Train Loss = 0.005899, Val Loss = 0.012361\n",
            "Epoch 500: Train Loss = 0.029881, Val Loss = 0.027723\n",
            "Split 5_95 — Final Test MSE: 0.008170\n",
            "\n",
            "==== Running Split: 3_97 ====\n",
            "Split 3_97 — X (INPUT): 3, Y (OUTPUT): 98\n",
            "Train: (1729, 3), Val: (433, 3), Test: (541, 3)\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.014540, Val Loss = 0.046144\n",
            "Epoch 20: Train Loss = 0.001946, Val Loss = 0.040828\n",
            "Epoch 30: Train Loss = 0.005957, Val Loss = 0.039979\n",
            "Epoch 40: Train Loss = 0.001424, Val Loss = 0.034911\n",
            "Epoch 50: Train Loss = 0.001610, Val Loss = 0.047484\n",
            "Epoch 60: Train Loss = 0.002941, Val Loss = 0.033505\n",
            "Epoch 70: Train Loss = 0.000633, Val Loss = 0.031602\n",
            "Epoch 80: Train Loss = 0.039634, Val Loss = 0.035633\n",
            "Epoch 90: Train Loss = 0.000965, Val Loss = 0.029685\n",
            "Epoch 100: Train Loss = 0.040065, Val Loss = 0.030131\n",
            "Epoch 110: Train Loss = 0.001026, Val Loss = 0.028870\n",
            "Epoch 120: Train Loss = 0.001971, Val Loss = 0.025942\n",
            "Epoch 130: Train Loss = 0.000883, Val Loss = 0.023882\n",
            "Epoch 140: Train Loss = 0.000506, Val Loss = 0.023274\n",
            "Epoch 150: Train Loss = 0.003416, Val Loss = 0.022546\n",
            "Epoch 160: Train Loss = 0.001005, Val Loss = 0.022884\n",
            "Epoch 170: Train Loss = 0.001104, Val Loss = 0.021006\n",
            "Epoch 180: Train Loss = 0.000452, Val Loss = 0.027293\n",
            "Epoch 190: Train Loss = 0.003811, Val Loss = 0.021707\n",
            "Epoch 200: Train Loss = 0.000572, Val Loss = 0.021134\n",
            "Epoch 210: Train Loss = 0.016774, Val Loss = 0.023777\n",
            "Epoch 220: Train Loss = 0.004059, Val Loss = 0.020026\n",
            "Epoch 230: Train Loss = 0.000419, Val Loss = 0.018972\n",
            "Epoch 240: Train Loss = 0.065364, Val Loss = 0.030864\n",
            "Epoch 250: Train Loss = 0.006658, Val Loss = 0.024829\n",
            "Epoch 260: Train Loss = 0.003006, Val Loss = 0.023075\n",
            "Epoch 270: Train Loss = 0.138761, Val Loss = 0.029514\n",
            "Epoch 280: Train Loss = 0.000382, Val Loss = 0.018434\n",
            "Epoch 290: Train Loss = 0.001588, Val Loss = 0.018273\n",
            "Epoch 300: Train Loss = 0.000297, Val Loss = 0.017966\n",
            "Epoch 310: Train Loss = 0.003444, Val Loss = 0.017112\n",
            "Epoch 320: Train Loss = 0.003057, Val Loss = 0.017117\n",
            "Epoch 330: Train Loss = 0.000309, Val Loss = 0.017440\n",
            "Epoch 340: Train Loss = 0.001029, Val Loss = 0.016515\n",
            "Epoch 350: Train Loss = 0.000652, Val Loss = 0.016766\n",
            "Epoch 360: Train Loss = 0.003177, Val Loss = 0.016882\n",
            "Epoch 370: Train Loss = 0.000270, Val Loss = 0.017569\n",
            "Epoch 380: Train Loss = 0.006321, Val Loss = 0.017233\n",
            "Epoch 390: Train Loss = 0.000375, Val Loss = 0.016836\n",
            "Epoch 400: Train Loss = 0.000099, Val Loss = 0.015318\n",
            "Epoch 410: Train Loss = 0.000357, Val Loss = 0.017674\n",
            "Epoch 420: Train Loss = 0.001008, Val Loss = 0.016420\n",
            "Epoch 430: Train Loss = 0.003855, Val Loss = 0.016168\n",
            "Epoch 440: Train Loss = 0.001366, Val Loss = 0.015758\n",
            "Epoch 450: Train Loss = 0.000346, Val Loss = 0.016865\n",
            "Epoch 460: Train Loss = 0.005236, Val Loss = 0.017896\n",
            "Epoch 470: Train Loss = 0.002660, Val Loss = 0.016618\n",
            "Epoch 480: Train Loss = 0.001254, Val Loss = 0.018237\n",
            "Epoch 490: Train Loss = 0.000215, Val Loss = 0.016430\n",
            "Epoch 500: Train Loss = 0.000940, Val Loss = 0.017536\n",
            "Split 3_97 — Final Test MSE: 0.013824\n",
            "\n",
            "==== Running Split: 1_99 ====\n",
            "Split 1_99 — X (INPUT): 1, Y (OUTPUT): 100\n",
            "Train: (1729, 1), Val: (433, 1), Test: (541, 1)\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.014183, Val Loss = 0.181612\n",
            "Epoch 20: Train Loss = 0.003316, Val Loss = 0.178056\n",
            "Epoch 30: Train Loss = 0.100422, Val Loss = 0.179356\n",
            "Epoch 40: Train Loss = 0.013539, Val Loss = 0.177172\n",
            "Epoch 50: Train Loss = 0.076330, Val Loss = 0.177331\n",
            "Epoch 60: Train Loss = 0.221658, Val Loss = 0.199550\n",
            "Epoch 70: Train Loss = 0.333816, Val Loss = 0.182784\n",
            "Epoch 80: Train Loss = 0.001963, Val Loss = 0.177368\n",
            "Epoch 90: Train Loss = 0.688506, Val Loss = 0.177638\n",
            "Epoch 100: Train Loss = 0.002317, Val Loss = 0.177986\n",
            "Epoch 110: Train Loss = 0.641180, Val Loss = 0.177189\n",
            "Epoch 120: Train Loss = 0.006267, Val Loss = 0.177114\n",
            "Epoch 130: Train Loss = 0.260212, Val Loss = 0.178666\n",
            "Epoch 140: Train Loss = 0.388792, Val Loss = 0.178370\n",
            "Epoch 150: Train Loss = 0.012338, Val Loss = 0.177074\n",
            "Epoch 160: Train Loss = 0.184709, Val Loss = 0.184004\n",
            "Epoch 170: Train Loss = 0.085475, Val Loss = 0.181761\n",
            "Epoch 180: Train Loss = 0.272180, Val Loss = 0.186041\n",
            "Epoch 190: Train Loss = 0.004510, Val Loss = 0.187517\n",
            "Epoch 200: Train Loss = 0.028046, Val Loss = 0.181389\n",
            "Epoch 210: Train Loss = 0.104658, Val Loss = 0.179334\n",
            "Epoch 220: Train Loss = 0.012035, Val Loss = 0.179235\n",
            "Epoch 230: Train Loss = 0.112231, Val Loss = 0.177600\n",
            "Epoch 240: Train Loss = 0.371793, Val Loss = 0.177202\n",
            "Epoch 250: Train Loss = 0.013416, Val Loss = 0.180521\n",
            "Epoch 260: Train Loss = 0.542196, Val Loss = 0.177082\n",
            "Epoch 270: Train Loss = 0.237098, Val Loss = 0.178449\n",
            "Epoch 280: Train Loss = 0.029319, Val Loss = 0.180434\n",
            "Epoch 290: Train Loss = 0.137527, Val Loss = 0.183709\n",
            "Epoch 300: Train Loss = 0.107263, Val Loss = 0.191217\n",
            "Epoch 310: Train Loss = 0.314691, Val Loss = 0.183180\n",
            "Epoch 320: Train Loss = 0.172559, Val Loss = 0.180024\n",
            "Epoch 330: Train Loss = 0.064730, Val Loss = 0.181665\n",
            "Epoch 340: Train Loss = 0.001175, Val Loss = 0.189238\n",
            "Epoch 350: Train Loss = 0.448189, Val Loss = 0.181527\n",
            "Epoch 360: Train Loss = 0.171217, Val Loss = 0.178691\n",
            "Epoch 370: Train Loss = 0.050066, Val Loss = 0.184410\n",
            "Epoch 380: Train Loss = 0.397886, Val Loss = 0.180345\n",
            "Epoch 390: Train Loss = 0.220139, Val Loss = 0.177084\n",
            "Epoch 400: Train Loss = 0.028444, Val Loss = 0.180417\n",
            "Epoch 410: Train Loss = 0.217196, Val Loss = 0.178425\n",
            "Epoch 420: Train Loss = 0.021331, Val Loss = 0.185792\n",
            "Epoch 430: Train Loss = 0.013221, Val Loss = 0.180503\n",
            "Epoch 440: Train Loss = 0.087531, Val Loss = 0.177835\n",
            "Epoch 450: Train Loss = 0.139354, Val Loss = 0.179578\n",
            "Epoch 460: Train Loss = 0.323008, Val Loss = 0.177525\n",
            "Epoch 470: Train Loss = 0.141720, Val Loss = 0.185639\n",
            "Epoch 480: Train Loss = 0.000863, Val Loss = 0.179913\n",
            "Epoch 490: Train Loss = 0.518532, Val Loss = 0.177587\n",
            "Epoch 500: Train Loss = 0.048838, Val Loss = 0.177141\n",
            "Split 1_99 — Final Test MSE: 0.151260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**New TFT experiment**"
      ],
      "metadata": {
        "id": "wNtzgTu6NTy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# === TFT Hyperparameter Configuration ===\n",
        "tft_config = {\n",
        "    'hidden_size': 64,\n",
        "    'num_heads': 4,\n",
        "    'dropout': 0.1,\n",
        "    'epochs': 500,\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 0.001,\n",
        "    'model_name': 'TFT'\n",
        "}\n",
        "\n",
        "# Ensure drive path exists\n",
        "drive_path = '/content/drive/MyDrive/DSSM-Figures'\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "# Initialize results container\n",
        "TFT_mse = pd.DataFrame(columns=['Split', 'Test_MSE'])\n",
        "\n",
        "# All input-output splits\n",
        "splits = [(80, 20), (60, 40), (50, 50), (40, 60), (20, 80), (10, 90), (5, 95), (3, 97), (1, 99)]\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "for train_pct, test_pct in splits:\n",
        "    split_name = f\"{train_pct}_{test_pct}\"\n",
        "    print(f\"\\n==== Running Split: {split_name} ====\")\n",
        "\n",
        "    # File ID splits\n",
        "    file_ids = df_output['file_id'].unique()\n",
        "    trainval_ids, test_ids = train_test_split(file_ids, test_size=0.2, random_state=42)\n",
        "    train_ids, val_ids = train_test_split(trainval_ids, test_size=0.2, random_state=42)\n",
        "\n",
        "    def extract_X_Y(ids, pct):\n",
        "        df_subset = df_output[df_output['file_id'].isin(ids)]\n",
        "        pivoted = df_subset.pivot(index='file_id', columns='timestep', values='CO2').values\n",
        "        split_idx = int(pct / 100 * 101)\n",
        "        X = pivoted[:, :split_idx]\n",
        "        Y = pivoted[:, split_idx:]\n",
        "        return X, Y\n",
        "\n",
        "    X_train, Y_train = extract_X_Y(train_ids, train_pct)\n",
        "    X_val, Y_val = extract_X_Y(val_ids, train_pct)\n",
        "    X_test, Y_test = extract_X_Y(test_ids, train_pct)\n",
        "\n",
        "    static_train = merged_df[merged_df['file_id'].isin(train_ids)].drop(columns=['file_id', 'cluster']).values\n",
        "    static_val = merged_df[merged_df['file_id'].isin(val_ids)].drop(columns=['file_id', 'cluster']).values\n",
        "    static_test = merged_df[merged_df['file_id'].isin(test_ids)].drop(columns=['file_id', 'cluster']).values\n",
        "\n",
        "    print(f\"Split {split_name} — X (INPUT): {X_train.shape[1]}, Y (OUTPUT): {Y_train.shape[1]}\")\n",
        "    print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train[:, :, None], dtype=torch.float32)\n",
        "    Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
        "    static_train_tensor = torch.tensor(static_train, dtype=torch.float32)\n",
        "\n",
        "    X_val_tensor = torch.tensor(X_val[:, :, None], dtype=torch.float32)\n",
        "    Y_val_tensor = torch.tensor(Y_val, dtype=torch.float32)\n",
        "    static_val_tensor = torch.tensor(static_val, dtype=torch.float32)\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test[:, :, None], dtype=torch.float32)\n",
        "    Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
        "    static_test_tensor = torch.tensor(static_test, dtype=torch.float32)\n",
        "\n",
        "    train_loader = DataLoader(TensorDataset(X_train_tensor, static_train_tensor, Y_train_tensor), batch_size=tft_config['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(TensorDataset(X_val_tensor, static_val_tensor, Y_val_tensor), batch_size=tft_config['batch_size'])\n",
        "\n",
        "    model = TemporalFusionTransformer(\n",
        "        input_size=1,\n",
        "        output_size=Y_train.shape[1],\n",
        "        static_size=static_train.shape[1],\n",
        "        hidden_size=tft_config['hidden_size'],\n",
        "        num_heads=tft_config['num_heads'],\n",
        "        dropout=tft_config['dropout']\n",
        "    )\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=tft_config['learning_rate'])\n",
        "    criterion = nn.MSELoss()\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "\n",
        "    print(\"Training started...\")\n",
        "    for epoch in range(tft_config['epochs']):\n",
        "        model.train()\n",
        "        for X_batch, static_batch, Y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X_batch, static_batch)\n",
        "            loss = criterion(preds, Y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, static_batch, Y_batch in val_loader:\n",
        "                preds = model(X_batch, static_batch)\n",
        "                val_loss += criterion(preds, Y_batch).item()\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}: Train Loss = {loss.item():.6f}, Val Loss = {val_loss:.6f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict()\n",
        "            #print(f\"New best model found at epoch {epoch+1} with val loss {val_loss:.6f}\")\n",
        "            torch.save(best_model_state, os.path.join(drive_path, f\"TFT_best_model_{split_name}.pt\"))\n",
        "\n",
        "    model.load_state_dict(torch.load(os.path.join(drive_path, f\"TFT_best_model_{split_name}.pt\")))\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        preds = model(X_test_tensor, static_test_tensor)\n",
        "        test_loss = criterion(preds, Y_test_tensor).item()\n",
        "\n",
        "    TFT_mse.loc[len(TFT_mse)] = [split_name, test_loss]\n",
        "    print(f\"Split {split_name} — Final Test MSE: {test_loss:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZZ-y4iPaFqmO",
        "outputId": "4cffc43c-7607-471a-8cfa-318a25f40d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Running Split: 80_20 ====\n",
            "Split 80_20 — X (INPUT): 80, Y (OUTPUT): 21\n",
            "Train: (1729, 80), Val: (433, 80), Test: (541, 80)\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.039389, Val Loss = 0.092938\n",
            "Epoch 20: Train Loss = 0.041222, Val Loss = 0.051265\n",
            "Epoch 30: Train Loss = 0.000654, Val Loss = 0.013818\n",
            "Epoch 40: Train Loss = 0.000836, Val Loss = 0.003894\n",
            "Epoch 50: Train Loss = 0.000830, Val Loss = 0.007091\n",
            "Epoch 60: Train Loss = 0.007971, Val Loss = 0.024487\n",
            "Epoch 70: Train Loss = 0.001138, Val Loss = 0.003994\n",
            "Epoch 80: Train Loss = 0.016205, Val Loss = 0.056298\n",
            "Epoch 90: Train Loss = 0.001833, Val Loss = 0.004595\n",
            "Epoch 100: Train Loss = 0.000128, Val Loss = 0.001589\n",
            "Epoch 110: Train Loss = 0.000026, Val Loss = 0.003082\n",
            "Epoch 120: Train Loss = 0.000091, Val Loss = 0.001999\n",
            "Epoch 130: Train Loss = 0.005087, Val Loss = 0.004575\n",
            "Epoch 140: Train Loss = 0.002832, Val Loss = 0.002473\n",
            "Epoch 150: Train Loss = 0.000482, Val Loss = 0.002220\n",
            "Epoch 160: Train Loss = 0.005423, Val Loss = 0.016566\n",
            "Epoch 170: Train Loss = 0.000220, Val Loss = 0.004535\n",
            "Epoch 180: Train Loss = 0.000021, Val Loss = 0.002123\n",
            "Epoch 190: Train Loss = 0.000280, Val Loss = 0.001406\n",
            "Epoch 200: Train Loss = 0.000231, Val Loss = 0.001477\n",
            "Epoch 210: Train Loss = 0.001787, Val Loss = 0.008669\n",
            "Epoch 220: Train Loss = 0.000051, Val Loss = 0.001251\n",
            "Epoch 230: Train Loss = 0.000114, Val Loss = 0.001517\n",
            "Epoch 240: Train Loss = 0.000882, Val Loss = 0.001042\n",
            "Epoch 250: Train Loss = 0.000112, Val Loss = 0.001008\n",
            "Epoch 260: Train Loss = 0.002942, Val Loss = 0.002113\n",
            "Epoch 270: Train Loss = 0.000038, Val Loss = 0.001131\n",
            "Epoch 280: Train Loss = 0.000486, Val Loss = 0.000826\n",
            "Epoch 290: Train Loss = 0.000336, Val Loss = 0.000996\n",
            "Epoch 300: Train Loss = 0.000019, Val Loss = 0.000853\n",
            "Epoch 310: Train Loss = 0.001347, Val Loss = 0.001065\n",
            "Epoch 320: Train Loss = 0.001008, Val Loss = 0.004604\n",
            "Epoch 330: Train Loss = 0.000100, Val Loss = 0.001089\n",
            "Epoch 340: Train Loss = 0.000373, Val Loss = 0.003709\n",
            "Epoch 350: Train Loss = 0.000049, Val Loss = 0.000856\n",
            "Epoch 360: Train Loss = 0.000454, Val Loss = 0.000944\n",
            "Epoch 370: Train Loss = 0.000025, Val Loss = 0.000334\n",
            "Epoch 380: Train Loss = 0.000056, Val Loss = 0.000354\n",
            "Epoch 390: Train Loss = 0.000055, Val Loss = 0.000294\n",
            "Epoch 400: Train Loss = 0.000110, Val Loss = 0.000796\n",
            "Epoch 410: Train Loss = 0.000087, Val Loss = 0.000828\n",
            "Epoch 420: Train Loss = 0.000089, Val Loss = 0.000487\n",
            "Epoch 430: Train Loss = 0.000040, Val Loss = 0.000766\n",
            "Epoch 440: Train Loss = 0.000217, Val Loss = 0.001183\n",
            "Epoch 450: Train Loss = 0.000096, Val Loss = 0.000426\n",
            "Epoch 460: Train Loss = 0.000017, Val Loss = 0.000405\n",
            "Epoch 470: Train Loss = 0.000107, Val Loss = 0.000382\n",
            "Epoch 480: Train Loss = 0.000296, Val Loss = 0.001148\n",
            "Epoch 490: Train Loss = 0.000091, Val Loss = 0.000487\n",
            "Epoch 500: Train Loss = 0.000112, Val Loss = 0.000382\n",
            "Split 80_20 — Final Test MSE: 0.000209\n",
            "\n",
            "==== Running Split: 60_40 ====\n",
            "Split 60_40 — X (INPUT): 60, Y (OUTPUT): 41\n",
            "Train: (1729, 60), Val: (433, 60), Test: (541, 60)\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.048894, Val Loss = 0.148893\n",
            "Epoch 20: Train Loss = 0.215064, Val Loss = 0.254213\n",
            "Epoch 30: Train Loss = 0.001050, Val Loss = 0.011305\n",
            "Epoch 40: Train Loss = 0.001799, Val Loss = 0.008985\n",
            "Epoch 50: Train Loss = 0.010837, Val Loss = 0.026992\n",
            "Epoch 60: Train Loss = 0.000725, Val Loss = 0.008076\n",
            "Epoch 70: Train Loss = 0.002407, Val Loss = 0.007948\n",
            "Epoch 80: Train Loss = 0.000289, Val Loss = 0.005439\n",
            "Epoch 90: Train Loss = 0.002373, Val Loss = 0.005981\n",
            "Epoch 100: Train Loss = 0.004703, Val Loss = 0.005013\n",
            "Epoch 110: Train Loss = 0.004443, Val Loss = 0.005137\n",
            "Epoch 120: Train Loss = 0.000157, Val Loss = 0.002431\n",
            "Epoch 130: Train Loss = 0.000105, Val Loss = 0.002513\n",
            "Epoch 140: Train Loss = 0.002081, Val Loss = 0.004898\n",
            "Epoch 150: Train Loss = 0.000507, Val Loss = 0.001444\n",
            "Epoch 160: Train Loss = 0.000738, Val Loss = 0.001310\n",
            "Epoch 170: Train Loss = 0.000173, Val Loss = 0.001653\n",
            "Epoch 180: Train Loss = 0.000078, Val Loss = 0.001147\n",
            "Epoch 190: Train Loss = 0.000294, Val Loss = 0.002132\n",
            "Epoch 200: Train Loss = 0.001730, Val Loss = 0.004601\n",
            "Epoch 210: Train Loss = 0.000307, Val Loss = 0.001246\n",
            "Epoch 220: Train Loss = 0.000726, Val Loss = 0.001425\n",
            "Epoch 230: Train Loss = 0.000257, Val Loss = 0.000912\n",
            "Epoch 240: Train Loss = 0.000594, Val Loss = 0.006066\n",
            "Epoch 250: Train Loss = 0.000618, Val Loss = 0.001430\n",
            "Epoch 260: Train Loss = 0.000222, Val Loss = 0.001064\n",
            "Epoch 270: Train Loss = 0.000322, Val Loss = 0.001041\n",
            "Epoch 280: Train Loss = 0.001020, Val Loss = 0.002892\n",
            "Epoch 290: Train Loss = 0.000294, Val Loss = 0.001878\n",
            "Epoch 300: Train Loss = 0.000299, Val Loss = 0.002029\n",
            "Epoch 310: Train Loss = 0.001779, Val Loss = 0.003161\n",
            "Epoch 320: Train Loss = 0.000077, Val Loss = 0.000609\n",
            "Epoch 330: Train Loss = 0.000354, Val Loss = 0.001578\n",
            "Epoch 340: Train Loss = 0.000972, Val Loss = 0.000908\n",
            "Epoch 350: Train Loss = 0.000047, Val Loss = 0.000712\n",
            "Epoch 360: Train Loss = 0.000226, Val Loss = 0.001472\n",
            "Epoch 370: Train Loss = 0.000013, Val Loss = 0.000545\n",
            "Epoch 380: Train Loss = 0.002081, Val Loss = 0.000805\n",
            "Epoch 390: Train Loss = 0.000283, Val Loss = 0.000698\n",
            "Epoch 400: Train Loss = 0.000082, Val Loss = 0.000596\n",
            "Epoch 410: Train Loss = 0.000230, Val Loss = 0.000517\n",
            "Epoch 420: Train Loss = 0.000041, Val Loss = 0.000526\n",
            "Epoch 430: Train Loss = 0.000440, Val Loss = 0.001353\n",
            "Epoch 440: Train Loss = 0.000059, Val Loss = 0.001215\n",
            "Epoch 450: Train Loss = 0.000660, Val Loss = 0.002726\n",
            "Epoch 460: Train Loss = 0.000220, Val Loss = 0.000728\n",
            "Epoch 470: Train Loss = 0.000059, Val Loss = 0.001016\n",
            "Epoch 480: Train Loss = 0.000159, Val Loss = 0.000451\n",
            "Epoch 490: Train Loss = 0.000533, Val Loss = 0.001941\n",
            "Epoch 500: Train Loss = 0.000020, Val Loss = 0.000412\n",
            "Split 60_40 — Final Test MSE: 0.000446\n",
            "\n",
            "==== Running Split: 50_50 ====\n",
            "Split 50_50 — X (INPUT): 50, Y (OUTPUT): 51\n",
            "Train: (1729, 50), Val: (433, 50), Test: (541, 50)\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.017026, Val Loss = 0.032170\n",
            "Epoch 20: Train Loss = 0.027162, Val Loss = 0.055706\n",
            "Epoch 30: Train Loss = 0.002313, Val Loss = 0.012403\n",
            "Epoch 40: Train Loss = 0.000341, Val Loss = 0.009114\n",
            "Epoch 50: Train Loss = 0.000478, Val Loss = 0.009651\n",
            "Epoch 60: Train Loss = 0.004494, Val Loss = 0.008028\n",
            "Epoch 70: Train Loss = 0.005256, Val Loss = 0.008104\n",
            "Epoch 80: Train Loss = 0.002362, Val Loss = 0.021925\n",
            "Epoch 90: Train Loss = 0.000159, Val Loss = 0.007731\n",
            "Epoch 100: Train Loss = 0.007028, Val Loss = 0.025425\n",
            "Epoch 110: Train Loss = 0.001795, Val Loss = 0.008098\n",
            "Epoch 120: Train Loss = 0.002210, Val Loss = 0.006659\n",
            "Epoch 130: Train Loss = 0.001342, Val Loss = 0.006713\n",
            "Epoch 140: Train Loss = 0.020256, Val Loss = 0.029474\n",
            "Epoch 150: Train Loss = 0.000192, Val Loss = 0.004770\n",
            "Epoch 160: Train Loss = 0.000142, Val Loss = 0.003655\n",
            "Epoch 170: Train Loss = 0.000821, Val Loss = 0.006005\n",
            "Epoch 180: Train Loss = 0.002062, Val Loss = 0.004822\n",
            "Epoch 190: Train Loss = 0.002256, Val Loss = 0.010022\n",
            "Epoch 200: Train Loss = 0.000372, Val Loss = 0.003420\n",
            "Epoch 210: Train Loss = 0.000822, Val Loss = 0.004224\n",
            "Epoch 220: Train Loss = 0.001432, Val Loss = 0.004183\n",
            "Epoch 230: Train Loss = 0.000427, Val Loss = 0.003325\n",
            "Epoch 240: Train Loss = 0.000084, Val Loss = 0.002219\n",
            "Epoch 250: Train Loss = 0.000424, Val Loss = 0.001951\n",
            "Epoch 260: Train Loss = 0.000105, Val Loss = 0.001923\n",
            "Epoch 270: Train Loss = 0.001983, Val Loss = 0.004409\n",
            "Epoch 280: Train Loss = 0.000348, Val Loss = 0.001795\n",
            "Epoch 290: Train Loss = 0.000771, Val Loss = 0.002813\n",
            "Epoch 300: Train Loss = 0.000036, Val Loss = 0.001580\n",
            "Epoch 310: Train Loss = 0.000338, Val Loss = 0.001590\n",
            "Epoch 320: Train Loss = 0.000479, Val Loss = 0.003614\n",
            "Epoch 330: Train Loss = 0.000233, Val Loss = 0.002066\n",
            "Epoch 340: Train Loss = 0.000141, Val Loss = 0.001418\n",
            "Epoch 350: Train Loss = 0.000012, Val Loss = 0.001340\n",
            "Epoch 360: Train Loss = 0.000130, Val Loss = 0.001352\n",
            "Epoch 370: Train Loss = 0.000011, Val Loss = 0.001048\n",
            "Epoch 380: Train Loss = 0.001455, Val Loss = 0.004622\n",
            "Epoch 390: Train Loss = 0.000022, Val Loss = 0.001129\n",
            "Epoch 400: Train Loss = 0.000828, Val Loss = 0.001544\n",
            "Epoch 410: Train Loss = 0.000107, Val Loss = 0.001098\n",
            "Epoch 420: Train Loss = 0.000283, Val Loss = 0.001835\n",
            "Epoch 430: Train Loss = 0.000005, Val Loss = 0.000916\n",
            "Epoch 440: Train Loss = 0.000525, Val Loss = 0.000924\n",
            "Epoch 450: Train Loss = 0.000412, Val Loss = 0.001062\n",
            "Epoch 460: Train Loss = 0.000016, Val Loss = 0.001161\n",
            "Epoch 470: Train Loss = 0.000174, Val Loss = 0.000796\n",
            "Epoch 480: Train Loss = 0.000165, Val Loss = 0.000776\n",
            "Epoch 490: Train Loss = 0.000014, Val Loss = 0.000850\n",
            "Epoch 500: Train Loss = 0.000023, Val Loss = 0.000907\n",
            "Split 50_50 — Final Test MSE: 0.000590\n",
            "\n",
            "==== Running Split: 40_60 ====\n",
            "Split 40_60 — X (INPUT): 40, Y (OUTPUT): 61\n",
            "Train: (1729, 40), Val: (433, 40), Test: (541, 40)\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.003685, Val Loss = 0.052216\n",
            "Epoch 20: Train Loss = 0.131197, Val Loss = 0.046448\n",
            "Epoch 30: Train Loss = 0.002186, Val Loss = 0.013480\n",
            "Epoch 40: Train Loss = 0.043657, Val Loss = 0.093087\n",
            "Epoch 50: Train Loss = 0.001088, Val Loss = 0.020774\n",
            "Epoch 60: Train Loss = 0.000260, Val Loss = 0.010658\n",
            "Epoch 70: Train Loss = 0.000425, Val Loss = 0.012779\n",
            "Epoch 80: Train Loss = 0.006634, Val Loss = 0.012475\n",
            "Epoch 90: Train Loss = 0.000532, Val Loss = 0.009342\n",
            "Epoch 100: Train Loss = 0.000595, Val Loss = 0.011669\n",
            "Epoch 110: Train Loss = 0.016134, Val Loss = 0.024043\n",
            "Epoch 120: Train Loss = 0.002750, Val Loss = 0.008813\n",
            "Epoch 130: Train Loss = 0.007098, Val Loss = 0.008594\n",
            "Epoch 140: Train Loss = 0.017803, Val Loss = 0.012725\n",
            "Epoch 150: Train Loss = 0.000232, Val Loss = 0.006298\n",
            "Epoch 160: Train Loss = 0.000288, Val Loss = 0.007923\n",
            "Epoch 170: Train Loss = 0.001609, Val Loss = 0.006817\n",
            "Epoch 180: Train Loss = 0.004072, Val Loss = 0.010914\n",
            "Epoch 190: Train Loss = 0.000503, Val Loss = 0.009421\n",
            "Epoch 200: Train Loss = 0.000305, Val Loss = 0.005607\n",
            "Epoch 210: Train Loss = 0.000232, Val Loss = 0.007719\n",
            "Epoch 220: Train Loss = 0.008973, Val Loss = 0.008822\n",
            "Epoch 230: Train Loss = 0.001332, Val Loss = 0.006277\n",
            "Epoch 240: Train Loss = 0.024304, Val Loss = 0.012929\n",
            "Epoch 250: Train Loss = 0.000372, Val Loss = 0.008473\n",
            "Epoch 260: Train Loss = 0.000158, Val Loss = 0.006240\n",
            "Epoch 270: Train Loss = 0.003204, Val Loss = 0.005953\n",
            "Epoch 280: Train Loss = 0.000736, Val Loss = 0.007572\n",
            "Epoch 290: Train Loss = 0.010030, Val Loss = 0.012809\n",
            "Epoch 300: Train Loss = 0.001303, Val Loss = 0.006151\n",
            "Epoch 310: Train Loss = 0.024808, Val Loss = 0.012198\n",
            "Epoch 320: Train Loss = 0.001857, Val Loss = 0.004703\n",
            "Epoch 330: Train Loss = 0.000603, Val Loss = 0.007402\n",
            "Epoch 340: Train Loss = 0.021342, Val Loss = 0.011978\n",
            "Epoch 350: Train Loss = 0.023264, Val Loss = 0.011658\n",
            "Epoch 360: Train Loss = 0.001099, Val Loss = 0.006049\n",
            "Epoch 370: Train Loss = 0.002485, Val Loss = 0.005476\n",
            "Epoch 380: Train Loss = 0.001326, Val Loss = 0.006801\n",
            "Epoch 390: Train Loss = 0.000682, Val Loss = 0.005096\n",
            "Epoch 400: Train Loss = 0.001198, Val Loss = 0.004459\n",
            "Epoch 410: Train Loss = 0.001724, Val Loss = 0.005346\n",
            "Epoch 420: Train Loss = 0.000349, Val Loss = 0.006023\n",
            "Epoch 430: Train Loss = 0.000546, Val Loss = 0.006278\n",
            "Epoch 440: Train Loss = 0.020172, Val Loss = 0.011834\n",
            "Epoch 450: Train Loss = 0.001462, Val Loss = 0.005338\n",
            "Epoch 460: Train Loss = 0.000978, Val Loss = 0.005047\n",
            "Epoch 470: Train Loss = 0.000276, Val Loss = 0.004641\n",
            "Epoch 480: Train Loss = 0.000055, Val Loss = 0.004473\n",
            "Epoch 490: Train Loss = 0.000641, Val Loss = 0.004476\n",
            "Epoch 500: Train Loss = 0.000094, Val Loss = 0.003642\n",
            "Split 40_60 — Final Test MSE: 0.002430\n",
            "\n",
            "==== Running Split: 20_80 ====\n",
            "Split 20_80 — X (INPUT): 20, Y (OUTPUT): 81\n",
            "Train: (1729, 20), Val: (433, 20), Test: (541, 20)\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.011774, Val Loss = 0.063376\n",
            "Epoch 20: Train Loss = 0.001535, Val Loss = 0.037950\n",
            "Epoch 30: Train Loss = 0.001186, Val Loss = 0.026080\n",
            "Epoch 40: Train Loss = 0.000970, Val Loss = 0.022153\n",
            "Epoch 50: Train Loss = 0.001133, Val Loss = 0.022298\n",
            "Epoch 60: Train Loss = 0.000678, Val Loss = 0.019692\n",
            "Epoch 70: Train Loss = 0.002156, Val Loss = 0.018613\n",
            "Epoch 80: Train Loss = 0.001648, Val Loss = 0.023043\n",
            "Epoch 90: Train Loss = 0.004357, Val Loss = 0.017873\n",
            "Epoch 100: Train Loss = 0.011954, Val Loss = 0.026358\n",
            "Epoch 110: Train Loss = 0.016877, Val Loss = 0.019500\n",
            "Epoch 120: Train Loss = 0.002619, Val Loss = 0.017823\n",
            "Epoch 130: Train Loss = 0.001138, Val Loss = 0.017515\n",
            "Epoch 140: Train Loss = 0.001822, Val Loss = 0.015008\n",
            "Epoch 150: Train Loss = 0.001814, Val Loss = 0.015646\n",
            "Epoch 160: Train Loss = 0.000125, Val Loss = 0.018685\n",
            "Epoch 170: Train Loss = 0.002022, Val Loss = 0.014954\n",
            "Epoch 180: Train Loss = 0.029475, Val Loss = 0.018183\n",
            "Epoch 190: Train Loss = 0.000198, Val Loss = 0.014825\n",
            "Epoch 200: Train Loss = 0.003465, Val Loss = 0.012624\n",
            "Epoch 210: Train Loss = 0.015566, Val Loss = 0.016365\n",
            "Epoch 220: Train Loss = 0.009417, Val Loss = 0.017017\n",
            "Epoch 230: Train Loss = 0.000173, Val Loss = 0.010934\n",
            "Epoch 240: Train Loss = 0.002209, Val Loss = 0.011466\n",
            "Epoch 250: Train Loss = 0.000217, Val Loss = 0.009558\n",
            "Epoch 260: Train Loss = 0.000129, Val Loss = 0.011169\n",
            "Epoch 270: Train Loss = 0.000248, Val Loss = 0.011730\n",
            "Epoch 280: Train Loss = 0.000147, Val Loss = 0.009672\n",
            "Epoch 290: Train Loss = 0.002052, Val Loss = 0.010635\n",
            "Epoch 300: Train Loss = 0.008664, Val Loss = 0.010973\n",
            "Epoch 310: Train Loss = 0.000685, Val Loss = 0.008511\n",
            "Epoch 320: Train Loss = 0.000387, Val Loss = 0.007989\n",
            "Epoch 330: Train Loss = 0.001444, Val Loss = 0.007742\n",
            "Epoch 340: Train Loss = 0.000108, Val Loss = 0.006982\n",
            "Epoch 350: Train Loss = 0.000094, Val Loss = 0.006774\n",
            "Epoch 360: Train Loss = 0.000203, Val Loss = 0.006787\n",
            "Epoch 370: Train Loss = 0.000073, Val Loss = 0.008127\n",
            "Epoch 380: Train Loss = 0.001768, Val Loss = 0.006861\n",
            "Epoch 390: Train Loss = 0.000082, Val Loss = 0.006821\n",
            "Epoch 400: Train Loss = 0.000047, Val Loss = 0.005555\n",
            "Epoch 410: Train Loss = 0.004906, Val Loss = 0.009897\n",
            "Epoch 420: Train Loss = 0.003681, Val Loss = 0.008402\n",
            "Epoch 430: Train Loss = 0.001579, Val Loss = 0.006015\n",
            "Epoch 440: Train Loss = 0.000140, Val Loss = 0.007042\n",
            "Epoch 450: Train Loss = 0.000808, Val Loss = 0.006872\n",
            "Epoch 460: Train Loss = 0.002635, Val Loss = 0.006393\n",
            "Epoch 470: Train Loss = 0.000214, Val Loss = 0.005194\n",
            "Epoch 480: Train Loss = 0.000145, Val Loss = 0.005276\n",
            "Epoch 490: Train Loss = 0.000047, Val Loss = 0.005281\n",
            "Epoch 500: Train Loss = 0.000042, Val Loss = 0.005337\n",
            "Split 20_80 — Final Test MSE: 0.003448\n",
            "\n",
            "==== Running Split: 10_90 ====\n",
            "Split 10_90 — X (INPUT): 10, Y (OUTPUT): 91\n",
            "Train: (1729, 10), Val: (433, 10), Test: (541, 10)\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.012341, Val Loss = 0.072017\n",
            "Epoch 20: Train Loss = 0.064816, Val Loss = 0.099203\n",
            "Epoch 30: Train Loss = 0.010708, Val Loss = 0.036623\n",
            "Epoch 40: Train Loss = 0.012826, Val Loss = 0.052105\n",
            "Epoch 50: Train Loss = 0.003344, Val Loss = 0.037705\n",
            "Epoch 60: Train Loss = 0.008976, Val Loss = 0.030712\n",
            "Epoch 70: Train Loss = 0.002247, Val Loss = 0.029059\n",
            "Epoch 80: Train Loss = 0.044502, Val Loss = 0.041600\n",
            "Epoch 90: Train Loss = 0.001782, Val Loss = 0.027455\n",
            "Epoch 100: Train Loss = 0.007645, Val Loss = 0.026592\n",
            "Epoch 110: Train Loss = 0.003117, Val Loss = 0.026956\n",
            "Epoch 120: Train Loss = 0.030174, Val Loss = 0.026204\n",
            "Epoch 130: Train Loss = 0.002925, Val Loss = 0.027007\n",
            "Epoch 140: Train Loss = 0.004394, Val Loss = 0.042941\n",
            "Epoch 150: Train Loss = 0.001226, Val Loss = 0.032249\n",
            "Epoch 160: Train Loss = 0.017625, Val Loss = 0.032853\n",
            "Epoch 170: Train Loss = 0.001145, Val Loss = 0.027820\n",
            "Epoch 180: Train Loss = 0.002050, Val Loss = 0.024014\n",
            "Epoch 190: Train Loss = 0.005862, Val Loss = 0.026045\n",
            "Epoch 200: Train Loss = 0.084786, Val Loss = 0.043495\n",
            "Epoch 210: Train Loss = 0.000354, Val Loss = 0.022479\n",
            "Epoch 220: Train Loss = 0.003413, Val Loss = 0.021294\n",
            "Epoch 230: Train Loss = 0.005009, Val Loss = 0.020853\n",
            "Epoch 240: Train Loss = 0.005404, Val Loss = 0.021978\n",
            "Epoch 250: Train Loss = 0.002808, Val Loss = 0.020802\n",
            "Epoch 260: Train Loss = 0.004069, Val Loss = 0.025855\n",
            "Epoch 270: Train Loss = 0.013168, Val Loss = 0.022098\n",
            "Epoch 280: Train Loss = 0.000417, Val Loss = 0.020469\n",
            "Epoch 290: Train Loss = 0.003747, Val Loss = 0.020861\n",
            "Epoch 300: Train Loss = 0.002512, Val Loss = 0.021835\n",
            "Epoch 310: Train Loss = 0.001180, Val Loss = 0.015243\n",
            "Epoch 320: Train Loss = 0.004250, Val Loss = 0.017300\n",
            "Epoch 330: Train Loss = 0.014490, Val Loss = 0.019297\n",
            "Epoch 340: Train Loss = 0.012556, Val Loss = 0.017748\n",
            "Epoch 350: Train Loss = 0.001137, Val Loss = 0.013809\n",
            "Epoch 360: Train Loss = 0.005967, Val Loss = 0.021063\n",
            "Epoch 370: Train Loss = 0.001381, Val Loss = 0.014553\n",
            "Epoch 380: Train Loss = 0.002308, Val Loss = 0.012144\n",
            "Epoch 390: Train Loss = 0.002571, Val Loss = 0.013547\n",
            "Epoch 400: Train Loss = 0.017368, Val Loss = 0.017826\n",
            "Epoch 410: Train Loss = 0.008670, Val Loss = 0.012928\n",
            "Epoch 420: Train Loss = 0.076284, Val Loss = 0.056234\n",
            "Epoch 430: Train Loss = 0.019532, Val Loss = 0.027236\n",
            "Epoch 440: Train Loss = 0.000531, Val Loss = 0.016776\n",
            "Epoch 450: Train Loss = 0.001917, Val Loss = 0.017880\n",
            "Epoch 460: Train Loss = 0.004708, Val Loss = 0.014955\n",
            "Epoch 470: Train Loss = 0.000564, Val Loss = 0.019834\n",
            "Epoch 480: Train Loss = 0.133943, Val Loss = 0.029585\n",
            "Epoch 490: Train Loss = 0.000194, Val Loss = 0.016356\n",
            "Epoch 500: Train Loss = 0.006296, Val Loss = 0.016666\n",
            "Split 10_90 — Final Test MSE: 0.010680\n",
            "\n",
            "==== Running Split: 5_95 ====\n",
            "Split 5_95 — X (INPUT): 5, Y (OUTPUT): 96\n",
            "Train: (1729, 5), Val: (433, 5), Test: (541, 5)\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.009123, Val Loss = 0.059192\n",
            "Epoch 20: Train Loss = 1.075334, Val Loss = 0.178913\n",
            "Epoch 30: Train Loss = 0.080765, Val Loss = 0.062656\n",
            "Epoch 40: Train Loss = 0.001381, Val Loss = 0.046251\n",
            "Epoch 50: Train Loss = 0.110005, Val Loss = 0.057438\n",
            "Epoch 60: Train Loss = 0.005097, Val Loss = 0.040912\n",
            "Epoch 70: Train Loss = 0.004008, Val Loss = 0.039245\n",
            "Epoch 80: Train Loss = 0.101585, Val Loss = 0.064620\n",
            "Epoch 90: Train Loss = 0.006395, Val Loss = 0.036572\n",
            "Epoch 100: Train Loss = 0.001567, Val Loss = 0.039506\n",
            "Epoch 110: Train Loss = 0.001377, Val Loss = 0.044196\n",
            "Epoch 120: Train Loss = 0.024076, Val Loss = 0.042401\n",
            "Epoch 130: Train Loss = 0.019075, Val Loss = 0.037856\n",
            "Epoch 140: Train Loss = 0.009736, Val Loss = 0.036537\n",
            "Epoch 150: Train Loss = 0.001407, Val Loss = 0.032528\n",
            "Epoch 160: Train Loss = 0.001402, Val Loss = 0.030314\n",
            "Epoch 170: Train Loss = 0.131554, Val Loss = 0.043297\n",
            "Epoch 180: Train Loss = 0.017394, Val Loss = 0.035164\n",
            "Epoch 190: Train Loss = 0.001554, Val Loss = 0.032724\n",
            "Epoch 200: Train Loss = 0.002858, Val Loss = 0.032853\n",
            "Epoch 210: Train Loss = 0.000991, Val Loss = 0.031381\n",
            "Epoch 220: Train Loss = 0.002084, Val Loss = 0.029680\n",
            "Epoch 230: Train Loss = 0.003812, Val Loss = 0.029834\n",
            "Epoch 240: Train Loss = 0.006713, Val Loss = 0.025342\n",
            "Epoch 250: Train Loss = 0.003837, Val Loss = 0.027871\n",
            "Epoch 260: Train Loss = 0.006116, Val Loss = 0.027915\n",
            "Epoch 270: Train Loss = 0.000657, Val Loss = 0.023777\n",
            "Epoch 280: Train Loss = 0.004915, Val Loss = 0.023530\n",
            "Epoch 290: Train Loss = 0.002060, Val Loss = 0.024639\n",
            "Epoch 300: Train Loss = 0.000900, Val Loss = 0.023005\n",
            "Epoch 310: Train Loss = 0.000782, Val Loss = 0.022964\n",
            "Epoch 320: Train Loss = 0.005347, Val Loss = 0.022743\n",
            "Epoch 330: Train Loss = 0.000487, Val Loss = 0.021722\n",
            "Epoch 340: Train Loss = 0.002069, Val Loss = 0.023072\n",
            "Epoch 350: Train Loss = 0.000289, Val Loss = 0.023433\n",
            "Epoch 360: Train Loss = 0.053635, Val Loss = 0.035955\n",
            "Epoch 370: Train Loss = 0.006615, Val Loss = 0.029468\n",
            "Epoch 380: Train Loss = 0.000604, Val Loss = 0.022829\n",
            "Epoch 390: Train Loss = 0.010098, Val Loss = 0.028143\n",
            "Epoch 400: Train Loss = 0.000726, Val Loss = 0.020485\n",
            "Epoch 410: Train Loss = 0.001236, Val Loss = 0.023346\n",
            "Epoch 420: Train Loss = 0.012967, Val Loss = 0.018929\n",
            "Epoch 430: Train Loss = 0.002766, Val Loss = 0.020559\n",
            "Epoch 440: Train Loss = 0.104209, Val Loss = 0.035507\n",
            "Epoch 450: Train Loss = 0.002854, Val Loss = 0.024619\n",
            "Epoch 460: Train Loss = 0.000192, Val Loss = 0.021774\n",
            "Epoch 470: Train Loss = 0.001615, Val Loss = 0.019263\n",
            "Epoch 480: Train Loss = 0.000780, Val Loss = 0.026996\n",
            "Epoch 490: Train Loss = 0.000365, Val Loss = 0.017105\n",
            "Epoch 500: Train Loss = 0.001063, Val Loss = 0.021212\n",
            "Split 5_95 — Final Test MSE: 0.016687\n",
            "\n",
            "==== Running Split: 3_97 ====\n",
            "Split 3_97 — X (INPUT): 3, Y (OUTPUT): 98\n",
            "Train: (1729, 3), Val: (433, 3), Test: (541, 3)\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.011877, Val Loss = 0.084633\n",
            "Epoch 20: Train Loss = 0.005015, Val Loss = 0.058163\n",
            "Epoch 30: Train Loss = 0.027990, Val Loss = 0.045366\n",
            "Epoch 40: Train Loss = 0.068254, Val Loss = 0.046251\n",
            "Epoch 50: Train Loss = 0.070552, Val Loss = 0.041206\n",
            "Epoch 60: Train Loss = 0.003387, Val Loss = 0.039742\n",
            "Epoch 70: Train Loss = 0.004522, Val Loss = 0.040386\n",
            "Epoch 80: Train Loss = 0.009670, Val Loss = 0.041349\n",
            "Epoch 90: Train Loss = 0.008721, Val Loss = 0.037288\n",
            "Epoch 100: Train Loss = 0.006775, Val Loss = 0.040653\n",
            "Epoch 110: Train Loss = 0.005445, Val Loss = 0.032525\n",
            "Epoch 120: Train Loss = 0.016259, Val Loss = 0.035589\n",
            "Epoch 130: Train Loss = 0.000361, Val Loss = 0.034645\n",
            "Epoch 140: Train Loss = 0.004659, Val Loss = 0.039286\n",
            "Epoch 150: Train Loss = 0.000658, Val Loss = 0.034123\n",
            "Epoch 160: Train Loss = 0.017059, Val Loss = 0.034075\n",
            "Epoch 170: Train Loss = 0.002453, Val Loss = 0.030850\n",
            "Epoch 180: Train Loss = 0.002593, Val Loss = 0.030039\n",
            "Epoch 190: Train Loss = 0.001595, Val Loss = 0.028992\n",
            "Epoch 200: Train Loss = 0.002836, Val Loss = 0.030408\n",
            "Epoch 210: Train Loss = 0.006194, Val Loss = 0.028931\n",
            "Epoch 220: Train Loss = 0.004088, Val Loss = 0.027208\n",
            "Epoch 230: Train Loss = 0.004814, Val Loss = 0.022679\n",
            "Epoch 240: Train Loss = 0.009537, Val Loss = 0.024418\n",
            "Epoch 250: Train Loss = 0.020149, Val Loss = 0.026874\n",
            "Epoch 260: Train Loss = 0.027661, Val Loss = 0.021785\n",
            "Epoch 270: Train Loss = 0.001628, Val Loss = 0.022070\n",
            "Epoch 280: Train Loss = 0.000523, Val Loss = 0.021402\n",
            "Epoch 290: Train Loss = 0.023536, Val Loss = 0.020957\n",
            "Epoch 300: Train Loss = 0.004337, Val Loss = 0.022975\n",
            "Epoch 310: Train Loss = 0.000635, Val Loss = 0.020558\n",
            "Epoch 320: Train Loss = 0.002193, Val Loss = 0.025652\n",
            "Epoch 330: Train Loss = 0.003827, Val Loss = 0.033673\n",
            "Epoch 340: Train Loss = 0.005734, Val Loss = 0.029519\n",
            "Epoch 350: Train Loss = 0.001851, Val Loss = 0.020197\n",
            "Epoch 360: Train Loss = 0.001937, Val Loss = 0.023575\n",
            "Epoch 370: Train Loss = 0.001435, Val Loss = 0.019158\n",
            "Epoch 380: Train Loss = 0.000281, Val Loss = 0.017984\n",
            "Epoch 390: Train Loss = 0.006101, Val Loss = 0.020054\n",
            "Epoch 400: Train Loss = 0.027644, Val Loss = 0.037986\n",
            "Epoch 410: Train Loss = 0.000733, Val Loss = 0.020805\n",
            "Epoch 420: Train Loss = 0.000901, Val Loss = 0.018065\n",
            "Epoch 430: Train Loss = 0.001492, Val Loss = 0.020001\n",
            "Epoch 440: Train Loss = 0.004536, Val Loss = 0.017233\n",
            "Epoch 450: Train Loss = 0.006368, Val Loss = 0.018507\n",
            "Epoch 460: Train Loss = 0.004204, Val Loss = 0.031659\n",
            "Epoch 470: Train Loss = 0.004910, Val Loss = 0.018501\n",
            "Epoch 480: Train Loss = 0.000044, Val Loss = 0.017404\n",
            "Epoch 490: Train Loss = 0.002682, Val Loss = 0.017346\n",
            "Epoch 500: Train Loss = 0.000277, Val Loss = 0.019350\n",
            "Split 3_97 — Final Test MSE: 0.012034\n",
            "\n",
            "==== Running Split: 1_99 ====\n",
            "Split 1_99 — X (INPUT): 1, Y (OUTPUT): 100\n",
            "Train: (1729, 1), Val: (433, 1), Test: (541, 1)\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.146947, Val Loss = 0.181048\n",
            "Epoch 20: Train Loss = 0.003481, Val Loss = 0.167712\n",
            "Epoch 30: Train Loss = 0.059510, Val Loss = 0.132918\n",
            "Epoch 40: Train Loss = 0.032382, Val Loss = 0.105079\n",
            "Epoch 50: Train Loss = 0.121130, Val Loss = 0.103731\n",
            "Epoch 60: Train Loss = 0.008535, Val Loss = 0.078700\n",
            "Epoch 70: Train Loss = 0.005509, Val Loss = 0.073959\n",
            "Epoch 80: Train Loss = 0.011817, Val Loss = 0.067156\n",
            "Epoch 90: Train Loss = 0.016715, Val Loss = 0.079367\n",
            "Epoch 100: Train Loss = 0.214531, Val Loss = 0.169158\n",
            "Epoch 110: Train Loss = 0.324911, Val Loss = 0.092339\n",
            "Epoch 120: Train Loss = 0.021805, Val Loss = 0.072190\n",
            "Epoch 130: Train Loss = 0.008901, Val Loss = 0.066024\n",
            "Epoch 140: Train Loss = 0.214141, Val Loss = 0.089720\n",
            "Epoch 150: Train Loss = 0.046318, Val Loss = 0.068840\n",
            "Epoch 160: Train Loss = 0.007091, Val Loss = 0.063943\n",
            "Epoch 170: Train Loss = 0.049143, Val Loss = 0.064191\n",
            "Epoch 180: Train Loss = 0.002106, Val Loss = 0.066567\n",
            "Epoch 190: Train Loss = 0.006445, Val Loss = 0.063514\n",
            "Epoch 200: Train Loss = 0.011091, Val Loss = 0.060748\n",
            "Epoch 210: Train Loss = 0.013325, Val Loss = 0.068486\n",
            "Epoch 220: Train Loss = 0.008920, Val Loss = 0.064051\n",
            "Epoch 230: Train Loss = 0.007489, Val Loss = 0.066144\n",
            "Epoch 240: Train Loss = 0.007626, Val Loss = 0.064675\n",
            "Epoch 250: Train Loss = 0.009927, Val Loss = 0.066158\n",
            "Epoch 260: Train Loss = 0.056643, Val Loss = 0.065262\n",
            "Epoch 270: Train Loss = 0.004762, Val Loss = 0.060780\n",
            "Epoch 280: Train Loss = 0.001724, Val Loss = 0.062946\n",
            "Epoch 290: Train Loss = 0.006003, Val Loss = 0.063840\n",
            "Epoch 300: Train Loss = 0.007176, Val Loss = 0.060221\n",
            "Epoch 310: Train Loss = 0.005561, Val Loss = 0.056386\n",
            "Epoch 320: Train Loss = 0.023473, Val Loss = 0.057494\n",
            "Epoch 330: Train Loss = 0.020269, Val Loss = 0.059824\n",
            "Epoch 340: Train Loss = 0.010271, Val Loss = 0.063734\n",
            "Epoch 350: Train Loss = 0.005656, Val Loss = 0.056805\n",
            "Epoch 360: Train Loss = 0.009144, Val Loss = 0.057176\n",
            "Epoch 370: Train Loss = 0.000403, Val Loss = 0.052776\n",
            "Epoch 380: Train Loss = 0.005404, Val Loss = 0.056823\n",
            "Epoch 390: Train Loss = 0.010721, Val Loss = 0.061697\n",
            "Epoch 400: Train Loss = 0.002779, Val Loss = 0.071943\n",
            "Epoch 410: Train Loss = 0.010235, Val Loss = 0.066816\n",
            "Epoch 420: Train Loss = 0.000291, Val Loss = 0.059867\n",
            "Epoch 430: Train Loss = 0.016640, Val Loss = 0.061699\n",
            "Epoch 440: Train Loss = 0.001880, Val Loss = 0.060538\n",
            "Epoch 450: Train Loss = 0.017140, Val Loss = 0.066109\n",
            "Epoch 460: Train Loss = 0.001889, Val Loss = 0.060094\n",
            "Epoch 470: Train Loss = 0.001915, Val Loss = 0.065104\n",
            "Epoch 480: Train Loss = 0.003223, Val Loss = 0.057680\n",
            "Epoch 490: Train Loss = 0.000456, Val Loss = 0.060298\n",
            "Epoch 500: Train Loss = 0.007632, Val Loss = 0.062308\n",
            "Split 1_99 — Final Test MSE: 0.046619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**New Nlinear experiment**"
      ],
      "metadata": {
        "id": "BVQUzali2Ebe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Model Definition\n",
        "class NLinear(nn.Module):\n",
        "    def __init__(self, seq_len, pred_len, individual=False):\n",
        "        super(NLinear, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.individual = individual\n",
        "        if self.individual:\n",
        "            self.Linear = nn.ModuleList([nn.Linear(self.seq_len, self.pred_len)])\n",
        "        else:\n",
        "            self.Linear = nn.Linear(self.seq_len, self.pred_len)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) == 3:\n",
        "            x = x.squeeze(-1)\n",
        "        seq_last = x[:, -1:].detach()\n",
        "        x = x - seq_last\n",
        "        if self.individual:\n",
        "            out = torch.zeros([x.size(0), self.pred_len], dtype=x.dtype).to(x.device)\n",
        "            for i in range(1):\n",
        "                out[:, :] = self.Linear[i](x)\n",
        "        else:\n",
        "            out = self.Linear(x)\n",
        "        out = out + seq_last\n",
        "        return out.unsqueeze(-1)\n",
        "\n",
        "# Hyperparameters and Output Storage\n",
        "nlinear_config = {\n",
        "    'epochs': 500,\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 0.001,\n",
        "    'individual': False,\n",
        "    'model_name': 'NLinear'\n",
        "}\n",
        "\n",
        "NLinear_mse = pd.DataFrame(columns=['Split', 'Test_MSE'])\n",
        "splits = [(80, 20), (60, 40), (50, 50), (40, 60), (20, 80), (10, 90), (5, 95), (3, 97), (1, 99)]\n",
        "drive_path = '/content/drive/MyDrive/DSSM-Figures'\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "for train_pct, test_pct in splits:\n",
        "    split_name = f\"{train_pct}_{test_pct}\"\n",
        "    print(f\"\\n==== Running Split: {split_name} ====\")\n",
        "\n",
        "    # Split file_ids\n",
        "    file_ids = df_output['file_id'].unique()\n",
        "    trainval_ids, test_ids = train_test_split(file_ids, test_size=0.2, random_state=42)\n",
        "    train_ids, val_ids = train_test_split(trainval_ids, test_size=0.2, random_state=42)\n",
        "\n",
        "    def extract_X_Y(ids, pct):\n",
        "        df_subset = df_output[df_output['file_id'].isin(ids)]\n",
        "        pivoted = df_subset.pivot(index='file_id', columns='timestep', values='CO2').values\n",
        "        split_idx = int(pct / 100 * 101)\n",
        "        X = pivoted[:, :split_idx]\n",
        "        Y = pivoted[:, split_idx:]\n",
        "        return X, Y\n",
        "\n",
        "    X_train, Y_train = extract_X_Y(train_ids, train_pct)\n",
        "    X_val, Y_val = extract_X_Y(val_ids, train_pct)\n",
        "    X_test, Y_test = extract_X_Y(test_ids, train_pct)\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train[:, :, None], dtype=torch.float32)\n",
        "    Y_train_tensor = torch.tensor(Y_train[:, :, None], dtype=torch.float32)\n",
        "    X_val_tensor = torch.tensor(X_val[:, :, None], dtype=torch.float32)\n",
        "    Y_val_tensor = torch.tensor(Y_val[:, :, None], dtype=torch.float32)\n",
        "    X_test_tensor = torch.tensor(X_test[:, :, None], dtype=torch.float32)\n",
        "    Y_test_tensor = torch.tensor(Y_test[:, :, None], dtype=torch.float32)\n",
        "\n",
        "    print(f\"Split {split_name} — X (INPUT): {X_train.shape[1]}, Y (OUTPUT): {Y_train.shape[1]}\")\n",
        "    print(f\"Train: {X_train_tensor.shape}, Val: {X_val_tensor.shape}, Test: {X_test_tensor.shape}\")\n",
        "\n",
        "    train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
        "    val_dataset = TensorDataset(X_val_tensor, Y_val_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=nlinear_config['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=nlinear_config['batch_size'])\n",
        "    test_loader = DataLoader(test_dataset, batch_size=nlinear_config['batch_size'])\n",
        "\n",
        "    model = NLinear(seq_len=X_train.shape[1], pred_len=Y_train.shape[1], individual=nlinear_config['individual'])\n",
        "    optimizer = optim.Adam(model.parameters(), lr=nlinear_config['learning_rate'])\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "\n",
        "    print(\"Training started...\")\n",
        "    for epoch in range(nlinear_config['epochs']):\n",
        "        model.train()\n",
        "        for X_batch, Y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X_batch)\n",
        "            loss = criterion(preds, Y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, Y_batch in val_loader:\n",
        "                preds = model(X_batch)\n",
        "                val_loss += criterion(preds, Y_batch).item()\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}: Train Loss = {loss.item():.6f}, Val Loss = {val_loss:.6f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict()\n",
        "            #print(f\"New best model found at epoch {epoch+1} with val loss {val_loss:.6f}\")\n",
        "            torch.save(best_model_state, os.path.join(drive_path, f\"best_model_{split_name}.pt\"))\n",
        "\n",
        "    model.load_state_dict(torch.load(os.path.join(drive_path, f\"best_model_{split_name}.pt\")))\n",
        "    model.eval()\n",
        "\n",
        "    total_mse = 0.0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, Y_batch in test_loader:\n",
        "            outputs = model(X_batch)\n",
        "            batch_mse = criterion(outputs, Y_batch).item()\n",
        "            total_mse += batch_mse * X_batch.size(0)\n",
        "            total_samples += X_batch.size(0)\n",
        "\n",
        "    avg_mse = total_mse / total_samples\n",
        "    print(f\"Final Test MSE ({split_name}): {avg_mse:.6f}\")\n",
        "    NLinear_mse.loc[len(NLinear_mse)] = [split_name, avg_mse]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eojVeGCUFqoi",
        "outputId": "3752a039-15c0-4e18-cff9-5f256936ede3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Running Split: 80_20 ====\n",
            "Split 80_20 — X (INPUT): 80, Y (OUTPUT): 21\n",
            "Train: torch.Size([1729, 80, 1]), Val: torch.Size([433, 80, 1]), Test: torch.Size([541, 80, 1])\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.000252, Val Loss = 0.000861\n",
            "Epoch 20: Train Loss = 0.000139, Val Loss = 0.000282\n",
            "Epoch 30: Train Loss = 0.000080, Val Loss = 0.000136\n",
            "Epoch 40: Train Loss = 0.000057, Val Loss = 0.000084\n",
            "Epoch 50: Train Loss = 0.000020, Val Loss = 0.000071\n",
            "Epoch 60: Train Loss = 0.000014, Val Loss = 0.000048\n",
            "Epoch 70: Train Loss = 0.000070, Val Loss = 0.000042\n",
            "Epoch 80: Train Loss = 0.000024, Val Loss = 0.000035\n",
            "Epoch 90: Train Loss = 0.000076, Val Loss = 0.000053\n",
            "Epoch 100: Train Loss = 0.000030, Val Loss = 0.000031\n",
            "Epoch 110: Train Loss = 0.000014, Val Loss = 0.000035\n",
            "Epoch 120: Train Loss = 0.000027, Val Loss = 0.000030\n",
            "Epoch 130: Train Loss = 0.000064, Val Loss = 0.000019\n",
            "Epoch 140: Train Loss = 0.000017, Val Loss = 0.000017\n",
            "Epoch 150: Train Loss = 0.000003, Val Loss = 0.000017\n",
            "Epoch 160: Train Loss = 0.000052, Val Loss = 0.000035\n",
            "Epoch 170: Train Loss = 0.000014, Val Loss = 0.000016\n",
            "Epoch 180: Train Loss = 0.000003, Val Loss = 0.000024\n",
            "Epoch 190: Train Loss = 0.000014, Val Loss = 0.000034\n",
            "Epoch 200: Train Loss = 0.000015, Val Loss = 0.000037\n",
            "Epoch 210: Train Loss = 0.000009, Val Loss = 0.000016\n",
            "Epoch 220: Train Loss = 0.000002, Val Loss = 0.000014\n",
            "Epoch 230: Train Loss = 0.000003, Val Loss = 0.000017\n",
            "Epoch 240: Train Loss = 0.000004, Val Loss = 0.000015\n",
            "Epoch 250: Train Loss = 0.000029, Val Loss = 0.000016\n",
            "Epoch 260: Train Loss = 0.000015, Val Loss = 0.000019\n",
            "Epoch 270: Train Loss = 0.000001, Val Loss = 0.000013\n",
            "Epoch 280: Train Loss = 0.000005, Val Loss = 0.000015\n",
            "Epoch 290: Train Loss = 0.000004, Val Loss = 0.000024\n",
            "Epoch 300: Train Loss = 0.000007, Val Loss = 0.000022\n",
            "Epoch 310: Train Loss = 0.000004, Val Loss = 0.000023\n",
            "Epoch 320: Train Loss = 0.000004, Val Loss = 0.000014\n",
            "Epoch 330: Train Loss = 0.000006, Val Loss = 0.000023\n",
            "Epoch 340: Train Loss = 0.000009, Val Loss = 0.000028\n",
            "Epoch 350: Train Loss = 0.000008, Val Loss = 0.000014\n",
            "Epoch 360: Train Loss = 0.000004, Val Loss = 0.000016\n",
            "Epoch 370: Train Loss = 0.000007, Val Loss = 0.000020\n",
            "Epoch 380: Train Loss = 0.000002, Val Loss = 0.000022\n",
            "Epoch 390: Train Loss = 0.000008, Val Loss = 0.000015\n",
            "Epoch 400: Train Loss = 0.000012, Val Loss = 0.000025\n",
            "Epoch 410: Train Loss = 0.000003, Val Loss = 0.000016\n",
            "Epoch 420: Train Loss = 0.000001, Val Loss = 0.000017\n",
            "Epoch 430: Train Loss = 0.000005, Val Loss = 0.000014\n",
            "Epoch 440: Train Loss = 0.000003, Val Loss = 0.000011\n",
            "Epoch 450: Train Loss = 0.000001, Val Loss = 0.000016\n",
            "Epoch 460: Train Loss = 0.000011, Val Loss = 0.000012\n",
            "Epoch 470: Train Loss = 0.000002, Val Loss = 0.000013\n",
            "Epoch 480: Train Loss = 0.000222, Val Loss = 0.000412\n",
            "Epoch 490: Train Loss = 0.000009, Val Loss = 0.000027\n",
            "Epoch 500: Train Loss = 0.000020, Val Loss = 0.000026\n",
            "Final Test MSE (80_20): 0.000010\n",
            "\n",
            "==== Running Split: 60_40 ====\n",
            "Split 60_40 — X (INPUT): 60, Y (OUTPUT): 41\n",
            "Train: torch.Size([1729, 60, 1]), Val: torch.Size([433, 60, 1]), Test: torch.Size([541, 60, 1])\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.000303, Val Loss = 0.001777\n",
            "Epoch 20: Train Loss = 0.000478, Val Loss = 0.000875\n",
            "Epoch 30: Train Loss = 0.000122, Val Loss = 0.000629\n",
            "Epoch 40: Train Loss = 0.000196, Val Loss = 0.000503\n",
            "Epoch 50: Train Loss = 0.000311, Val Loss = 0.000461\n",
            "Epoch 60: Train Loss = 0.000213, Val Loss = 0.000428\n",
            "Epoch 70: Train Loss = 0.000062, Val Loss = 0.000411\n",
            "Epoch 80: Train Loss = 0.000071, Val Loss = 0.000396\n",
            "Epoch 90: Train Loss = 0.000207, Val Loss = 0.000387\n",
            "Epoch 100: Train Loss = 0.000888, Val Loss = 0.000381\n",
            "Epoch 110: Train Loss = 0.001096, Val Loss = 0.000414\n",
            "Epoch 120: Train Loss = 0.000604, Val Loss = 0.000339\n",
            "Epoch 130: Train Loss = 0.000318, Val Loss = 0.000323\n",
            "Epoch 140: Train Loss = 0.000548, Val Loss = 0.000320\n",
            "Epoch 150: Train Loss = 0.000121, Val Loss = 0.000314\n",
            "Epoch 160: Train Loss = 0.000180, Val Loss = 0.000316\n",
            "Epoch 170: Train Loss = 0.000014, Val Loss = 0.000328\n",
            "Epoch 180: Train Loss = 0.000014, Val Loss = 0.000318\n",
            "Epoch 190: Train Loss = 0.000072, Val Loss = 0.000380\n",
            "Epoch 200: Train Loss = 0.000030, Val Loss = 0.000287\n",
            "Epoch 210: Train Loss = 0.000363, Val Loss = 0.000333\n",
            "Epoch 220: Train Loss = 0.000511, Val Loss = 0.000315\n",
            "Epoch 230: Train Loss = 0.000299, Val Loss = 0.000278\n",
            "Epoch 240: Train Loss = 0.000356, Val Loss = 0.000280\n",
            "Epoch 250: Train Loss = 0.000037, Val Loss = 0.000279\n",
            "Epoch 260: Train Loss = 0.000216, Val Loss = 0.000397\n",
            "Epoch 270: Train Loss = 0.000277, Val Loss = 0.000258\n",
            "Epoch 280: Train Loss = 0.000325, Val Loss = 0.000258\n",
            "Epoch 290: Train Loss = 0.000013, Val Loss = 0.000264\n",
            "Epoch 300: Train Loss = 0.000101, Val Loss = 0.000252\n",
            "Epoch 310: Train Loss = 0.000035, Val Loss = 0.000226\n",
            "Epoch 320: Train Loss = 0.000012, Val Loss = 0.000256\n",
            "Epoch 330: Train Loss = 0.000083, Val Loss = 0.000250\n",
            "Epoch 340: Train Loss = 0.000234, Val Loss = 0.000247\n",
            "Epoch 350: Train Loss = 0.000197, Val Loss = 0.000258\n",
            "Epoch 360: Train Loss = 0.000056, Val Loss = 0.000212\n",
            "Epoch 370: Train Loss = 0.000118, Val Loss = 0.000214\n",
            "Epoch 380: Train Loss = 0.000138, Val Loss = 0.000228\n",
            "Epoch 390: Train Loss = 0.000005, Val Loss = 0.000209\n",
            "Epoch 400: Train Loss = 0.000005, Val Loss = 0.000246\n",
            "Epoch 410: Train Loss = 0.000046, Val Loss = 0.000219\n",
            "Epoch 420: Train Loss = 0.000111, Val Loss = 0.000255\n",
            "Epoch 430: Train Loss = 0.000009, Val Loss = 0.000217\n",
            "Epoch 440: Train Loss = 0.000211, Val Loss = 0.000216\n",
            "Epoch 450: Train Loss = 0.000012, Val Loss = 0.000200\n",
            "Epoch 460: Train Loss = 0.000170, Val Loss = 0.000192\n",
            "Epoch 470: Train Loss = 0.000263, Val Loss = 0.000206\n",
            "Epoch 480: Train Loss = 0.000407, Val Loss = 0.000205\n",
            "Epoch 490: Train Loss = 0.000707, Val Loss = 0.000207\n",
            "Epoch 500: Train Loss = 0.000289, Val Loss = 0.000255\n",
            "Final Test MSE (60_40): 0.000140\n",
            "\n",
            "==== Running Split: 50_50 ====\n",
            "Split 50_50 — X (INPUT): 50, Y (OUTPUT): 51\n",
            "Train: torch.Size([1729, 50, 1]), Val: torch.Size([433, 50, 1]), Test: torch.Size([541, 50, 1])\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.009217, Val Loss = 0.002435\n",
            "Epoch 20: Train Loss = 0.000424, Val Loss = 0.001495\n",
            "Epoch 30: Train Loss = 0.000320, Val Loss = 0.001156\n",
            "Epoch 40: Train Loss = 0.001463, Val Loss = 0.001039\n",
            "Epoch 50: Train Loss = 0.001377, Val Loss = 0.000943\n",
            "Epoch 60: Train Loss = 0.000545, Val Loss = 0.000888\n",
            "Epoch 70: Train Loss = 0.000376, Val Loss = 0.000860\n",
            "Epoch 80: Train Loss = 0.000060, Val Loss = 0.000797\n",
            "Epoch 90: Train Loss = 0.000243, Val Loss = 0.000773\n",
            "Epoch 100: Train Loss = 0.000771, Val Loss = 0.000746\n",
            "Epoch 110: Train Loss = 0.000166, Val Loss = 0.000735\n",
            "Epoch 120: Train Loss = 0.000120, Val Loss = 0.000699\n",
            "Epoch 130: Train Loss = 0.000037, Val Loss = 0.000725\n",
            "Epoch 140: Train Loss = 0.000043, Val Loss = 0.000703\n",
            "Epoch 150: Train Loss = 0.003217, Val Loss = 0.000756\n",
            "Epoch 160: Train Loss = 0.001669, Val Loss = 0.000681\n",
            "Epoch 170: Train Loss = 0.000431, Val Loss = 0.000630\n",
            "Epoch 180: Train Loss = 0.000084, Val Loss = 0.000623\n",
            "Epoch 190: Train Loss = 0.000221, Val Loss = 0.000693\n",
            "Epoch 200: Train Loss = 0.000023, Val Loss = 0.000598\n",
            "Epoch 210: Train Loss = 0.001004, Val Loss = 0.000586\n",
            "Epoch 220: Train Loss = 0.000060, Val Loss = 0.000574\n",
            "Epoch 230: Train Loss = 0.000265, Val Loss = 0.000588\n",
            "Epoch 240: Train Loss = 0.000082, Val Loss = 0.000562\n",
            "Epoch 250: Train Loss = 0.000074, Val Loss = 0.000572\n",
            "Epoch 260: Train Loss = 0.000048, Val Loss = 0.000564\n",
            "Epoch 270: Train Loss = 0.000026, Val Loss = 0.000551\n",
            "Epoch 280: Train Loss = 0.000230, Val Loss = 0.000550\n",
            "Epoch 290: Train Loss = 0.000282, Val Loss = 0.000545\n",
            "Epoch 300: Train Loss = 0.000478, Val Loss = 0.000690\n",
            "Epoch 310: Train Loss = 0.000010, Val Loss = 0.000521\n",
            "Epoch 320: Train Loss = 0.000023, Val Loss = 0.000525\n",
            "Epoch 330: Train Loss = 0.000140, Val Loss = 0.000533\n",
            "Epoch 340: Train Loss = 0.000984, Val Loss = 0.000555\n",
            "Epoch 350: Train Loss = 0.000016, Val Loss = 0.000523\n",
            "Epoch 360: Train Loss = 0.000068, Val Loss = 0.000523\n",
            "Epoch 370: Train Loss = 0.000216, Val Loss = 0.000530\n",
            "Epoch 380: Train Loss = 0.000005, Val Loss = 0.000524\n",
            "Epoch 390: Train Loss = 0.000013, Val Loss = 0.000504\n",
            "Epoch 400: Train Loss = 0.000028, Val Loss = 0.000526\n",
            "Epoch 410: Train Loss = 0.000067, Val Loss = 0.000518\n",
            "Epoch 420: Train Loss = 0.000218, Val Loss = 0.000509\n",
            "Epoch 430: Train Loss = 0.000119, Val Loss = 0.000489\n",
            "Epoch 440: Train Loss = 0.000037, Val Loss = 0.000488\n",
            "Epoch 450: Train Loss = 0.000089, Val Loss = 0.000507\n",
            "Epoch 460: Train Loss = 0.000646, Val Loss = 0.000518\n",
            "Epoch 470: Train Loss = 0.000612, Val Loss = 0.000485\n",
            "Epoch 480: Train Loss = 0.000573, Val Loss = 0.000487\n",
            "Epoch 490: Train Loss = 0.000040, Val Loss = 0.000477\n",
            "Epoch 500: Train Loss = 0.000027, Val Loss = 0.000492\n",
            "Final Test MSE (50_50): 0.000340\n",
            "\n",
            "==== Running Split: 40_60 ====\n",
            "Split 40_60 — X (INPUT): 40, Y (OUTPUT): 61\n",
            "Train: torch.Size([1729, 40, 1]), Val: torch.Size([433, 40, 1]), Test: torch.Size([541, 40, 1])\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.000360, Val Loss = 0.005304\n",
            "Epoch 20: Train Loss = 0.000665, Val Loss = 0.003680\n",
            "Epoch 30: Train Loss = 0.002173, Val Loss = 0.002856\n",
            "Epoch 40: Train Loss = 0.000146, Val Loss = 0.002396\n",
            "Epoch 50: Train Loss = 0.001300, Val Loss = 0.002165\n",
            "Epoch 60: Train Loss = 0.000249, Val Loss = 0.001992\n",
            "Epoch 70: Train Loss = 0.000246, Val Loss = 0.001925\n",
            "Epoch 80: Train Loss = 0.000042, Val Loss = 0.001835\n",
            "Epoch 90: Train Loss = 0.000391, Val Loss = 0.001790\n",
            "Epoch 100: Train Loss = 0.000064, Val Loss = 0.001757\n",
            "Epoch 110: Train Loss = 0.002834, Val Loss = 0.001835\n",
            "Epoch 120: Train Loss = 0.000073, Val Loss = 0.001727\n",
            "Epoch 130: Train Loss = 0.000165, Val Loss = 0.001738\n",
            "Epoch 140: Train Loss = 0.003266, Val Loss = 0.001797\n",
            "Epoch 150: Train Loss = 0.001253, Val Loss = 0.001702\n",
            "Epoch 160: Train Loss = 0.006600, Val Loss = 0.001833\n",
            "Epoch 170: Train Loss = 0.000102, Val Loss = 0.001726\n",
            "Epoch 180: Train Loss = 0.000350, Val Loss = 0.001717\n",
            "Epoch 190: Train Loss = 0.002583, Val Loss = 0.001727\n",
            "Epoch 200: Train Loss = 0.000088, Val Loss = 0.001710\n",
            "Epoch 210: Train Loss = 0.000551, Val Loss = 0.001711\n",
            "Epoch 220: Train Loss = 0.000192, Val Loss = 0.001712\n",
            "Epoch 230: Train Loss = 0.000046, Val Loss = 0.001731\n",
            "Epoch 240: Train Loss = 0.001374, Val Loss = 0.001765\n",
            "Epoch 250: Train Loss = 0.002295, Val Loss = 0.001773\n",
            "Epoch 260: Train Loss = 0.001603, Val Loss = 0.001739\n",
            "Epoch 270: Train Loss = 0.000045, Val Loss = 0.001703\n",
            "Epoch 280: Train Loss = 0.000116, Val Loss = 0.001728\n",
            "Epoch 290: Train Loss = 0.000525, Val Loss = 0.001705\n",
            "Epoch 300: Train Loss = 0.000066, Val Loss = 0.001708\n",
            "Epoch 310: Train Loss = 0.020099, Val Loss = 0.002051\n",
            "Epoch 320: Train Loss = 0.000184, Val Loss = 0.001761\n",
            "Epoch 330: Train Loss = 0.000024, Val Loss = 0.001751\n",
            "Epoch 340: Train Loss = 0.000207, Val Loss = 0.001738\n",
            "Epoch 350: Train Loss = 0.000143, Val Loss = 0.001737\n",
            "Epoch 360: Train Loss = 0.000109, Val Loss = 0.001751\n",
            "Epoch 370: Train Loss = 0.000518, Val Loss = 0.001731\n",
            "Epoch 380: Train Loss = 0.000092, Val Loss = 0.001768\n",
            "Epoch 390: Train Loss = 0.001094, Val Loss = 0.001830\n",
            "Epoch 400: Train Loss = 0.000054, Val Loss = 0.001763\n",
            "Epoch 410: Train Loss = 0.002475, Val Loss = 0.001891\n",
            "Epoch 420: Train Loss = 0.001794, Val Loss = 0.001818\n",
            "Epoch 430: Train Loss = 0.005196, Val Loss = 0.001904\n",
            "Epoch 440: Train Loss = 0.000043, Val Loss = 0.001745\n",
            "Epoch 450: Train Loss = 0.000018, Val Loss = 0.001765\n",
            "Epoch 460: Train Loss = 0.002093, Val Loss = 0.001815\n",
            "Epoch 470: Train Loss = 0.001090, Val Loss = 0.001789\n",
            "Epoch 480: Train Loss = 0.000189, Val Loss = 0.001762\n",
            "Epoch 490: Train Loss = 0.001055, Val Loss = 0.001763\n",
            "Epoch 500: Train Loss = 0.002784, Val Loss = 0.001834\n",
            "Final Test MSE (40_60): 0.001077\n",
            "\n",
            "==== Running Split: 20_80 ====\n",
            "Split 20_80 — X (INPUT): 20, Y (OUTPUT): 81\n",
            "Train: torch.Size([1729, 20, 1]), Val: torch.Size([433, 20, 1]), Test: torch.Size([541, 20, 1])\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.019322, Val Loss = 0.024558\n",
            "Epoch 20: Train Loss = 0.005497, Val Loss = 0.021164\n",
            "Epoch 30: Train Loss = 0.000463, Val Loss = 0.019535\n",
            "Epoch 40: Train Loss = 0.008600, Val Loss = 0.018393\n",
            "Epoch 50: Train Loss = 0.004983, Val Loss = 0.017545\n",
            "Epoch 60: Train Loss = 0.045409, Val Loss = 0.016755\n",
            "Epoch 70: Train Loss = 0.080640, Val Loss = 0.015990\n",
            "Epoch 80: Train Loss = 0.000391, Val Loss = 0.015407\n",
            "Epoch 90: Train Loss = 0.001378, Val Loss = 0.014898\n",
            "Epoch 100: Train Loss = 0.004271, Val Loss = 0.014529\n",
            "Epoch 110: Train Loss = 0.002601, Val Loss = 0.014101\n",
            "Epoch 120: Train Loss = 0.000520, Val Loss = 0.013684\n",
            "Epoch 130: Train Loss = 0.020001, Val Loss = 0.013416\n",
            "Epoch 140: Train Loss = 0.023815, Val Loss = 0.013147\n",
            "Epoch 150: Train Loss = 0.032610, Val Loss = 0.012946\n",
            "Epoch 160: Train Loss = 0.000888, Val Loss = 0.012833\n",
            "Epoch 170: Train Loss = 0.043859, Val Loss = 0.012724\n",
            "Epoch 180: Train Loss = 0.000679, Val Loss = 0.012525\n",
            "Epoch 190: Train Loss = 0.000873, Val Loss = 0.012377\n",
            "Epoch 200: Train Loss = 0.000109, Val Loss = 0.012272\n",
            "Epoch 210: Train Loss = 0.002830, Val Loss = 0.012296\n",
            "Epoch 220: Train Loss = 0.003989, Val Loss = 0.012170\n",
            "Epoch 230: Train Loss = 0.001545, Val Loss = 0.012163\n",
            "Epoch 240: Train Loss = 0.003911, Val Loss = 0.012134\n",
            "Epoch 250: Train Loss = 0.003007, Val Loss = 0.012067\n",
            "Epoch 260: Train Loss = 0.000229, Val Loss = 0.012017\n",
            "Epoch 270: Train Loss = 0.000029, Val Loss = 0.012004\n",
            "Epoch 280: Train Loss = 0.114010, Val Loss = 0.012017\n",
            "Epoch 290: Train Loss = 0.000605, Val Loss = 0.011987\n",
            "Epoch 300: Train Loss = 0.033819, Val Loss = 0.011875\n",
            "Epoch 310: Train Loss = 0.001588, Val Loss = 0.011865\n",
            "Epoch 320: Train Loss = 0.000077, Val Loss = 0.011839\n",
            "Epoch 330: Train Loss = 0.000723, Val Loss = 0.011808\n",
            "Epoch 340: Train Loss = 0.028437, Val Loss = 0.011853\n",
            "Epoch 350: Train Loss = 0.003750, Val Loss = 0.011887\n",
            "Epoch 360: Train Loss = 0.000397, Val Loss = 0.011737\n",
            "Epoch 370: Train Loss = 0.005432, Val Loss = 0.011687\n",
            "Epoch 380: Train Loss = 0.004813, Val Loss = 0.011682\n",
            "Epoch 390: Train Loss = 0.000125, Val Loss = 0.011653\n",
            "Epoch 400: Train Loss = 0.020263, Val Loss = 0.011658\n",
            "Epoch 410: Train Loss = 0.002075, Val Loss = 0.011709\n",
            "Epoch 420: Train Loss = 0.032754, Val Loss = 0.011606\n",
            "Epoch 430: Train Loss = 0.002564, Val Loss = 0.011570\n",
            "Epoch 440: Train Loss = 0.000378, Val Loss = 0.011603\n",
            "Epoch 450: Train Loss = 0.004954, Val Loss = 0.011564\n",
            "Epoch 460: Train Loss = 0.074457, Val Loss = 0.011528\n",
            "Epoch 470: Train Loss = 0.003963, Val Loss = 0.011504\n",
            "Epoch 480: Train Loss = 0.035914, Val Loss = 0.011592\n",
            "Epoch 490: Train Loss = 0.001799, Val Loss = 0.011476\n",
            "Epoch 500: Train Loss = 0.000039, Val Loss = 0.011466\n",
            "Final Test MSE (20_80): 0.009894\n",
            "\n",
            "==== Running Split: 10_90 ====\n",
            "Split 10_90 — X (INPUT): 10, Y (OUTPUT): 91\n",
            "Train: torch.Size([1729, 10, 1]), Val: torch.Size([433, 10, 1]), Test: torch.Size([541, 10, 1])\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.047875, Val Loss = 0.067081\n",
            "Epoch 20: Train Loss = 0.024967, Val Loss = 0.049318\n",
            "Epoch 30: Train Loss = 0.017496, Val Loss = 0.046169\n",
            "Epoch 40: Train Loss = 0.002188, Val Loss = 0.044183\n",
            "Epoch 50: Train Loss = 0.180149, Val Loss = 0.042576\n",
            "Epoch 60: Train Loss = 0.001022, Val Loss = 0.041428\n",
            "Epoch 70: Train Loss = 0.002233, Val Loss = 0.040392\n",
            "Epoch 80: Train Loss = 0.023350, Val Loss = 0.039407\n",
            "Epoch 90: Train Loss = 0.056711, Val Loss = 0.038593\n",
            "Epoch 100: Train Loss = 0.006760, Val Loss = 0.037812\n",
            "Epoch 110: Train Loss = 0.011377, Val Loss = 0.036962\n",
            "Epoch 120: Train Loss = 0.000432, Val Loss = 0.036322\n",
            "Epoch 130: Train Loss = 0.206308, Val Loss = 0.035777\n",
            "Epoch 140: Train Loss = 0.026053, Val Loss = 0.035515\n",
            "Epoch 150: Train Loss = 0.003525, Val Loss = 0.035114\n",
            "Epoch 160: Train Loss = 0.010922, Val Loss = 0.034754\n",
            "Epoch 170: Train Loss = 0.012301, Val Loss = 0.034411\n",
            "Epoch 180: Train Loss = 0.001933, Val Loss = 0.034131\n",
            "Epoch 190: Train Loss = 0.001557, Val Loss = 0.033797\n",
            "Epoch 200: Train Loss = 0.008552, Val Loss = 0.033490\n",
            "Epoch 210: Train Loss = 0.000892, Val Loss = 0.033547\n",
            "Epoch 220: Train Loss = 0.016115, Val Loss = 0.033521\n",
            "Epoch 230: Train Loss = 0.029911, Val Loss = 0.033135\n",
            "Epoch 240: Train Loss = 0.004804, Val Loss = 0.033031\n",
            "Epoch 250: Train Loss = 0.054639, Val Loss = 0.032786\n",
            "Epoch 260: Train Loss = 0.000962, Val Loss = 0.032945\n",
            "Epoch 270: Train Loss = 0.004995, Val Loss = 0.032703\n",
            "Epoch 280: Train Loss = 0.001762, Val Loss = 0.032729\n",
            "Epoch 290: Train Loss = 0.020403, Val Loss = 0.032460\n",
            "Epoch 300: Train Loss = 0.000193, Val Loss = 0.032355\n",
            "Epoch 310: Train Loss = 0.002003, Val Loss = 0.032469\n",
            "Epoch 320: Train Loss = 0.031551, Val Loss = 0.032440\n",
            "Epoch 330: Train Loss = 0.006501, Val Loss = 0.032324\n",
            "Epoch 340: Train Loss = 0.339192, Val Loss = 0.032099\n",
            "Epoch 350: Train Loss = 0.000650, Val Loss = 0.032063\n",
            "Epoch 360: Train Loss = 0.013626, Val Loss = 0.032075\n",
            "Epoch 370: Train Loss = 0.013767, Val Loss = 0.032135\n",
            "Epoch 380: Train Loss = 0.022344, Val Loss = 0.031985\n",
            "Epoch 390: Train Loss = 0.000268, Val Loss = 0.031859\n",
            "Epoch 400: Train Loss = 0.062157, Val Loss = 0.031887\n",
            "Epoch 410: Train Loss = 0.033968, Val Loss = 0.031903\n",
            "Epoch 420: Train Loss = 0.006639, Val Loss = 0.031916\n",
            "Epoch 430: Train Loss = 0.001139, Val Loss = 0.031742\n",
            "Epoch 440: Train Loss = 0.082944, Val Loss = 0.031886\n",
            "Epoch 450: Train Loss = 0.001169, Val Loss = 0.031643\n",
            "Epoch 460: Train Loss = 0.017865, Val Loss = 0.031672\n",
            "Epoch 470: Train Loss = 0.005394, Val Loss = 0.031521\n",
            "Epoch 480: Train Loss = 0.019450, Val Loss = 0.031510\n",
            "Epoch 490: Train Loss = 0.011208, Val Loss = 0.031622\n",
            "Epoch 500: Train Loss = 0.014290, Val Loss = 0.031501\n",
            "Final Test MSE (10_90): 0.024607\n",
            "\n",
            "==== Running Split: 5_95 ====\n",
            "Split 5_95 — X (INPUT): 5, Y (OUTPUT): 96\n",
            "Train: torch.Size([1729, 5, 1]), Val: torch.Size([433, 5, 1]), Test: torch.Size([541, 5, 1])\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.149409, Val Loss = 0.269215\n",
            "Epoch 20: Train Loss = 0.050573, Val Loss = 0.160866\n",
            "Epoch 30: Train Loss = 0.206517, Val Loss = 0.135086\n",
            "Epoch 40: Train Loss = 0.009569, Val Loss = 0.117364\n",
            "Epoch 50: Train Loss = 0.004146, Val Loss = 0.102821\n",
            "Epoch 60: Train Loss = 0.022807, Val Loss = 0.090316\n",
            "Epoch 70: Train Loss = 0.030980, Val Loss = 0.079928\n",
            "Epoch 80: Train Loss = 0.221336, Val Loss = 0.071386\n",
            "Epoch 90: Train Loss = 0.336359, Val Loss = 0.064476\n",
            "Epoch 100: Train Loss = 0.191531, Val Loss = 0.059035\n",
            "Epoch 110: Train Loss = 0.006188, Val Loss = 0.054802\n",
            "Epoch 120: Train Loss = 0.138467, Val Loss = 0.051655\n",
            "Epoch 130: Train Loss = 0.009110, Val Loss = 0.049151\n",
            "Epoch 140: Train Loss = 0.002170, Val Loss = 0.047389\n",
            "Epoch 150: Train Loss = 0.001793, Val Loss = 0.046206\n",
            "Epoch 160: Train Loss = 0.035448, Val Loss = 0.044990\n",
            "Epoch 170: Train Loss = 0.005896, Val Loss = 0.044221\n",
            "Epoch 180: Train Loss = 0.059003, Val Loss = 0.043668\n",
            "Epoch 190: Train Loss = 0.001215, Val Loss = 0.043236\n",
            "Epoch 200: Train Loss = 0.036647, Val Loss = 0.042927\n",
            "Epoch 210: Train Loss = 0.040295, Val Loss = 0.042651\n",
            "Epoch 220: Train Loss = 0.000956, Val Loss = 0.042411\n",
            "Epoch 230: Train Loss = 0.463735, Val Loss = 0.042325\n",
            "Epoch 240: Train Loss = 0.001965, Val Loss = 0.042272\n",
            "Epoch 250: Train Loss = 0.038374, Val Loss = 0.042232\n",
            "Epoch 260: Train Loss = 0.139856, Val Loss = 0.042259\n",
            "Epoch 270: Train Loss = 0.022113, Val Loss = 0.042036\n",
            "Epoch 280: Train Loss = 0.080109, Val Loss = 0.042044\n",
            "Epoch 290: Train Loss = 0.011134, Val Loss = 0.042102\n",
            "Epoch 300: Train Loss = 0.157143, Val Loss = 0.042127\n",
            "Epoch 310: Train Loss = 0.050749, Val Loss = 0.042016\n",
            "Epoch 320: Train Loss = 0.022145, Val Loss = 0.041950\n",
            "Epoch 330: Train Loss = 0.002654, Val Loss = 0.041954\n",
            "Epoch 340: Train Loss = 0.001361, Val Loss = 0.041920\n",
            "Epoch 350: Train Loss = 0.000893, Val Loss = 0.041964\n",
            "Epoch 360: Train Loss = 0.014047, Val Loss = 0.042188\n",
            "Epoch 370: Train Loss = 0.007947, Val Loss = 0.041833\n",
            "Epoch 380: Train Loss = 0.027890, Val Loss = 0.041865\n",
            "Epoch 390: Train Loss = 0.013295, Val Loss = 0.041803\n",
            "Epoch 400: Train Loss = 0.001158, Val Loss = 0.041829\n",
            "Epoch 410: Train Loss = 0.002743, Val Loss = 0.041919\n",
            "Epoch 420: Train Loss = 0.075192, Val Loss = 0.041789\n",
            "Epoch 430: Train Loss = 0.022611, Val Loss = 0.041818\n",
            "Epoch 440: Train Loss = 0.082120, Val Loss = 0.041780\n",
            "Epoch 450: Train Loss = 0.001141, Val Loss = 0.041839\n",
            "Epoch 460: Train Loss = 0.001129, Val Loss = 0.041757\n",
            "Epoch 470: Train Loss = 0.021197, Val Loss = 0.041877\n",
            "Epoch 480: Train Loss = 0.004778, Val Loss = 0.041758\n",
            "Epoch 490: Train Loss = 0.009462, Val Loss = 0.041731\n",
            "Epoch 500: Train Loss = 0.032683, Val Loss = 0.041865\n",
            "Final Test MSE (5_95): 0.033830\n",
            "\n",
            "==== Running Split: 3_97 ====\n",
            "Split 3_97 — X (INPUT): 3, Y (OUTPUT): 98\n",
            "Train: torch.Size([1729, 3, 1]), Val: torch.Size([433, 3, 1]), Test: torch.Size([541, 3, 1])\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.439172, Val Loss = 0.793093\n",
            "Epoch 20: Train Loss = 0.192938, Val Loss = 0.439939\n",
            "Epoch 30: Train Loss = 1.080538, Val Loss = 0.306984\n",
            "Epoch 40: Train Loss = 0.115938, Val Loss = 0.248151\n",
            "Epoch 50: Train Loss = 0.477082, Val Loss = 0.212468\n",
            "Epoch 60: Train Loss = 0.008043, Val Loss = 0.183942\n",
            "Epoch 70: Train Loss = 0.071708, Val Loss = 0.158916\n",
            "Epoch 80: Train Loss = 0.188892, Val Loss = 0.136296\n",
            "Epoch 90: Train Loss = 0.073290, Val Loss = 0.116632\n",
            "Epoch 100: Train Loss = 0.126030, Val Loss = 0.100250\n",
            "Epoch 110: Train Loss = 0.095941, Val Loss = 0.086522\n",
            "Epoch 120: Train Loss = 0.051778, Val Loss = 0.075409\n",
            "Epoch 130: Train Loss = 0.005434, Val Loss = 0.066813\n",
            "Epoch 140: Train Loss = 0.027820, Val Loss = 0.060116\n",
            "Epoch 150: Train Loss = 0.002948, Val Loss = 0.055255\n",
            "Epoch 160: Train Loss = 0.006832, Val Loss = 0.051930\n",
            "Epoch 170: Train Loss = 0.008154, Val Loss = 0.049569\n",
            "Epoch 180: Train Loss = 0.002416, Val Loss = 0.047902\n",
            "Epoch 190: Train Loss = 0.001425, Val Loss = 0.046921\n",
            "Epoch 200: Train Loss = 0.098315, Val Loss = 0.046124\n",
            "Epoch 210: Train Loss = 0.067348, Val Loss = 0.045720\n",
            "Epoch 220: Train Loss = 0.026311, Val Loss = 0.045391\n",
            "Epoch 230: Train Loss = 0.013232, Val Loss = 0.045066\n",
            "Epoch 240: Train Loss = 0.003320, Val Loss = 0.044943\n",
            "Epoch 250: Train Loss = 0.001431, Val Loss = 0.044817\n",
            "Epoch 260: Train Loss = 0.017837, Val Loss = 0.044751\n",
            "Epoch 270: Train Loss = 0.017259, Val Loss = 0.044743\n",
            "Epoch 280: Train Loss = 0.003094, Val Loss = 0.044641\n",
            "Epoch 290: Train Loss = 0.081044, Val Loss = 0.044459\n",
            "Epoch 300: Train Loss = 0.031680, Val Loss = 0.044436\n",
            "Epoch 310: Train Loss = 0.013557, Val Loss = 0.044294\n",
            "Epoch 320: Train Loss = 0.030568, Val Loss = 0.044332\n",
            "Epoch 330: Train Loss = 0.065260, Val Loss = 0.044432\n",
            "Epoch 340: Train Loss = 0.032685, Val Loss = 0.044476\n",
            "Epoch 350: Train Loss = 0.008260, Val Loss = 0.044141\n",
            "Epoch 360: Train Loss = 0.006010, Val Loss = 0.044066\n",
            "Epoch 370: Train Loss = 0.000638, Val Loss = 0.044058\n",
            "Epoch 380: Train Loss = 0.038889, Val Loss = 0.044078\n",
            "Epoch 390: Train Loss = 0.003854, Val Loss = 0.043960\n",
            "Epoch 400: Train Loss = 0.135179, Val Loss = 0.043967\n",
            "Epoch 410: Train Loss = 0.029453, Val Loss = 0.044142\n",
            "Epoch 420: Train Loss = 0.033882, Val Loss = 0.044273\n",
            "Epoch 430: Train Loss = 0.017138, Val Loss = 0.043941\n",
            "Epoch 440: Train Loss = 0.019586, Val Loss = 0.043884\n",
            "Epoch 450: Train Loss = 0.327251, Val Loss = 0.043805\n",
            "Epoch 460: Train Loss = 0.008424, Val Loss = 0.043828\n",
            "Epoch 470: Train Loss = 0.003467, Val Loss = 0.043807\n",
            "Epoch 480: Train Loss = 0.001203, Val Loss = 0.043761\n",
            "Epoch 490: Train Loss = 0.017687, Val Loss = 0.043734\n",
            "Epoch 500: Train Loss = 0.004750, Val Loss = 0.043733\n",
            "Final Test MSE (3_97): 0.036144\n",
            "\n",
            "==== Running Split: 1_99 ====\n",
            "Split 1_99 — X (INPUT): 1, Y (OUTPUT): 100\n",
            "Train: torch.Size([1729, 1, 1]), Val: torch.Size([433, 1, 1]), Test: torch.Size([541, 1, 1])\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 4.360520, Val Loss = 3.574580\n",
            "Epoch 20: Train Loss = 1.296825, Val Loss = 2.763415\n",
            "Epoch 30: Train Loss = 2.912303, Val Loss = 2.117400\n",
            "Epoch 40: Train Loss = 1.180799, Val Loss = 1.610719\n",
            "Epoch 50: Train Loss = 1.013454, Val Loss = 1.217736\n",
            "Epoch 60: Train Loss = 1.868613, Val Loss = 0.917304\n",
            "Epoch 70: Train Loss = 0.705091, Val Loss = 0.690772\n",
            "Epoch 80: Train Loss = 0.501812, Val Loss = 0.523585\n",
            "Epoch 90: Train Loss = 0.269302, Val Loss = 0.401912\n",
            "Epoch 100: Train Loss = 0.077593, Val Loss = 0.315300\n",
            "Epoch 110: Train Loss = 0.062275, Val Loss = 0.253987\n",
            "Epoch 120: Train Loss = 0.519434, Val Loss = 0.213329\n",
            "Epoch 130: Train Loss = 0.086694, Val Loss = 0.186575\n",
            "Epoch 140: Train Loss = 0.024541, Val Loss = 0.170133\n",
            "Epoch 150: Train Loss = 0.017828, Val Loss = 0.159978\n",
            "Epoch 160: Train Loss = 0.095077, Val Loss = 0.154091\n",
            "Epoch 170: Train Loss = 0.057535, Val Loss = 0.151474\n",
            "Epoch 180: Train Loss = 0.019989, Val Loss = 0.150222\n",
            "Epoch 190: Train Loss = 0.214716, Val Loss = 0.149320\n",
            "Epoch 200: Train Loss = 0.004779, Val Loss = 0.148779\n",
            "Epoch 210: Train Loss = 0.013888, Val Loss = 0.148706\n",
            "Epoch 220: Train Loss = 0.510923, Val Loss = 0.148279\n",
            "Epoch 230: Train Loss = 0.161462, Val Loss = 0.148453\n",
            "Epoch 240: Train Loss = 0.002231, Val Loss = 0.148584\n",
            "Epoch 250: Train Loss = 0.041307, Val Loss = 0.148622\n",
            "Epoch 260: Train Loss = 0.006195, Val Loss = 0.148636\n",
            "Epoch 270: Train Loss = 0.259186, Val Loss = 0.148141\n",
            "Epoch 280: Train Loss = 0.187350, Val Loss = 0.148847\n",
            "Epoch 290: Train Loss = 0.240584, Val Loss = 0.149123\n",
            "Epoch 300: Train Loss = 0.006454, Val Loss = 0.148519\n",
            "Epoch 310: Train Loss = 0.036998, Val Loss = 0.148465\n",
            "Epoch 320: Train Loss = 0.169879, Val Loss = 0.148434\n",
            "Epoch 330: Train Loss = 0.350974, Val Loss = 0.148987\n",
            "Epoch 340: Train Loss = 0.083969, Val Loss = 0.148478\n",
            "Epoch 350: Train Loss = 0.136681, Val Loss = 0.148583\n",
            "Epoch 360: Train Loss = 0.187711, Val Loss = 0.148650\n",
            "Epoch 370: Train Loss = 0.048771, Val Loss = 0.148161\n",
            "Epoch 380: Train Loss = 0.240496, Val Loss = 0.148338\n",
            "Epoch 390: Train Loss = 0.045714, Val Loss = 0.148949\n",
            "Epoch 400: Train Loss = 0.359315, Val Loss = 0.148942\n",
            "Epoch 410: Train Loss = 0.212245, Val Loss = 0.148671\n",
            "Epoch 420: Train Loss = 0.067927, Val Loss = 0.148238\n",
            "Epoch 430: Train Loss = 0.121162, Val Loss = 0.148594\n",
            "Epoch 440: Train Loss = 0.496811, Val Loss = 0.148619\n",
            "Epoch 450: Train Loss = 0.030917, Val Loss = 0.147813\n",
            "Epoch 460: Train Loss = 0.032094, Val Loss = 0.148376\n",
            "Epoch 470: Train Loss = 0.333708, Val Loss = 0.148535\n",
            "Epoch 480: Train Loss = 0.225796, Val Loss = 0.148438\n",
            "Epoch 490: Train Loss = 0.138715, Val Loss = 0.148460\n",
            "Epoch 500: Train Loss = 0.020107, Val Loss = 0.148320\n",
            "Final Test MSE (1_99): 0.123073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**New BASIC-DSSM EXPERIMENT**"
      ],
      "metadata": {
        "id": "hzJO31L04ZgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Set seeds\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Output path\n",
        "drive_path = '/content/drive/MyDrive/DSSM-Figures'\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "# Config\n",
        "config = {\n",
        "    'hidden_dim': 101,\n",
        "    'epochs': 500,\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 0.001,\n",
        "    'model_name': 'DSSM_Basic'\n",
        "}\n",
        "\n",
        "# Store results\n",
        "DSSM_BASIC_MSE = pd.DataFrame(columns=['Split', 'Test_MSE'])\n",
        "\n",
        "# Loop through splits\n",
        "splits = [(80, 20), (60, 40), (50, 50), (40, 60), (20, 80), (10, 90), (5, 95), (3, 97), (1, 99)]\n",
        "for train_pct, test_pct in splits:\n",
        "    split_name = f\"{train_pct}_{test_pct}\"\n",
        "    print(f\"\\n==== Running Split: {split_name} ====\")\n",
        "\n",
        "    file_ids = df_output['file_id'].unique()\n",
        "    train_ids, test_ids = train_test_split(file_ids, test_size=0.2, random_state=42)\n",
        "\n",
        "    df_train = df_output[df_output['file_id'].isin(train_ids)]\n",
        "    df_test = df_output[df_output['file_id'].isin(test_ids)]\n",
        "\n",
        "    train_timestep = int(train_pct / 100 * 101)\n",
        "    print(f\"X (INPUT): {train_timestep}, Y (OUTPUT): {101 - train_timestep}\")\n",
        "\n",
        "    X_train = df_train.pivot(index='file_id', columns='timestep', values='CO2').values[:, :train_timestep]\n",
        "    Y_train = df_train.pivot(index='file_id', columns='timestep', values='CO2').values[:, train_timestep:]\n",
        "    X_test = df_test.pivot(index='file_id', columns='timestep', values='CO2').values[:, :train_timestep]\n",
        "    Y_test = df_test.pivot(index='file_id', columns='timestep', values='CO2').values[:, train_timestep:]\n",
        "\n",
        "    static_train = merged_df[merged_df['file_id'].isin(train_ids)].drop(columns=['file_id', 'cluster']).values\n",
        "    static_test = merged_df[merged_df['file_id'].isin(test_ids)].drop(columns=['file_id', 'cluster']).values\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
        "    static_train_tensor = torch.tensor(static_train, dtype=torch.float32)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
        "    static_test_tensor = torch.tensor(static_test, dtype=torch.float32)\n",
        "\n",
        "    print(f\"Train X shape: {X_train_tensor.shape}, Y: {Y_train_tensor.shape}, Static: {static_train_tensor.shape}\")\n",
        "    print(f\"Test  X shape: {X_test_tensor.shape}, Y: {Y_test_tensor.shape}, Static: {static_test_tensor.shape}\")\n",
        "\n",
        "    train_dataset = TensorDataset(X_train_tensor, static_train_tensor, Y_train_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, static_test_tensor, Y_test_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'])\n",
        "\n",
        "    model = DeepStateSpaceModel(\n",
        "        input_dim=1,\n",
        "        static_dim=static_train.shape[1],\n",
        "        hidden_dim=config['hidden_dim'],\n",
        "        output_dim=Y_train.shape[1]\n",
        "    )\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    best_model_path = os.path.join(drive_path, f\"{config['model_name']}_best_model_{split_name}.pt\")\n",
        "\n",
        "    print(\"Training started...\")\n",
        "    for epoch in range(config['epochs']):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        for X_batch, static_batch, Y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X_batch, static_batch)\n",
        "            loss = criterion(preds, Y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_loss /= len(train_loader)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}: Train Loss = {epoch_loss:.6f}\")\n",
        "\n",
        "        if epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"New best model saved at epoch {epoch+1} with loss {epoch_loss:.6f}\")\n",
        "\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_mse = 0.0\n",
        "        total_samples = 0\n",
        "        for X_batch, static_batch, Y_batch in test_loader:\n",
        "            preds = model(X_batch, static_batch)\n",
        "            batch_mse = criterion(preds, Y_batch).item()\n",
        "            total_mse += batch_mse * X_batch.size(0)\n",
        "            total_samples += X_batch.size(0)\n",
        "\n",
        "    avg_mse = total_mse / total_samples\n",
        "    print(f\"Final Test MSE for DSSM-Basic ({split_name}): {avg_mse:.6f}\")\n",
        "    DSSM_BASIC_MSE.loc[len(DSSM_BASIC_MSE)] = [split_name, avg_mse]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8da-rj5sFqqX",
        "outputId": "6b6df362-4606-448a-8397-107de658d159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Running Split: 80_20 ====\n",
            "X (INPUT): 80, Y (OUTPUT): 21\n",
            "Train X shape: torch.Size([2162, 80]), Y: torch.Size([2162, 21]), Static: torch.Size([2162, 14])\n",
            "Test  X shape: torch.Size([541, 80]), Y: torch.Size([541, 21]), Static: torch.Size([541, 14])\n",
            "Training started...\n",
            "New best model saved at epoch 1 with loss 2.632255\n",
            "New best model saved at epoch 2 with loss 0.206199\n",
            "New best model saved at epoch 3 with loss 0.158680\n",
            "New best model saved at epoch 4 with loss 0.153687\n",
            "New best model saved at epoch 5 with loss 0.149631\n",
            "New best model saved at epoch 6 with loss 0.147199\n",
            "New best model saved at epoch 7 with loss 0.142990\n",
            "New best model saved at epoch 8 with loss 0.138572\n",
            "New best model saved at epoch 9 with loss 0.132981\n",
            "Epoch 10: Train Loss = 0.128052\n",
            "New best model saved at epoch 10 with loss 0.128052\n",
            "New best model saved at epoch 11 with loss 0.124268\n",
            "New best model saved at epoch 12 with loss 0.120602\n",
            "New best model saved at epoch 13 with loss 0.114972\n",
            "New best model saved at epoch 14 with loss 0.112965\n",
            "New best model saved at epoch 15 with loss 0.104846\n",
            "New best model saved at epoch 16 with loss 0.100228\n",
            "New best model saved at epoch 17 with loss 0.094520\n",
            "New best model saved at epoch 18 with loss 0.089652\n",
            "New best model saved at epoch 19 with loss 0.084782\n",
            "Epoch 20: Train Loss = 0.075948\n",
            "New best model saved at epoch 20 with loss 0.075948\n",
            "New best model saved at epoch 21 with loss 0.073284\n",
            "New best model saved at epoch 22 with loss 0.065013\n",
            "New best model saved at epoch 23 with loss 0.064561\n",
            "New best model saved at epoch 24 with loss 0.062078\n",
            "New best model saved at epoch 25 with loss 0.054606\n",
            "New best model saved at epoch 26 with loss 0.052491\n",
            "New best model saved at epoch 27 with loss 0.051079\n",
            "New best model saved at epoch 28 with loss 0.049275\n",
            "New best model saved at epoch 29 with loss 0.046622\n",
            "Epoch 30: Train Loss = 0.044285\n",
            "New best model saved at epoch 30 with loss 0.044285\n",
            "New best model saved at epoch 31 with loss 0.041763\n",
            "New best model saved at epoch 32 with loss 0.039092\n",
            "New best model saved at epoch 33 with loss 0.035174\n",
            "New best model saved at epoch 34 with loss 0.033939\n",
            "New best model saved at epoch 35 with loss 0.030861\n",
            "New best model saved at epoch 36 with loss 0.028406\n",
            "New best model saved at epoch 38 with loss 0.025129\n",
            "New best model saved at epoch 39 with loss 0.022249\n",
            "Epoch 40: Train Loss = 0.019362\n",
            "New best model saved at epoch 40 with loss 0.019362\n",
            "New best model saved at epoch 41 with loss 0.019132\n",
            "New best model saved at epoch 42 with loss 0.016328\n",
            "New best model saved at epoch 43 with loss 0.015143\n",
            "New best model saved at epoch 45 with loss 0.012964\n",
            "New best model saved at epoch 48 with loss 0.012116\n",
            "New best model saved at epoch 49 with loss 0.011371\n",
            "Epoch 50: Train Loss = 0.009464\n",
            "New best model saved at epoch 50 with loss 0.009464\n",
            "New best model saved at epoch 51 with loss 0.009004\n",
            "New best model saved at epoch 54 with loss 0.006916\n",
            "New best model saved at epoch 55 with loss 0.006053\n",
            "New best model saved at epoch 57 with loss 0.005671\n",
            "New best model saved at epoch 58 with loss 0.005011\n",
            "Epoch 60: Train Loss = 0.009172\n",
            "New best model saved at epoch 62 with loss 0.004998\n",
            "New best model saved at epoch 63 with loss 0.004383\n",
            "Epoch 70: Train Loss = 0.004356\n",
            "New best model saved at epoch 70 with loss 0.004356\n",
            "New best model saved at epoch 73 with loss 0.003409\n",
            "Epoch 80: Train Loss = 0.003424\n",
            "New best model saved at epoch 82 with loss 0.003333\n",
            "New best model saved at epoch 88 with loss 0.002921\n",
            "New best model saved at epoch 89 with loss 0.002751\n",
            "Epoch 90: Train Loss = 0.003720\n",
            "Epoch 100: Train Loss = 0.003462\n",
            "New best model saved at epoch 104 with loss 0.002646\n",
            "New best model saved at epoch 105 with loss 0.002167\n",
            "Epoch 110: Train Loss = 0.003115\n",
            "Epoch 120: Train Loss = 0.003362\n",
            "New best model saved at epoch 128 with loss 0.001675\n",
            "Epoch 130: Train Loss = 0.002443\n",
            "Epoch 140: Train Loss = 0.001819\n",
            "Epoch 150: Train Loss = 0.002568\n",
            "Epoch 160: Train Loss = 0.001824\n",
            "Epoch 170: Train Loss = 0.002030\n",
            "New best model saved at epoch 172 with loss 0.001618\n",
            "Epoch 180: Train Loss = 0.002329\n",
            "Epoch 190: Train Loss = 0.001365\n",
            "New best model saved at epoch 190 with loss 0.001365\n",
            "Epoch 200: Train Loss = 0.003765\n",
            "Epoch 210: Train Loss = 0.002043\n",
            "Epoch 220: Train Loss = 0.001549\n",
            "New best model saved at epoch 224 with loss 0.001301\n",
            "Epoch 230: Train Loss = 0.002014\n",
            "New best model saved at epoch 234 with loss 0.001069\n",
            "Epoch 240: Train Loss = 0.001607\n",
            "Epoch 250: Train Loss = 0.001684\n",
            "Epoch 260: Train Loss = 0.001840\n",
            "New best model saved at epoch 263 with loss 0.000996\n",
            "New best model saved at epoch 265 with loss 0.000971\n",
            "Epoch 270: Train Loss = 0.001940\n",
            "Epoch 280: Train Loss = 0.002718\n",
            "Epoch 290: Train Loss = 0.001088\n",
            "New best model saved at epoch 291 with loss 0.000870\n",
            "New best model saved at epoch 292 with loss 0.000774\n",
            "Epoch 300: Train Loss = 0.001164\n",
            "Epoch 310: Train Loss = 0.001674\n",
            "Epoch 320: Train Loss = 0.002260\n",
            "Epoch 330: Train Loss = 0.001327\n",
            "Epoch 340: Train Loss = 0.002011\n",
            "Epoch 350: Train Loss = 0.001094\n",
            "Epoch 360: Train Loss = 0.001380\n",
            "Epoch 370: Train Loss = 0.001896\n",
            "Epoch 380: Train Loss = 0.001758\n",
            "Epoch 390: Train Loss = 0.001388\n",
            "New best model saved at epoch 397 with loss 0.000642\n",
            "Epoch 400: Train Loss = 0.002104\n",
            "Epoch 410: Train Loss = 0.003059\n",
            "Epoch 420: Train Loss = 0.000891\n",
            "New best model saved at epoch 421 with loss 0.000584\n",
            "Epoch 430: Train Loss = 0.000931\n",
            "Epoch 440: Train Loss = 0.000847\n",
            "Epoch 450: Train Loss = 0.001343\n",
            "Epoch 460: Train Loss = 0.001478\n",
            "Epoch 470: Train Loss = 0.000631\n",
            "Epoch 480: Train Loss = 0.000814\n",
            "Epoch 490: Train Loss = 0.000964\n",
            "Epoch 500: Train Loss = 0.001971\n",
            "Final Test MSE for DSSM-Basic (80_20): 0.001168\n",
            "\n",
            "==== Running Split: 60_40 ====\n",
            "X (INPUT): 60, Y (OUTPUT): 41\n",
            "Train X shape: torch.Size([2162, 60]), Y: torch.Size([2162, 41]), Static: torch.Size([2162, 14])\n",
            "Test  X shape: torch.Size([541, 60]), Y: torch.Size([541, 41]), Static: torch.Size([541, 14])\n",
            "Training started...\n",
            "New best model saved at epoch 1 with loss 2.403551\n",
            "New best model saved at epoch 2 with loss 0.181001\n",
            "New best model saved at epoch 3 with loss 0.152587\n",
            "New best model saved at epoch 4 with loss 0.148787\n",
            "New best model saved at epoch 5 with loss 0.143975\n",
            "New best model saved at epoch 6 with loss 0.139555\n",
            "New best model saved at epoch 7 with loss 0.132624\n",
            "New best model saved at epoch 8 with loss 0.125155\n",
            "New best model saved at epoch 9 with loss 0.118414\n",
            "Epoch 10: Train Loss = 0.114817\n",
            "New best model saved at epoch 10 with loss 0.114817\n",
            "New best model saved at epoch 11 with loss 0.104800\n",
            "New best model saved at epoch 12 with loss 0.099648\n",
            "New best model saved at epoch 13 with loss 0.093981\n",
            "New best model saved at epoch 14 with loss 0.090199\n",
            "New best model saved at epoch 15 with loss 0.080533\n",
            "New best model saved at epoch 16 with loss 0.073549\n",
            "New best model saved at epoch 17 with loss 0.065833\n",
            "New best model saved at epoch 18 with loss 0.061513\n",
            "New best model saved at epoch 19 with loss 0.054452\n",
            "Epoch 20: Train Loss = 0.053398\n",
            "New best model saved at epoch 20 with loss 0.053398\n",
            "New best model saved at epoch 21 with loss 0.047900\n",
            "New best model saved at epoch 22 with loss 0.041270\n",
            "New best model saved at epoch 24 with loss 0.037048\n",
            "New best model saved at epoch 25 with loss 0.030915\n",
            "New best model saved at epoch 26 with loss 0.028537\n",
            "New best model saved at epoch 27 with loss 0.027924\n",
            "New best model saved at epoch 29 with loss 0.025135\n",
            "Epoch 30: Train Loss = 0.021040\n",
            "New best model saved at epoch 30 with loss 0.021040\n",
            "New best model saved at epoch 31 with loss 0.019781\n",
            "New best model saved at epoch 32 with loss 0.017099\n",
            "New best model saved at epoch 33 with loss 0.016049\n",
            "New best model saved at epoch 36 with loss 0.013881\n",
            "New best model saved at epoch 37 with loss 0.011810\n",
            "New best model saved at epoch 38 with loss 0.011699\n",
            "Epoch 40: Train Loss = 0.010433\n",
            "New best model saved at epoch 40 with loss 0.010433\n",
            "New best model saved at epoch 44 with loss 0.009841\n",
            "New best model saved at epoch 45 with loss 0.009485\n",
            "New best model saved at epoch 47 with loss 0.008417\n",
            "Epoch 50: Train Loss = 0.007280\n",
            "New best model saved at epoch 50 with loss 0.007280\n",
            "New best model saved at epoch 52 with loss 0.006525\n",
            "New best model saved at epoch 56 with loss 0.006508\n",
            "New best model saved at epoch 58 with loss 0.006359\n",
            "New best model saved at epoch 59 with loss 0.005299\n",
            "Epoch 60: Train Loss = 0.005412\n",
            "New best model saved at epoch 62 with loss 0.005142\n",
            "New best model saved at epoch 68 with loss 0.004841\n",
            "New best model saved at epoch 69 with loss 0.004786\n",
            "Epoch 70: Train Loss = 0.004387\n",
            "New best model saved at epoch 70 with loss 0.004387\n",
            "New best model saved at epoch 75 with loss 0.004374\n",
            "New best model saved at epoch 76 with loss 0.003669\n",
            "Epoch 80: Train Loss = 0.004883\n",
            "Epoch 90: Train Loss = 0.004457\n",
            "New best model saved at epoch 94 with loss 0.003658\n",
            "New best model saved at epoch 98 with loss 0.003497\n",
            "Epoch 100: Train Loss = 0.003210\n",
            "New best model saved at epoch 100 with loss 0.003210\n",
            "Epoch 110: Train Loss = 0.005434\n",
            "New best model saved at epoch 114 with loss 0.003079\n",
            "New best model saved at epoch 115 with loss 0.002783\n",
            "Epoch 120: Train Loss = 0.003902\n",
            "Epoch 130: Train Loss = 0.003765\n",
            "New best model saved at epoch 134 with loss 0.002631\n",
            "Epoch 140: Train Loss = 0.002875\n",
            "Epoch 150: Train Loss = 0.002535\n",
            "New best model saved at epoch 150 with loss 0.002535\n",
            "New best model saved at epoch 156 with loss 0.002360\n",
            "Epoch 160: Train Loss = 0.003756\n",
            "New best model saved at epoch 166 with loss 0.002320\n",
            "Epoch 170: Train Loss = 0.002650\n",
            "Epoch 180: Train Loss = 0.002840\n",
            "New best model saved at epoch 181 with loss 0.002261\n",
            "New best model saved at epoch 182 with loss 0.002223\n",
            "New best model saved at epoch 183 with loss 0.002180\n",
            "Epoch 190: Train Loss = 0.002953\n",
            "New best model saved at epoch 197 with loss 0.001900\n",
            "Epoch 200: Train Loss = 0.003032\n",
            "Epoch 210: Train Loss = 0.004853\n",
            "Epoch 220: Train Loss = 0.002727\n",
            "Epoch 230: Train Loss = 0.003061\n",
            "Epoch 240: Train Loss = 0.003283\n",
            "New best model saved at epoch 243 with loss 0.001863\n",
            "New best model saved at epoch 244 with loss 0.001842\n",
            "Epoch 250: Train Loss = 0.001819\n",
            "New best model saved at epoch 250 with loss 0.001819\n",
            "Epoch 260: Train Loss = 0.003091\n",
            "Epoch 270: Train Loss = 0.002250\n",
            "New best model saved at epoch 274 with loss 0.001704\n",
            "Epoch 280: Train Loss = 0.002123\n",
            "Epoch 290: Train Loss = 0.002027\n",
            "New best model saved at epoch 295 with loss 0.001560\n",
            "Epoch 300: Train Loss = 0.002290\n",
            "Epoch 310: Train Loss = 0.001528\n",
            "New best model saved at epoch 310 with loss 0.001528\n",
            "Epoch 320: Train Loss = 0.002195\n",
            "Epoch 330: Train Loss = 0.001920\n",
            "New best model saved at epoch 333 with loss 0.001391\n",
            "Epoch 340: Train Loss = 0.002202\n",
            "Epoch 350: Train Loss = 0.001788\n",
            "Epoch 360: Train Loss = 0.001560\n",
            "Epoch 370: Train Loss = 0.002488\n",
            "Epoch 380: Train Loss = 0.002104\n",
            "Epoch 390: Train Loss = 0.001955\n",
            "New best model saved at epoch 399 with loss 0.001324\n",
            "Epoch 400: Train Loss = 0.001474\n",
            "Epoch 410: Train Loss = 0.001736\n",
            "Epoch 420: Train Loss = 0.002205\n",
            "Epoch 430: Train Loss = 0.001692\n",
            "New best model saved at epoch 435 with loss 0.001288\n",
            "Epoch 440: Train Loss = 0.001291\n",
            "New best model saved at epoch 442 with loss 0.001172\n",
            "Epoch 450: Train Loss = 0.002230\n",
            "Epoch 460: Train Loss = 0.001272\n",
            "Epoch 470: Train Loss = 0.001800\n",
            "Epoch 480: Train Loss = 0.001785\n",
            "Epoch 490: Train Loss = 0.001620\n",
            "Epoch 500: Train Loss = 0.001449\n",
            "Final Test MSE for DSSM-Basic (60_40): 0.002288\n",
            "\n",
            "==== Running Split: 50_50 ====\n",
            "X (INPUT): 50, Y (OUTPUT): 51\n",
            "Train X shape: torch.Size([2162, 50]), Y: torch.Size([2162, 51]), Static: torch.Size([2162, 14])\n",
            "Test  X shape: torch.Size([541, 50]), Y: torch.Size([541, 51]), Static: torch.Size([541, 14])\n",
            "Training started...\n",
            "New best model saved at epoch 1 with loss 2.627712\n",
            "New best model saved at epoch 2 with loss 0.200432\n",
            "New best model saved at epoch 3 with loss 0.154606\n",
            "New best model saved at epoch 4 with loss 0.146556\n",
            "New best model saved at epoch 5 with loss 0.141826\n",
            "New best model saved at epoch 6 with loss 0.140089\n",
            "New best model saved at epoch 7 with loss 0.133748\n",
            "New best model saved at epoch 8 with loss 0.130288\n",
            "New best model saved at epoch 9 with loss 0.124350\n",
            "Epoch 10: Train Loss = 0.119619\n",
            "New best model saved at epoch 10 with loss 0.119619\n",
            "New best model saved at epoch 11 with loss 0.114431\n",
            "New best model saved at epoch 12 with loss 0.108920\n",
            "New best model saved at epoch 13 with loss 0.103426\n",
            "New best model saved at epoch 14 with loss 0.094922\n",
            "New best model saved at epoch 15 with loss 0.083242\n",
            "New best model saved at epoch 16 with loss 0.080795\n",
            "New best model saved at epoch 17 with loss 0.070422\n",
            "New best model saved at epoch 19 with loss 0.063196\n",
            "Epoch 20: Train Loss = 0.059882\n",
            "New best model saved at epoch 20 with loss 0.059882\n",
            "New best model saved at epoch 21 with loss 0.052974\n",
            "New best model saved at epoch 22 with loss 0.050205\n",
            "New best model saved at epoch 23 with loss 0.041699\n",
            "New best model saved at epoch 26 with loss 0.036342\n",
            "New best model saved at epoch 27 with loss 0.034060\n",
            "New best model saved at epoch 28 with loss 0.030811\n",
            "New best model saved at epoch 29 with loss 0.029368\n",
            "Epoch 30: Train Loss = 0.030568\n",
            "New best model saved at epoch 31 with loss 0.028682\n",
            "New best model saved at epoch 33 with loss 0.026117\n",
            "New best model saved at epoch 34 with loss 0.023001\n",
            "New best model saved at epoch 36 with loss 0.021890\n",
            "New best model saved at epoch 38 with loss 0.021101\n",
            "New best model saved at epoch 39 with loss 0.019479\n",
            "Epoch 40: Train Loss = 0.017405\n",
            "New best model saved at epoch 40 with loss 0.017405\n",
            "New best model saved at epoch 41 with loss 0.015525\n",
            "New best model saved at epoch 42 with loss 0.013791\n",
            "New best model saved at epoch 44 with loss 0.013573\n",
            "New best model saved at epoch 49 with loss 0.012327\n",
            "Epoch 50: Train Loss = 0.011880\n",
            "New best model saved at epoch 50 with loss 0.011880\n",
            "New best model saved at epoch 51 with loss 0.010460\n",
            "New best model saved at epoch 52 with loss 0.010009\n",
            "New best model saved at epoch 56 with loss 0.009680\n",
            "New best model saved at epoch 57 with loss 0.009262\n",
            "New best model saved at epoch 58 with loss 0.008793\n",
            "New best model saved at epoch 59 with loss 0.008728\n",
            "Epoch 60: Train Loss = 0.010073\n",
            "New best model saved at epoch 68 with loss 0.007836\n",
            "Epoch 70: Train Loss = 0.007237\n",
            "New best model saved at epoch 70 with loss 0.007237\n",
            "New best model saved at epoch 74 with loss 0.007235\n",
            "New best model saved at epoch 75 with loss 0.006869\n",
            "New best model saved at epoch 77 with loss 0.006359\n",
            "Epoch 80: Train Loss = 0.005759\n",
            "New best model saved at epoch 80 with loss 0.005759\n",
            "Epoch 90: Train Loss = 0.008364\n",
            "New best model saved at epoch 93 with loss 0.005345\n",
            "Epoch 100: Train Loss = 0.005451\n",
            "New best model saved at epoch 104 with loss 0.004938\n",
            "New best model saved at epoch 105 with loss 0.004734\n",
            "Epoch 110: Train Loss = 0.005491\n",
            "Epoch 120: Train Loss = 0.005508\n",
            "New best model saved at epoch 121 with loss 0.004727\n",
            "New best model saved at epoch 127 with loss 0.004496\n",
            "Epoch 130: Train Loss = 0.004398\n",
            "New best model saved at epoch 130 with loss 0.004398\n",
            "New best model saved at epoch 135 with loss 0.004291\n",
            "New best model saved at epoch 137 with loss 0.004170\n",
            "Epoch 140: Train Loss = 0.004912\n",
            "New best model saved at epoch 144 with loss 0.004013\n",
            "New best model saved at epoch 145 with loss 0.003915\n",
            "New best model saved at epoch 146 with loss 0.003846\n",
            "Epoch 150: Train Loss = 0.005232\n",
            "New best model saved at epoch 155 with loss 0.003790\n",
            "Epoch 160: Train Loss = 0.004331\n",
            "New best model saved at epoch 165 with loss 0.003741\n",
            "New best model saved at epoch 168 with loss 0.003496\n",
            "Epoch 170: Train Loss = 0.004646\n",
            "New best model saved at epoch 175 with loss 0.003437\n",
            "Epoch 180: Train Loss = 0.003223\n",
            "New best model saved at epoch 180 with loss 0.003223\n",
            "New best model saved at epoch 181 with loss 0.003076\n",
            "New best model saved at epoch 188 with loss 0.002977\n",
            "Epoch 190: Train Loss = 0.003235\n",
            "New best model saved at epoch 191 with loss 0.002835\n",
            "Epoch 200: Train Loss = 0.003522\n",
            "Epoch 210: Train Loss = 0.005468\n",
            "Epoch 220: Train Loss = 0.004303\n",
            "New best model saved at epoch 222 with loss 0.002719\n",
            "New best model saved at epoch 223 with loss 0.002700\n",
            "New best model saved at epoch 224 with loss 0.002512\n",
            "Epoch 230: Train Loss = 0.003076\n",
            "Epoch 240: Train Loss = 0.003258\n",
            "Epoch 250: Train Loss = 0.003860\n",
            "Epoch 260: Train Loss = 0.003355\n",
            "New best model saved at epoch 269 with loss 0.002460\n",
            "Epoch 270: Train Loss = 0.002907\n",
            "New best model saved at epoch 278 with loss 0.002441\n",
            "Epoch 280: Train Loss = 0.003187\n",
            "Epoch 290: Train Loss = 0.003610\n",
            "New best model saved at epoch 298 with loss 0.002330\n",
            "Epoch 300: Train Loss = 0.003082\n",
            "New best model saved at epoch 308 with loss 0.002158\n",
            "Epoch 310: Train Loss = 0.002227\n",
            "New best model saved at epoch 313 with loss 0.002029\n",
            "Epoch 320: Train Loss = 0.002460\n",
            "Epoch 330: Train Loss = 0.002964\n",
            "Epoch 340: Train Loss = 0.003576\n",
            "Epoch 350: Train Loss = 0.002281\n",
            "Epoch 360: Train Loss = 0.003210\n",
            "Epoch 370: Train Loss = 0.002947\n",
            "New best model saved at epoch 373 with loss 0.001961\n",
            "Epoch 380: Train Loss = 0.002135\n",
            "Epoch 390: Train Loss = 0.003844\n",
            "Epoch 400: Train Loss = 0.002516\n",
            "New best model saved at epoch 404 with loss 0.001926\n",
            "New best model saved at epoch 409 with loss 0.001820\n",
            "Epoch 410: Train Loss = 0.001815\n",
            "New best model saved at epoch 410 with loss 0.001815\n",
            "New best model saved at epoch 411 with loss 0.001794\n",
            "New best model saved at epoch 412 with loss 0.001745\n",
            "Epoch 420: Train Loss = 0.002811\n",
            "Epoch 430: Train Loss = 0.002616\n",
            "New best model saved at epoch 434 with loss 0.001725\n",
            "New best model saved at epoch 438 with loss 0.001725\n",
            "Epoch 440: Train Loss = 0.001972\n",
            "New best model saved at epoch 449 with loss 0.001690\n",
            "Epoch 450: Train Loss = 0.002114\n",
            "Epoch 460: Train Loss = 0.001828\n",
            "Epoch 470: Train Loss = 0.001803\n",
            "Epoch 480: Train Loss = 0.002600\n",
            "New best model saved at epoch 484 with loss 0.001652\n",
            "Epoch 490: Train Loss = 0.002203\n",
            "New best model saved at epoch 494 with loss 0.001544\n",
            "Epoch 500: Train Loss = 0.001840\n",
            "Final Test MSE for DSSM-Basic (50_50): 0.003769\n",
            "\n",
            "==== Running Split: 40_60 ====\n",
            "X (INPUT): 40, Y (OUTPUT): 61\n",
            "Train X shape: torch.Size([2162, 40]), Y: torch.Size([2162, 61]), Static: torch.Size([2162, 14])\n",
            "Test  X shape: torch.Size([541, 40]), Y: torch.Size([541, 61]), Static: torch.Size([541, 14])\n",
            "Training started...\n",
            "New best model saved at epoch 1 with loss 2.658046\n",
            "New best model saved at epoch 2 with loss 0.216091\n",
            "New best model saved at epoch 3 with loss 0.158086\n",
            "New best model saved at epoch 4 with loss 0.150897\n",
            "New best model saved at epoch 5 with loss 0.146636\n",
            "New best model saved at epoch 6 with loss 0.143093\n",
            "New best model saved at epoch 7 with loss 0.140033\n",
            "New best model saved at epoch 8 with loss 0.135839\n",
            "New best model saved at epoch 9 with loss 0.131145\n",
            "Epoch 10: Train Loss = 0.128019\n",
            "New best model saved at epoch 10 with loss 0.128019\n",
            "New best model saved at epoch 11 with loss 0.125142\n",
            "New best model saved at epoch 12 with loss 0.116904\n",
            "New best model saved at epoch 13 with loss 0.107750\n",
            "New best model saved at epoch 14 with loss 0.102849\n",
            "New best model saved at epoch 15 with loss 0.090621\n",
            "New best model saved at epoch 16 with loss 0.085500\n",
            "New best model saved at epoch 17 with loss 0.077993\n",
            "New best model saved at epoch 18 with loss 0.067165\n",
            "New best model saved at epoch 19 with loss 0.059297\n",
            "Epoch 20: Train Loss = 0.058717\n",
            "New best model saved at epoch 20 with loss 0.058717\n",
            "New best model saved at epoch 21 with loss 0.054039\n",
            "New best model saved at epoch 22 with loss 0.050636\n",
            "New best model saved at epoch 23 with loss 0.046135\n",
            "New best model saved at epoch 24 with loss 0.042226\n",
            "New best model saved at epoch 25 with loss 0.039089\n",
            "New best model saved at epoch 26 with loss 0.033904\n",
            "New best model saved at epoch 28 with loss 0.030901\n",
            "New best model saved at epoch 29 with loss 0.030672\n",
            "Epoch 30: Train Loss = 0.024555\n",
            "New best model saved at epoch 30 with loss 0.024555\n",
            "New best model saved at epoch 31 with loss 0.022978\n",
            "New best model saved at epoch 34 with loss 0.021022\n",
            "New best model saved at epoch 35 with loss 0.020952\n",
            "New best model saved at epoch 36 with loss 0.020166\n",
            "New best model saved at epoch 37 with loss 0.019187\n",
            "New best model saved at epoch 38 with loss 0.018784\n",
            "New best model saved at epoch 39 with loss 0.017415\n",
            "Epoch 40: Train Loss = 0.017920\n",
            "New best model saved at epoch 43 with loss 0.016478\n",
            "New best model saved at epoch 46 with loss 0.015080\n",
            "New best model saved at epoch 48 with loss 0.014415\n",
            "New best model saved at epoch 49 with loss 0.014100\n",
            "Epoch 50: Train Loss = 0.013947\n",
            "New best model saved at epoch 50 with loss 0.013947\n",
            "New best model saved at epoch 51 with loss 0.013942\n",
            "New best model saved at epoch 52 with loss 0.012955\n",
            "New best model saved at epoch 53 with loss 0.012138\n",
            "New best model saved at epoch 54 with loss 0.011820\n",
            "New best model saved at epoch 55 with loss 0.011104\n",
            "Epoch 60: Train Loss = 0.012975\n",
            "New best model saved at epoch 63 with loss 0.010978\n",
            "New best model saved at epoch 64 with loss 0.010211\n",
            "New best model saved at epoch 69 with loss 0.008928\n",
            "Epoch 70: Train Loss = 0.011159\n",
            "New best model saved at epoch 73 with loss 0.008751\n",
            "Epoch 80: Train Loss = 0.009506\n",
            "New best model saved at epoch 85 with loss 0.008333\n",
            "New best model saved at epoch 87 with loss 0.007680\n",
            "Epoch 90: Train Loss = 0.009710\n",
            "Epoch 100: Train Loss = 0.007428\n",
            "New best model saved at epoch 100 with loss 0.007428\n",
            "Epoch 110: Train Loss = 0.007710\n",
            "New best model saved at epoch 113 with loss 0.006983\n",
            "New best model saved at epoch 117 with loss 0.006668\n",
            "New best model saved at epoch 118 with loss 0.006502\n",
            "Epoch 120: Train Loss = 0.007393\n",
            "New best model saved at epoch 123 with loss 0.006183\n",
            "Epoch 130: Train Loss = 0.007190\n",
            "New best model saved at epoch 135 with loss 0.005966\n",
            "Epoch 140: Train Loss = 0.007127\n",
            "Epoch 150: Train Loss = 0.007470\n",
            "New best model saved at epoch 154 with loss 0.005673\n",
            "New best model saved at epoch 155 with loss 0.005572\n",
            "Epoch 160: Train Loss = 0.006259\n",
            "New best model saved at epoch 164 with loss 0.005469\n",
            "New best model saved at epoch 167 with loss 0.005100\n",
            "Epoch 170: Train Loss = 0.005629\n",
            "New best model saved at epoch 172 with loss 0.004820\n",
            "New best model saved at epoch 173 with loss 0.004588\n",
            "Epoch 180: Train Loss = 0.006045\n",
            "Epoch 190: Train Loss = 0.004796\n",
            "New best model saved at epoch 199 with loss 0.004411\n",
            "Epoch 200: Train Loss = 0.004586\n",
            "New best model saved at epoch 201 with loss 0.004331\n",
            "New best model saved at epoch 203 with loss 0.004041\n",
            "Epoch 210: Train Loss = 0.004872\n",
            "Epoch 220: Train Loss = 0.004387\n",
            "New best model saved at epoch 221 with loss 0.003988\n",
            "New best model saved at epoch 223 with loss 0.003743\n",
            "New best model saved at epoch 224 with loss 0.003741\n",
            "Epoch 230: Train Loss = 0.004687\n",
            "Epoch 240: Train Loss = 0.003834\n",
            "Epoch 250: Train Loss = 0.003831\n",
            "New best model saved at epoch 251 with loss 0.003388\n",
            "Epoch 260: Train Loss = 0.007905\n",
            "New best model saved at epoch 264 with loss 0.003252\n",
            "New best model saved at epoch 266 with loss 0.003184\n",
            "Epoch 270: Train Loss = 0.003509\n",
            "Epoch 280: Train Loss = 0.003988\n",
            "Epoch 290: Train Loss = 0.003968\n",
            "Epoch 300: Train Loss = 0.003523\n",
            "Epoch 310: Train Loss = 0.003393\n",
            "Epoch 320: Train Loss = 0.003530\n",
            "New best model saved at epoch 325 with loss 0.003167\n",
            "New best model saved at epoch 329 with loss 0.003166\n",
            "Epoch 330: Train Loss = 0.003686\n",
            "Epoch 340: Train Loss = 0.003395\n",
            "Epoch 350: Train Loss = 0.004162\n",
            "New best model saved at epoch 351 with loss 0.003047\n",
            "New best model saved at epoch 358 with loss 0.002927\n",
            "Epoch 360: Train Loss = 0.003364\n",
            "New best model saved at epoch 362 with loss 0.002735\n",
            "Epoch 370: Train Loss = 0.003359\n",
            "New best model saved at epoch 378 with loss 0.002641\n",
            "New best model saved at epoch 379 with loss 0.002496\n",
            "Epoch 380: Train Loss = 0.002435\n",
            "New best model saved at epoch 380 with loss 0.002435\n",
            "Epoch 390: Train Loss = 0.003659\n",
            "Epoch 400: Train Loss = 0.003029\n",
            "Epoch 410: Train Loss = 0.003072\n",
            "Epoch 420: Train Loss = 0.004545\n",
            "Epoch 430: Train Loss = 0.003295\n",
            "Epoch 440: Train Loss = 0.003535\n",
            "Epoch 450: Train Loss = 0.002678\n",
            "Epoch 460: Train Loss = 0.003270\n",
            "Epoch 470: Train Loss = 0.002539\n",
            "Epoch 480: Train Loss = 0.002518\n",
            "Epoch 490: Train Loss = 0.003151\n",
            "New best model saved at epoch 497 with loss 0.002371\n",
            "Epoch 500: Train Loss = 0.002659\n",
            "Final Test MSE for DSSM-Basic (40_60): 0.005900\n",
            "\n",
            "==== Running Split: 20_80 ====\n",
            "X (INPUT): 20, Y (OUTPUT): 81\n",
            "Train X shape: torch.Size([2162, 20]), Y: torch.Size([2162, 81]), Static: torch.Size([2162, 14])\n",
            "Test  X shape: torch.Size([541, 20]), Y: torch.Size([541, 81]), Static: torch.Size([541, 14])\n",
            "Training started...\n",
            "New best model saved at epoch 1 with loss 2.472921\n",
            "New best model saved at epoch 2 with loss 0.196240\n",
            "New best model saved at epoch 3 with loss 0.152756\n",
            "New best model saved at epoch 4 with loss 0.148713\n",
            "New best model saved at epoch 5 with loss 0.144369\n",
            "New best model saved at epoch 6 with loss 0.142741\n",
            "New best model saved at epoch 7 with loss 0.136461\n",
            "New best model saved at epoch 8 with loss 0.133173\n",
            "New best model saved at epoch 9 with loss 0.128301\n",
            "Epoch 10: Train Loss = 0.125924\n",
            "New best model saved at epoch 10 with loss 0.125924\n",
            "New best model saved at epoch 11 with loss 0.124644\n",
            "New best model saved at epoch 12 with loss 0.115923\n",
            "New best model saved at epoch 13 with loss 0.111153\n",
            "New best model saved at epoch 14 with loss 0.107991\n",
            "New best model saved at epoch 15 with loss 0.101166\n",
            "New best model saved at epoch 16 with loss 0.095812\n",
            "New best model saved at epoch 17 with loss 0.089833\n",
            "New best model saved at epoch 18 with loss 0.083540\n",
            "New best model saved at epoch 19 with loss 0.080132\n",
            "Epoch 20: Train Loss = 0.075865\n",
            "New best model saved at epoch 20 with loss 0.075865\n",
            "New best model saved at epoch 21 with loss 0.072344\n",
            "New best model saved at epoch 22 with loss 0.070958\n",
            "New best model saved at epoch 23 with loss 0.068689\n",
            "New best model saved at epoch 24 with loss 0.064610\n",
            "New best model saved at epoch 25 with loss 0.059208\n",
            "New best model saved at epoch 27 with loss 0.053649\n",
            "New best model saved at epoch 28 with loss 0.052111\n",
            "New best model saved at epoch 29 with loss 0.046979\n",
            "Epoch 30: Train Loss = 0.050364\n",
            "New best model saved at epoch 31 with loss 0.045785\n",
            "New best model saved at epoch 33 with loss 0.041636\n",
            "New best model saved at epoch 34 with loss 0.037597\n",
            "New best model saved at epoch 35 with loss 0.037137\n",
            "New best model saved at epoch 37 with loss 0.033364\n",
            "New best model saved at epoch 38 with loss 0.030974\n",
            "Epoch 40: Train Loss = 0.034146\n",
            "New best model saved at epoch 41 with loss 0.028241\n",
            "New best model saved at epoch 42 with loss 0.026820\n",
            "New best model saved at epoch 44 with loss 0.026663\n",
            "New best model saved at epoch 45 with loss 0.024153\n",
            "New best model saved at epoch 46 with loss 0.023451\n",
            "Epoch 50: Train Loss = 0.022376\n",
            "New best model saved at epoch 50 with loss 0.022376\n",
            "New best model saved at epoch 51 with loss 0.021632\n",
            "New best model saved at epoch 53 with loss 0.021035\n",
            "New best model saved at epoch 54 with loss 0.019476\n",
            "New best model saved at epoch 55 with loss 0.019218\n",
            "New best model saved at epoch 56 with loss 0.018940\n",
            "New best model saved at epoch 57 with loss 0.017917\n",
            "New best model saved at epoch 58 with loss 0.017406\n",
            "Epoch 60: Train Loss = 0.017879\n",
            "New best model saved at epoch 61 with loss 0.017087\n",
            "New best model saved at epoch 62 with loss 0.016318\n",
            "New best model saved at epoch 66 with loss 0.015621\n",
            "New best model saved at epoch 67 with loss 0.015513\n",
            "Epoch 70: Train Loss = 0.016420\n",
            "New best model saved at epoch 71 with loss 0.015078\n",
            "New best model saved at epoch 76 with loss 0.013851\n",
            "New best model saved at epoch 79 with loss 0.012981\n",
            "Epoch 80: Train Loss = 0.013244\n",
            "New best model saved at epoch 84 with loss 0.012842\n",
            "New best model saved at epoch 85 with loss 0.012345\n",
            "Epoch 90: Train Loss = 0.015072\n",
            "New best model saved at epoch 95 with loss 0.011751\n",
            "New best model saved at epoch 96 with loss 0.011640\n",
            "New best model saved at epoch 99 with loss 0.011150\n",
            "Epoch 100: Train Loss = 0.010963\n",
            "New best model saved at epoch 100 with loss 0.010963\n",
            "New best model saved at epoch 103 with loss 0.010634\n",
            "New best model saved at epoch 104 with loss 0.010593\n",
            "Epoch 110: Train Loss = 0.012417\n",
            "New best model saved at epoch 117 with loss 0.010578\n",
            "New best model saved at epoch 119 with loss 0.009939\n",
            "Epoch 120: Train Loss = 0.009829\n",
            "New best model saved at epoch 120 with loss 0.009829\n",
            "New best model saved at epoch 128 with loss 0.009305\n",
            "Epoch 130: Train Loss = 0.011486\n",
            "New best model saved at epoch 135 with loss 0.009124\n",
            "Epoch 140: Train Loss = 0.009163\n",
            "New best model saved at epoch 145 with loss 0.008688\n",
            "Epoch 150: Train Loss = 0.010492\n",
            "New best model saved at epoch 154 with loss 0.008593\n",
            "New best model saved at epoch 155 with loss 0.007904\n",
            "Epoch 160: Train Loss = 0.008492\n",
            "Epoch 170: Train Loss = 0.009920\n",
            "Epoch 180: Train Loss = 0.009214\n",
            "New best model saved at epoch 189 with loss 0.007895\n",
            "Epoch 190: Train Loss = 0.007368\n",
            "New best model saved at epoch 190 with loss 0.007368\n",
            "Epoch 200: Train Loss = 0.007922\n",
            "New best model saved at epoch 206 with loss 0.007166\n",
            "New best model saved at epoch 209 with loss 0.006981\n",
            "Epoch 210: Train Loss = 0.008350\n",
            "Epoch 220: Train Loss = 0.011914\n",
            "New best model saved at epoch 227 with loss 0.006828\n",
            "Epoch 230: Train Loss = 0.006616\n",
            "New best model saved at epoch 230 with loss 0.006616\n",
            "Epoch 240: Train Loss = 0.006364\n",
            "New best model saved at epoch 240 with loss 0.006364\n",
            "New best model saved at epoch 242 with loss 0.006281\n",
            "New best model saved at epoch 249 with loss 0.006098\n",
            "Epoch 250: Train Loss = 0.006487\n",
            "New best model saved at epoch 259 with loss 0.005993\n",
            "Epoch 260: Train Loss = 0.007460\n",
            "Epoch 270: Train Loss = 0.005994\n",
            "Epoch 280: Train Loss = 0.006826\n",
            "Epoch 290: Train Loss = 0.006642\n",
            "Epoch 300: Train Loss = 0.008574\n",
            "New best model saved at epoch 301 with loss 0.005907\n",
            "New best model saved at epoch 303 with loss 0.005824\n",
            "New best model saved at epoch 304 with loss 0.005542\n",
            "New best model saved at epoch 306 with loss 0.005492\n",
            "Epoch 310: Train Loss = 0.008001\n",
            "Epoch 320: Train Loss = 0.005375\n",
            "New best model saved at epoch 320 with loss 0.005375\n",
            "Epoch 330: Train Loss = 0.005662\n",
            "Epoch 340: Train Loss = 0.006803\n",
            "Epoch 350: Train Loss = 0.005621\n",
            "New best model saved at epoch 355 with loss 0.005331\n",
            "Epoch 360: Train Loss = 0.006099\n",
            "New best model saved at epoch 361 with loss 0.005213\n",
            "New best model saved at epoch 362 with loss 0.005163\n",
            "Epoch 370: Train Loss = 0.005889\n",
            "New best model saved at epoch 378 with loss 0.005100\n",
            "Epoch 380: Train Loss = 0.006142\n",
            "New best model saved at epoch 384 with loss 0.005010\n",
            "Epoch 390: Train Loss = 0.005354\n",
            "Epoch 400: Train Loss = 0.005573\n",
            "New best model saved at epoch 407 with loss 0.004484\n",
            "Epoch 410: Train Loss = 0.004817\n",
            "Epoch 420: Train Loss = 0.005327\n",
            "Epoch 430: Train Loss = 0.005009\n",
            "Epoch 440: Train Loss = 0.005371\n",
            "Epoch 450: Train Loss = 0.005394\n",
            "Epoch 460: Train Loss = 0.006107\n",
            "New best model saved at epoch 466 with loss 0.004381\n",
            "New best model saved at epoch 467 with loss 0.004365\n",
            "Epoch 470: Train Loss = 0.004695\n",
            "New best model saved at epoch 477 with loss 0.004077\n",
            "Epoch 480: Train Loss = 0.006519\n",
            "Epoch 490: Train Loss = 0.005098\n",
            "New best model saved at epoch 496 with loss 0.003921\n",
            "Epoch 500: Train Loss = 0.004980\n",
            "Final Test MSE for DSSM-Basic (20_80): 0.010188\n",
            "\n",
            "==== Running Split: 10_90 ====\n",
            "X (INPUT): 10, Y (OUTPUT): 91\n",
            "Train X shape: torch.Size([2162, 10]), Y: torch.Size([2162, 91]), Static: torch.Size([2162, 14])\n",
            "Test  X shape: torch.Size([541, 10]), Y: torch.Size([541, 91]), Static: torch.Size([541, 14])\n",
            "Training started...\n",
            "New best model saved at epoch 1 with loss 2.221325\n",
            "New best model saved at epoch 2 with loss 0.183896\n",
            "New best model saved at epoch 3 with loss 0.152053\n",
            "New best model saved at epoch 4 with loss 0.146000\n",
            "New best model saved at epoch 5 with loss 0.142900\n",
            "New best model saved at epoch 6 with loss 0.136987\n",
            "New best model saved at epoch 7 with loss 0.133761\n",
            "New best model saved at epoch 8 with loss 0.124831\n",
            "New best model saved at epoch 9 with loss 0.116163\n",
            "Epoch 10: Train Loss = 0.107342\n",
            "New best model saved at epoch 10 with loss 0.107342\n",
            "New best model saved at epoch 11 with loss 0.094520\n",
            "New best model saved at epoch 12 with loss 0.084660\n",
            "New best model saved at epoch 13 with loss 0.079708\n",
            "New best model saved at epoch 14 with loss 0.072674\n",
            "New best model saved at epoch 15 with loss 0.066776\n",
            "New best model saved at epoch 16 with loss 0.064072\n",
            "New best model saved at epoch 17 with loss 0.061961\n",
            "New best model saved at epoch 18 with loss 0.058119\n",
            "Epoch 20: Train Loss = 0.054648\n",
            "New best model saved at epoch 20 with loss 0.054648\n",
            "New best model saved at epoch 21 with loss 0.050050\n",
            "New best model saved at epoch 22 with loss 0.049075\n",
            "New best model saved at epoch 23 with loss 0.047916\n",
            "New best model saved at epoch 24 with loss 0.044347\n",
            "New best model saved at epoch 25 with loss 0.042918\n",
            "New best model saved at epoch 26 with loss 0.041965\n",
            "New best model saved at epoch 27 with loss 0.037989\n",
            "New best model saved at epoch 29 with loss 0.036905\n",
            "Epoch 30: Train Loss = 0.036308\n",
            "New best model saved at epoch 30 with loss 0.036308\n",
            "New best model saved at epoch 32 with loss 0.034353\n",
            "New best model saved at epoch 33 with loss 0.032865\n",
            "New best model saved at epoch 35 with loss 0.030173\n",
            "New best model saved at epoch 37 with loss 0.028076\n",
            "Epoch 40: Train Loss = 0.029326\n",
            "New best model saved at epoch 43 with loss 0.027224\n",
            "New best model saved at epoch 44 with loss 0.026071\n",
            "New best model saved at epoch 45 with loss 0.025041\n",
            "New best model saved at epoch 47 with loss 0.024179\n",
            "Epoch 50: Train Loss = 0.024948\n",
            "New best model saved at epoch 53 with loss 0.023714\n",
            "New best model saved at epoch 54 with loss 0.023256\n",
            "New best model saved at epoch 55 with loss 0.021898\n",
            "Epoch 60: Train Loss = 0.021718\n",
            "New best model saved at epoch 60 with loss 0.021718\n",
            "New best model saved at epoch 61 with loss 0.021222\n",
            "New best model saved at epoch 62 with loss 0.021123\n",
            "New best model saved at epoch 63 with loss 0.019994\n",
            "New best model saved at epoch 68 with loss 0.019749\n",
            "New best model saved at epoch 69 with loss 0.019288\n",
            "Epoch 70: Train Loss = 0.023428\n",
            "New best model saved at epoch 72 with loss 0.018618\n",
            "New best model saved at epoch 76 with loss 0.018562\n",
            "New best model saved at epoch 78 with loss 0.018022\n",
            "New best model saved at epoch 79 with loss 0.017512\n",
            "Epoch 80: Train Loss = 0.017604\n",
            "New best model saved at epoch 84 with loss 0.016689\n",
            "Epoch 90: Train Loss = 0.016948\n",
            "New best model saved at epoch 92 with loss 0.016546\n",
            "New best model saved at epoch 93 with loss 0.016377\n",
            "New best model saved at epoch 94 with loss 0.014812\n",
            "Epoch 100: Train Loss = 0.016642\n",
            "New best model saved at epoch 109 with loss 0.014592\n",
            "Epoch 110: Train Loss = 0.015690\n",
            "New best model saved at epoch 118 with loss 0.014476\n",
            "New best model saved at epoch 119 with loss 0.014121\n",
            "Epoch 120: Train Loss = 0.013488\n",
            "New best model saved at epoch 120 with loss 0.013488\n",
            "New best model saved at epoch 121 with loss 0.013219\n",
            "Epoch 130: Train Loss = 0.013233\n",
            "New best model saved at epoch 134 with loss 0.012963\n",
            "Epoch 140: Train Loss = 0.013927\n",
            "New best model saved at epoch 144 with loss 0.012755\n",
            "Epoch 150: Train Loss = 0.012743\n",
            "New best model saved at epoch 150 with loss 0.012743\n",
            "New best model saved at epoch 151 with loss 0.011868\n",
            "Epoch 160: Train Loss = 0.011855\n",
            "New best model saved at epoch 160 with loss 0.011855\n",
            "Epoch 170: Train Loss = 0.012143\n",
            "Epoch 180: Train Loss = 0.013345\n",
            "New best model saved at epoch 182 with loss 0.011852\n",
            "New best model saved at epoch 183 with loss 0.011596\n",
            "New best model saved at epoch 189 with loss 0.011512\n",
            "Epoch 190: Train Loss = 0.014803\n",
            "Epoch 200: Train Loss = 0.012409\n",
            "Epoch 210: Train Loss = 0.011848\n",
            "New best model saved at epoch 214 with loss 0.011050\n",
            "Epoch 220: Train Loss = 0.010865\n",
            "New best model saved at epoch 220 with loss 0.010865\n",
            "New best model saved at epoch 221 with loss 0.010800\n",
            "Epoch 230: Train Loss = 0.010969\n",
            "New best model saved at epoch 234 with loss 0.010598\n",
            "Epoch 240: Train Loss = 0.012345\n",
            "New best model saved at epoch 244 with loss 0.010508\n",
            "New best model saved at epoch 245 with loss 0.010199\n",
            "Epoch 250: Train Loss = 0.011197\n",
            "Epoch 260: Train Loss = 0.011660\n",
            "New best model saved at epoch 263 with loss 0.009892\n",
            "New best model saved at epoch 264 with loss 0.009694\n",
            "Epoch 270: Train Loss = 0.010809\n",
            "Epoch 280: Train Loss = 0.011574\n",
            "Epoch 290: Train Loss = 0.010598\n",
            "Epoch 300: Train Loss = 0.011272\n",
            "Epoch 310: Train Loss = 0.011356\n",
            "Epoch 320: Train Loss = 0.009751\n",
            "New best model saved at epoch 325 with loss 0.009420\n",
            "Epoch 330: Train Loss = 0.010469\n",
            "Epoch 340: Train Loss = 0.010886\n",
            "New best model saved at epoch 348 with loss 0.009265\n",
            "New best model saved at epoch 349 with loss 0.009184\n",
            "Epoch 350: Train Loss = 0.010025\n",
            "Epoch 360: Train Loss = 0.010070\n",
            "New best model saved at epoch 362 with loss 0.009036\n",
            "New best model saved at epoch 367 with loss 0.009011\n",
            "Epoch 370: Train Loss = 0.009040\n",
            "New best model saved at epoch 371 with loss 0.009008\n",
            "New best model saved at epoch 377 with loss 0.008805\n",
            "New best model saved at epoch 378 with loss 0.008742\n",
            "Epoch 380: Train Loss = 0.009533\n",
            "New best model saved at epoch 387 with loss 0.008612\n",
            "Epoch 390: Train Loss = 0.011001\n",
            "New best model saved at epoch 395 with loss 0.008549\n",
            "Epoch 400: Train Loss = 0.010927\n",
            "Epoch 410: Train Loss = 0.009089\n",
            "New best model saved at epoch 414 with loss 0.008212\n",
            "Epoch 420: Train Loss = 0.008754\n",
            "New best model saved at epoch 429 with loss 0.008077\n",
            "Epoch 430: Train Loss = 0.008377\n",
            "Epoch 440: Train Loss = 0.008518\n",
            "Epoch 450: Train Loss = 0.009381\n",
            "Epoch 460: Train Loss = 0.008467\n",
            "Epoch 470: Train Loss = 0.008392\n",
            "New best model saved at epoch 471 with loss 0.007931\n",
            "Epoch 480: Train Loss = 0.008691\n",
            "New best model saved at epoch 484 with loss 0.007871\n",
            "Epoch 490: Train Loss = 0.007844\n",
            "New best model saved at epoch 490 with loss 0.007844\n",
            "New best model saved at epoch 491 with loss 0.007824\n",
            "Epoch 500: Train Loss = 0.008377\n",
            "Final Test MSE for DSSM-Basic (10_90): 0.016807\n",
            "\n",
            "==== Running Split: 5_95 ====\n",
            "X (INPUT): 5, Y (OUTPUT): 96\n",
            "Train X shape: torch.Size([2162, 5]), Y: torch.Size([2162, 96]), Static: torch.Size([2162, 14])\n",
            "Test  X shape: torch.Size([541, 5]), Y: torch.Size([541, 96]), Static: torch.Size([541, 14])\n",
            "Training started...\n",
            "New best model saved at epoch 1 with loss 2.421121\n",
            "New best model saved at epoch 2 with loss 0.192718\n",
            "New best model saved at epoch 3 with loss 0.153868\n",
            "New best model saved at epoch 4 with loss 0.148914\n",
            "New best model saved at epoch 5 with loss 0.139737\n",
            "New best model saved at epoch 6 with loss 0.133151\n",
            "New best model saved at epoch 7 with loss 0.120739\n",
            "New best model saved at epoch 8 with loss 0.112931\n",
            "New best model saved at epoch 9 with loss 0.105838\n",
            "Epoch 10: Train Loss = 0.104786\n",
            "New best model saved at epoch 10 with loss 0.104786\n",
            "New best model saved at epoch 11 with loss 0.098049\n",
            "New best model saved at epoch 12 with loss 0.088142\n",
            "New best model saved at epoch 13 with loss 0.082434\n",
            "New best model saved at epoch 14 with loss 0.075958\n",
            "New best model saved at epoch 15 with loss 0.071518\n",
            "New best model saved at epoch 16 with loss 0.067423\n",
            "New best model saved at epoch 17 with loss 0.065227\n",
            "New best model saved at epoch 18 with loss 0.062277\n",
            "New best model saved at epoch 19 with loss 0.056433\n",
            "Epoch 20: Train Loss = 0.052920\n",
            "New best model saved at epoch 20 with loss 0.052920\n",
            "New best model saved at epoch 21 with loss 0.052808\n",
            "New best model saved at epoch 22 with loss 0.049088\n",
            "New best model saved at epoch 23 with loss 0.048909\n",
            "New best model saved at epoch 24 with loss 0.046729\n",
            "New best model saved at epoch 25 with loss 0.043731\n",
            "New best model saved at epoch 26 with loss 0.041985\n",
            "New best model saved at epoch 27 with loss 0.041815\n",
            "New best model saved at epoch 29 with loss 0.041282\n",
            "Epoch 30: Train Loss = 0.038548\n",
            "New best model saved at epoch 30 with loss 0.038548\n",
            "New best model saved at epoch 31 with loss 0.036339\n",
            "New best model saved at epoch 32 with loss 0.036159\n",
            "New best model saved at epoch 33 with loss 0.036051\n",
            "New best model saved at epoch 34 with loss 0.034501\n",
            "New best model saved at epoch 35 with loss 0.032030\n",
            "New best model saved at epoch 36 with loss 0.031373\n",
            "New best model saved at epoch 38 with loss 0.030820\n",
            "New best model saved at epoch 39 with loss 0.030386\n",
            "Epoch 40: Train Loss = 0.029822\n",
            "New best model saved at epoch 40 with loss 0.029822\n",
            "New best model saved at epoch 41 with loss 0.029815\n",
            "New best model saved at epoch 44 with loss 0.029020\n",
            "New best model saved at epoch 45 with loss 0.027594\n",
            "New best model saved at epoch 48 with loss 0.027216\n",
            "Epoch 50: Train Loss = 0.026378\n",
            "New best model saved at epoch 50 with loss 0.026378\n",
            "New best model saved at epoch 52 with loss 0.026205\n",
            "New best model saved at epoch 54 with loss 0.025887\n",
            "New best model saved at epoch 55 with loss 0.025224\n",
            "New best model saved at epoch 58 with loss 0.024543\n",
            "Epoch 60: Train Loss = 0.025350\n",
            "New best model saved at epoch 65 with loss 0.024018\n",
            "New best model saved at epoch 66 with loss 0.023005\n",
            "Epoch 70: Train Loss = 0.026428\n",
            "New best model saved at epoch 76 with loss 0.022716\n",
            "New best model saved at epoch 79 with loss 0.022223\n",
            "Epoch 80: Train Loss = 0.021467\n",
            "New best model saved at epoch 80 with loss 0.021467\n",
            "New best model saved at epoch 83 with loss 0.020886\n",
            "Epoch 90: Train Loss = 0.020619\n",
            "New best model saved at epoch 90 with loss 0.020619\n",
            "New best model saved at epoch 92 with loss 0.019901\n",
            "New best model saved at epoch 95 with loss 0.019676\n",
            "New best model saved at epoch 97 with loss 0.019152\n",
            "Epoch 100: Train Loss = 0.019685\n",
            "New best model saved at epoch 107 with loss 0.019008\n",
            "Epoch 110: Train Loss = 0.019848\n",
            "New best model saved at epoch 111 with loss 0.018763\n",
            "New best model saved at epoch 112 with loss 0.018301\n",
            "New best model saved at epoch 116 with loss 0.018123\n",
            "New best model saved at epoch 118 with loss 0.018027\n",
            "Epoch 120: Train Loss = 0.018786\n",
            "New best model saved at epoch 124 with loss 0.017551\n",
            "New best model saved at epoch 125 with loss 0.017010\n",
            "Epoch 130: Train Loss = 0.016796\n",
            "New best model saved at epoch 130 with loss 0.016796\n",
            "New best model saved at epoch 136 with loss 0.016501\n",
            "Epoch 140: Train Loss = 0.016868\n",
            "New best model saved at epoch 142 with loss 0.015656\n",
            "Epoch 150: Train Loss = 0.016983\n",
            "Epoch 160: Train Loss = 0.015210\n",
            "New best model saved at epoch 160 with loss 0.015210\n",
            "New best model saved at epoch 164 with loss 0.015112\n",
            "New best model saved at epoch 167 with loss 0.014802\n",
            "New best model saved at epoch 168 with loss 0.014461\n",
            "Epoch 170: Train Loss = 0.014769\n",
            "Epoch 180: Train Loss = 0.016981\n",
            "New best model saved at epoch 187 with loss 0.014354\n",
            "Epoch 190: Train Loss = 0.014670\n",
            "New best model saved at epoch 194 with loss 0.014204\n",
            "New best model saved at epoch 198 with loss 0.014174\n",
            "New best model saved at epoch 199 with loss 0.013995\n",
            "Epoch 200: Train Loss = 0.013777\n",
            "New best model saved at epoch 200 with loss 0.013777\n",
            "New best model saved at epoch 201 with loss 0.013574\n",
            "Epoch 210: Train Loss = 0.013212\n",
            "New best model saved at epoch 210 with loss 0.013212\n",
            "Epoch 220: Train Loss = 0.014683\n",
            "New best model saved at epoch 226 with loss 0.012919\n",
            "Epoch 230: Train Loss = 0.012856\n",
            "New best model saved at epoch 230 with loss 0.012856\n",
            "New best model saved at epoch 236 with loss 0.012511\n",
            "Epoch 240: Train Loss = 0.013547\n",
            "Epoch 250: Train Loss = 0.012222\n",
            "New best model saved at epoch 250 with loss 0.012222\n",
            "New best model saved at epoch 252 with loss 0.012161\n",
            "Epoch 260: Train Loss = 0.012509\n",
            "New best model saved at epoch 261 with loss 0.012084\n",
            "New best model saved at epoch 266 with loss 0.011740\n",
            "New best model saved at epoch 269 with loss 0.011686\n",
            "Epoch 270: Train Loss = 0.012322\n",
            "Epoch 280: Train Loss = 0.011235\n",
            "New best model saved at epoch 280 with loss 0.011235\n",
            "Epoch 290: Train Loss = 0.013615\n",
            "Epoch 300: Train Loss = 0.014451\n",
            "New best model saved at epoch 304 with loss 0.011174\n",
            "Epoch 310: Train Loss = 0.011228\n",
            "New best model saved at epoch 319 with loss 0.010934\n",
            "Epoch 320: Train Loss = 0.011635\n",
            "Epoch 330: Train Loss = 0.011120\n",
            "New best model saved at epoch 334 with loss 0.010718\n",
            "Epoch 340: Train Loss = 0.011156\n",
            "Epoch 350: Train Loss = 0.011622\n",
            "New best model saved at epoch 352 with loss 0.010493\n",
            "Epoch 360: Train Loss = 0.010331\n",
            "New best model saved at epoch 360 with loss 0.010331\n",
            "New best model saved at epoch 362 with loss 0.010217\n",
            "Epoch 370: Train Loss = 0.011558\n",
            "Epoch 380: Train Loss = 0.012427\n",
            "New best model saved at epoch 387 with loss 0.009844\n",
            "Epoch 390: Train Loss = 0.011582\n",
            "Epoch 400: Train Loss = 0.011301\n",
            "Epoch 410: Train Loss = 0.009837\n",
            "New best model saved at epoch 410 with loss 0.009837\n",
            "New best model saved at epoch 419 with loss 0.009722\n",
            "Epoch 420: Train Loss = 0.011054\n",
            "New best model saved at epoch 423 with loss 0.009624\n",
            "New best model saved at epoch 424 with loss 0.009542\n",
            "New best model saved at epoch 428 with loss 0.009532\n",
            "Epoch 430: Train Loss = 0.009971\n",
            "New best model saved at epoch 433 with loss 0.009429\n",
            "Epoch 440: Train Loss = 0.009730\n",
            "New best model saved at epoch 447 with loss 0.009313\n",
            "Epoch 450: Train Loss = 0.011471\n",
            "New best model saved at epoch 453 with loss 0.009158\n",
            "Epoch 460: Train Loss = 0.009965\n",
            "Epoch 470: Train Loss = 0.009605\n",
            "New best model saved at epoch 477 with loss 0.009113\n",
            "New best model saved at epoch 478 with loss 0.008820\n",
            "Epoch 480: Train Loss = 0.009765\n",
            "New best model saved at epoch 484 with loss 0.008595\n",
            "Epoch 490: Train Loss = 0.009768\n",
            "Epoch 500: Train Loss = 0.011123\n",
            "Final Test MSE for DSSM-Basic (5_95): 0.021219\n",
            "\n",
            "==== Running Split: 3_97 ====\n",
            "X (INPUT): 3, Y (OUTPUT): 98\n",
            "Train X shape: torch.Size([2162, 3]), Y: torch.Size([2162, 98]), Static: torch.Size([2162, 14])\n",
            "Test  X shape: torch.Size([541, 3]), Y: torch.Size([541, 98]), Static: torch.Size([541, 14])\n",
            "Training started...\n",
            "New best model saved at epoch 1 with loss 2.056826\n",
            "New best model saved at epoch 2 with loss 0.181867\n",
            "New best model saved at epoch 3 with loss 0.156631\n",
            "New best model saved at epoch 4 with loss 0.152709\n",
            "New best model saved at epoch 5 with loss 0.145754\n",
            "New best model saved at epoch 6 with loss 0.141864\n",
            "New best model saved at epoch 7 with loss 0.136658\n",
            "New best model saved at epoch 8 with loss 0.133413\n",
            "New best model saved at epoch 9 with loss 0.127556\n",
            "Epoch 10: Train Loss = 0.122607\n",
            "New best model saved at epoch 10 with loss 0.122607\n",
            "New best model saved at epoch 11 with loss 0.112481\n",
            "New best model saved at epoch 12 with loss 0.102277\n",
            "New best model saved at epoch 13 with loss 0.092737\n",
            "New best model saved at epoch 14 with loss 0.080033\n",
            "New best model saved at epoch 15 with loss 0.074537\n",
            "New best model saved at epoch 16 with loss 0.063299\n",
            "New best model saved at epoch 17 with loss 0.058413\n",
            "New best model saved at epoch 18 with loss 0.051752\n",
            "Epoch 20: Train Loss = 0.048393\n",
            "New best model saved at epoch 20 with loss 0.048393\n",
            "New best model saved at epoch 21 with loss 0.045719\n",
            "New best model saved at epoch 23 with loss 0.044634\n",
            "New best model saved at epoch 24 with loss 0.042515\n",
            "New best model saved at epoch 25 with loss 0.040482\n",
            "New best model saved at epoch 26 with loss 0.039140\n",
            "New best model saved at epoch 29 with loss 0.039015\n",
            "Epoch 30: Train Loss = 0.038228\n",
            "New best model saved at epoch 30 with loss 0.038228\n",
            "New best model saved at epoch 32 with loss 0.036403\n",
            "New best model saved at epoch 33 with loss 0.035995\n",
            "New best model saved at epoch 35 with loss 0.035781\n",
            "New best model saved at epoch 39 with loss 0.034398\n",
            "Epoch 40: Train Loss = 0.033510\n",
            "New best model saved at epoch 40 with loss 0.033510\n",
            "New best model saved at epoch 43 with loss 0.032379\n",
            "New best model saved at epoch 47 with loss 0.031529\n",
            "New best model saved at epoch 48 with loss 0.031400\n",
            "Epoch 50: Train Loss = 0.031827\n",
            "New best model saved at epoch 51 with loss 0.030990\n",
            "New best model saved at epoch 52 with loss 0.030719\n",
            "New best model saved at epoch 53 with loss 0.030540\n",
            "Epoch 60: Train Loss = 0.034597\n",
            "New best model saved at epoch 62 with loss 0.028929\n",
            "New best model saved at epoch 68 with loss 0.028837\n",
            "New best model saved at epoch 69 with loss 0.028513\n",
            "Epoch 70: Train Loss = 0.028578\n",
            "New best model saved at epoch 72 with loss 0.027516\n",
            "New best model saved at epoch 77 with loss 0.026880\n",
            "Epoch 80: Train Loss = 0.026820\n",
            "New best model saved at epoch 80 with loss 0.026820\n",
            "New best model saved at epoch 81 with loss 0.026115\n",
            "New best model saved at epoch 86 with loss 0.025743\n",
            "New best model saved at epoch 87 with loss 0.025447\n",
            "New best model saved at epoch 88 with loss 0.025026\n",
            "Epoch 90: Train Loss = 0.026791\n",
            "New best model saved at epoch 94 with loss 0.024914\n",
            "New best model saved at epoch 99 with loss 0.024378\n",
            "Epoch 100: Train Loss = 0.025903\n",
            "New best model saved at epoch 103 with loss 0.023612\n",
            "New best model saved at epoch 104 with loss 0.023087\n",
            "Epoch 110: Train Loss = 0.022956\n",
            "New best model saved at epoch 110 with loss 0.022956\n",
            "New best model saved at epoch 116 with loss 0.022852\n",
            "Epoch 120: Train Loss = 0.022570\n",
            "New best model saved at epoch 120 with loss 0.022570\n",
            "New best model saved at epoch 121 with loss 0.021799\n",
            "New best model saved at epoch 128 with loss 0.020756\n",
            "Epoch 130: Train Loss = 0.022122\n",
            "New best model saved at epoch 133 with loss 0.020263\n",
            "New best model saved at epoch 139 with loss 0.019671\n",
            "Epoch 140: Train Loss = 0.021817\n",
            "New best model saved at epoch 144 with loss 0.019211\n",
            "Epoch 150: Train Loss = 0.022974\n",
            "New best model saved at epoch 153 with loss 0.018887\n",
            "Epoch 160: Train Loss = 0.018322\n",
            "New best model saved at epoch 160 with loss 0.018322\n",
            "New best model saved at epoch 161 with loss 0.017867\n",
            "New best model saved at epoch 162 with loss 0.017529\n",
            "Epoch 170: Train Loss = 0.017008\n",
            "New best model saved at epoch 170 with loss 0.017008\n",
            "New best model saved at epoch 172 with loss 0.016676\n",
            "Epoch 180: Train Loss = 0.017140\n",
            "New best model saved at epoch 181 with loss 0.016351\n",
            "New best model saved at epoch 188 with loss 0.015998\n",
            "Epoch 190: Train Loss = 0.018810\n",
            "New best model saved at epoch 193 with loss 0.015943\n",
            "New best model saved at epoch 194 with loss 0.015829\n",
            "Epoch 200: Train Loss = 0.015534\n",
            "New best model saved at epoch 200 with loss 0.015534\n",
            "Epoch 210: Train Loss = 0.016811\n",
            "New best model saved at epoch 213 with loss 0.015214\n",
            "New best model saved at epoch 214 with loss 0.014945\n",
            "Epoch 220: Train Loss = 0.015885\n",
            "New best model saved at epoch 227 with loss 0.014609\n",
            "Epoch 230: Train Loss = 0.015410\n",
            "Epoch 240: Train Loss = 0.017287\n",
            "New best model saved at epoch 241 with loss 0.014412\n",
            "Epoch 250: Train Loss = 0.014291\n",
            "New best model saved at epoch 250 with loss 0.014291\n",
            "New best model saved at epoch 251 with loss 0.013941\n",
            "New best model saved at epoch 255 with loss 0.013344\n",
            "Epoch 260: Train Loss = 0.014109\n",
            "Epoch 270: Train Loss = 0.013968\n",
            "New best model saved at epoch 279 with loss 0.012518\n",
            "Epoch 280: Train Loss = 0.012500\n",
            "New best model saved at epoch 280 with loss 0.012500\n",
            "Epoch 290: Train Loss = 0.012460\n",
            "New best model saved at epoch 290 with loss 0.012460\n",
            "Epoch 300: Train Loss = 0.012665\n",
            "New best model saved at epoch 302 with loss 0.012347\n",
            "New best model saved at epoch 303 with loss 0.012070\n",
            "Epoch 310: Train Loss = 0.013733\n",
            "New best model saved at epoch 312 with loss 0.012021\n",
            "Epoch 320: Train Loss = 0.015043\n",
            "New best model saved at epoch 324 with loss 0.011616\n",
            "Epoch 330: Train Loss = 0.012719\n",
            "Epoch 340: Train Loss = 0.011767\n",
            "New best model saved at epoch 349 with loss 0.011537\n",
            "Epoch 350: Train Loss = 0.011781\n",
            "New best model saved at epoch 354 with loss 0.011419\n",
            "New best model saved at epoch 356 with loss 0.011380\n",
            "New best model saved at epoch 359 with loss 0.011227\n",
            "Epoch 360: Train Loss = 0.012458\n",
            "Epoch 370: Train Loss = 0.013670\n",
            "New best model saved at epoch 376 with loss 0.010810\n",
            "Epoch 380: Train Loss = 0.010306\n",
            "New best model saved at epoch 380 with loss 0.010306\n",
            "Epoch 390: Train Loss = 0.013262\n",
            "Epoch 400: Train Loss = 0.013066\n",
            "Epoch 410: Train Loss = 0.010941\n",
            "Epoch 420: Train Loss = 0.010932\n",
            "Epoch 430: Train Loss = 0.011488\n",
            "New best model saved at epoch 436 with loss 0.010117\n",
            "New best model saved at epoch 437 with loss 0.009693\n",
            "Epoch 440: Train Loss = 0.010661\n",
            "Epoch 450: Train Loss = 0.011362\n",
            "Epoch 460: Train Loss = 0.010736\n",
            "Epoch 470: Train Loss = 0.009800\n",
            "Epoch 480: Train Loss = 0.010449\n",
            "New best model saved at epoch 484 with loss 0.009534\n",
            "Epoch 490: Train Loss = 0.009731\n",
            "Epoch 500: Train Loss = 0.010988\n",
            "Final Test MSE for DSSM-Basic (3_97): 0.024256\n",
            "\n",
            "==== Running Split: 1_99 ====\n",
            "X (INPUT): 1, Y (OUTPUT): 100\n",
            "Train X shape: torch.Size([2162, 1]), Y: torch.Size([2162, 100]), Static: torch.Size([2162, 14])\n",
            "Test  X shape: torch.Size([541, 1]), Y: torch.Size([541, 100]), Static: torch.Size([541, 14])\n",
            "Training started...\n",
            "New best model saved at epoch 1 with loss 2.396048\n",
            "New best model saved at epoch 2 with loss 0.218298\n",
            "New best model saved at epoch 3 with loss 0.162315\n",
            "New best model saved at epoch 4 with loss 0.158552\n",
            "New best model saved at epoch 5 with loss 0.157276\n",
            "New best model saved at epoch 6 with loss 0.156392\n",
            "New best model saved at epoch 7 with loss 0.153777\n",
            "New best model saved at epoch 8 with loss 0.151688\n",
            "Epoch 10: Train Loss = 0.148730\n",
            "New best model saved at epoch 10 with loss 0.148730\n",
            "New best model saved at epoch 11 with loss 0.144900\n",
            "New best model saved at epoch 12 with loss 0.143245\n",
            "New best model saved at epoch 13 with loss 0.140694\n",
            "New best model saved at epoch 14 with loss 0.139491\n",
            "New best model saved at epoch 15 with loss 0.135872\n",
            "New best model saved at epoch 16 with loss 0.130776\n",
            "New best model saved at epoch 17 with loss 0.129264\n",
            "New best model saved at epoch 19 with loss 0.124295\n",
            "Epoch 20: Train Loss = 0.122510\n",
            "New best model saved at epoch 20 with loss 0.122510\n",
            "New best model saved at epoch 22 with loss 0.119314\n",
            "New best model saved at epoch 23 with loss 0.118182\n",
            "New best model saved at epoch 25 with loss 0.112735\n",
            "New best model saved at epoch 27 with loss 0.111175\n",
            "New best model saved at epoch 28 with loss 0.108561\n",
            "New best model saved at epoch 29 with loss 0.107107\n",
            "Epoch 30: Train Loss = 0.103467\n",
            "New best model saved at epoch 30 with loss 0.103467\n",
            "New best model saved at epoch 31 with loss 0.099545\n",
            "New best model saved at epoch 35 with loss 0.095119\n",
            "New best model saved at epoch 36 with loss 0.090852\n",
            "New best model saved at epoch 37 with loss 0.087779\n",
            "New best model saved at epoch 38 with loss 0.086263\n",
            "New best model saved at epoch 39 with loss 0.084808\n",
            "Epoch 40: Train Loss = 0.084903\n",
            "New best model saved at epoch 42 with loss 0.077909\n",
            "New best model saved at epoch 44 with loss 0.074194\n",
            "New best model saved at epoch 45 with loss 0.071461\n",
            "New best model saved at epoch 48 with loss 0.067834\n",
            "Epoch 50: Train Loss = 0.065844\n",
            "New best model saved at epoch 50 with loss 0.065844\n",
            "New best model saved at epoch 51 with loss 0.064171\n",
            "New best model saved at epoch 52 with loss 0.063187\n",
            "New best model saved at epoch 53 with loss 0.060469\n",
            "New best model saved at epoch 54 with loss 0.058202\n",
            "New best model saved at epoch 58 with loss 0.056709\n",
            "Epoch 60: Train Loss = 0.057778\n",
            "New best model saved at epoch 61 with loss 0.056024\n",
            "New best model saved at epoch 64 with loss 0.054649\n",
            "New best model saved at epoch 65 with loss 0.052268\n",
            "New best model saved at epoch 69 with loss 0.051849\n",
            "Epoch 70: Train Loss = 0.051629\n",
            "New best model saved at epoch 70 with loss 0.051629\n",
            "New best model saved at epoch 72 with loss 0.051348\n",
            "New best model saved at epoch 75 with loss 0.049985\n",
            "New best model saved at epoch 76 with loss 0.048720\n",
            "Epoch 80: Train Loss = 0.056897\n",
            "New best model saved at epoch 82 with loss 0.047519\n",
            "New best model saved at epoch 89 with loss 0.046232\n",
            "Epoch 90: Train Loss = 0.049192\n",
            "New best model saved at epoch 94 with loss 0.046009\n",
            "New best model saved at epoch 99 with loss 0.045753\n",
            "Epoch 100: Train Loss = 0.050725\n",
            "New best model saved at epoch 102 with loss 0.044584\n",
            "New best model saved at epoch 105 with loss 0.044535\n",
            "Epoch 110: Train Loss = 0.042124\n",
            "New best model saved at epoch 110 with loss 0.042124\n",
            "Epoch 120: Train Loss = 0.043726\n",
            "New best model saved at epoch 124 with loss 0.040899\n",
            "Epoch 130: Train Loss = 0.044713\n",
            "Epoch 140: Train Loss = 0.040126\n",
            "New best model saved at epoch 140 with loss 0.040126\n",
            "New best model saved at epoch 146 with loss 0.039166\n",
            "Epoch 150: Train Loss = 0.038863\n",
            "New best model saved at epoch 150 with loss 0.038863\n",
            "Epoch 160: Train Loss = 0.040356\n",
            "Epoch 170: Train Loss = 0.037971\n",
            "New best model saved at epoch 170 with loss 0.037971\n",
            "New best model saved at epoch 176 with loss 0.036792\n",
            "Epoch 180: Train Loss = 0.039424\n",
            "Epoch 190: Train Loss = 0.038187\n",
            "New best model saved at epoch 192 with loss 0.036345\n",
            "Epoch 200: Train Loss = 0.039441\n",
            "New best model saved at epoch 205 with loss 0.036049\n",
            "Epoch 210: Train Loss = 0.038423\n",
            "New best model saved at epoch 218 with loss 0.035188\n",
            "Epoch 220: Train Loss = 0.037312\n",
            "New best model saved at epoch 225 with loss 0.034353\n",
            "Epoch 230: Train Loss = 0.037611\n",
            "Epoch 240: Train Loss = 0.035291\n",
            "Epoch 250: Train Loss = 0.035437\n",
            "New best model saved at epoch 256 with loss 0.034064\n",
            "Epoch 260: Train Loss = 0.035684\n",
            "New best model saved at epoch 264 with loss 0.034058\n",
            "Epoch 270: Train Loss = 0.033447\n",
            "New best model saved at epoch 270 with loss 0.033447\n",
            "New best model saved at epoch 271 with loss 0.033023\n",
            "Epoch 280: Train Loss = 0.037066\n",
            "New best model saved at epoch 284 with loss 0.032892\n",
            "Epoch 290: Train Loss = 0.033397\n",
            "Epoch 300: Train Loss = 0.033557\n",
            "Epoch 310: Train Loss = 0.037726\n",
            "New best model saved at epoch 316 with loss 0.032424\n",
            "Epoch 320: Train Loss = 0.031876\n",
            "New best model saved at epoch 320 with loss 0.031876\n",
            "New best model saved at epoch 321 with loss 0.031044\n",
            "New best model saved at epoch 328 with loss 0.030914\n",
            "New best model saved at epoch 329 with loss 0.030231\n",
            "Epoch 330: Train Loss = 0.032097\n",
            "Epoch 340: Train Loss = 0.031167\n",
            "Epoch 350: Train Loss = 0.030871\n",
            "New best model saved at epoch 355 with loss 0.030096\n",
            "Epoch 360: Train Loss = 0.030045\n",
            "New best model saved at epoch 360 with loss 0.030045\n",
            "Epoch 370: Train Loss = 0.034124\n",
            "New best model saved at epoch 375 with loss 0.030041\n",
            "New best model saved at epoch 378 with loss 0.029886\n",
            "New best model saved at epoch 379 with loss 0.029705\n",
            "Epoch 380: Train Loss = 0.029956\n",
            "Epoch 390: Train Loss = 0.030164\n",
            "New best model saved at epoch 399 with loss 0.029012\n",
            "Epoch 400: Train Loss = 0.030047\n",
            "Epoch 410: Train Loss = 0.029080\n",
            "New best model saved at epoch 419 with loss 0.028699\n",
            "Epoch 420: Train Loss = 0.029215\n",
            "New best model saved at epoch 421 with loss 0.028312\n",
            "Epoch 430: Train Loss = 0.029787\n",
            "New best model saved at epoch 438 with loss 0.028085\n",
            "New best model saved at epoch 439 with loss 0.027405\n",
            "Epoch 440: Train Loss = 0.028870\n",
            "Epoch 450: Train Loss = 0.028007\n",
            "New best model saved at epoch 454 with loss 0.027171\n",
            "New best model saved at epoch 455 with loss 0.026952\n",
            "Epoch 460: Train Loss = 0.028480\n",
            "Epoch 470: Train Loss = 0.028180\n",
            "Epoch 480: Train Loss = 0.026574\n",
            "New best model saved at epoch 480 with loss 0.026574\n",
            "Epoch 490: Train Loss = 0.028316\n",
            "Epoch 500: Train Loss = 0.028002\n",
            "Final Test MSE for DSSM-Basic (1_99): 0.058431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**New LSTM experiment-**"
      ],
      "metadata": {
        "id": "2RL_dz5J663w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Setup\n",
        "splits = [(80, 20), (60, 40), (50, 50), (40, 60), (20, 80), (10, 90), (5, 95), (3, 97), (1, 99)]\n",
        "LSTM_mse = []\n",
        "\n",
        "# For saving models\n",
        "os.makedirs(\"drive/My Drive/DSSM-Models\", exist_ok=True)\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "for train_pct, test_pct in splits:\n",
        "    split_name = f\"{train_pct}_{test_pct}\"\n",
        "    train_ids, test_ids = train_test_split(df_output['file_id'].unique(), test_size=0.2, random_state=42)\n",
        "    df_train = df_output[df_output['file_id'].isin(train_ids)]\n",
        "    df_test = df_output[df_output['file_id'].isin(test_ids)]\n",
        "\n",
        "    train_timestep = int(train_pct / 100 * 101)\n",
        "    pred_timestep = 101 - train_timestep\n",
        "    print(f\"\\n====== Split: {split_name} | X (input): {train_timestep} AND Y (output): {pred_timestep} ======\")\n",
        "\n",
        "    # Pivot time series\n",
        "    X_train = df_train.pivot(index='file_id', columns='timestep', values='CO2').values[:, :train_timestep]\n",
        "    Y_train = df_train.pivot(index='file_id', columns='timestep', values='CO2').values[:, train_timestep:]\n",
        "    X_test = df_test.pivot(index='file_id', columns='timestep', values='CO2').values[:, :train_timestep]\n",
        "    Y_test = df_test.pivot(index='file_id', columns='timestep', values='CO2').values[:, train_timestep:]\n",
        "\n",
        "    static_train = merged_df[merged_df['file_id'].isin(train_ids)].drop(columns=['file_id', 'cluster']).values\n",
        "    static_test = merged_df[merged_df['file_id'].isin(test_ids)].drop(columns=['file_id', 'cluster']).values\n",
        "\n",
        "    # Tensors\n",
        "    X_train_tensor = torch.tensor(X_train[:, :, None], dtype=torch.float32)\n",
        "    Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
        "    static_train_tensor = torch.tensor(static_train, dtype=torch.float32)\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test[:, :, None], dtype=torch.float32)\n",
        "    Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
        "    static_test_tensor = torch.tensor(static_test, dtype=torch.float32)\n",
        "\n",
        "    print(f\"Train X shape: {X_train_tensor.shape}, Train Y shape: {Y_train_tensor.shape}, Static: {static_train_tensor.shape}\")\n",
        "    print(f\"Test  X shape: {X_test_tensor.shape}, Test  Y shape: {Y_test_tensor.shape}, Static: {static_test_tensor.shape}\")\n",
        "\n",
        "    # Dataset and loaders\n",
        "    full_train_dataset = TensorDataset(X_train_tensor, static_train_tensor, Y_train_tensor)\n",
        "    val_size = int(0.1 * len(full_train_dataset))\n",
        "    train_size = len(full_train_dataset) - val_size\n",
        "    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
        "    test_dataset = TensorDataset(X_test_tensor, static_test_tensor, Y_test_tensor)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "    # ------------------ Model and Config ------------------\n",
        "    lstm_config = {\n",
        "        \"static_dim\": static_train.shape[1],\n",
        "        \"hidden_dim\": 128,\n",
        "        \"num_layers\": 2,\n",
        "        \"dropout\": 0.0,\n",
        "        \"lr\": 0.001,\n",
        "        \"batch_size\": 64,\n",
        "        \"epochs\": 500\n",
        "    }\n",
        "\n",
        "    model = LSTM_MIMO(\n",
        "        input_len=train_timestep,\n",
        "        output_len=pred_timestep,\n",
        "        static_dim=lstm_config[\"static_dim\"],\n",
        "        hidden_dim=lstm_config[\"hidden_dim\"],\n",
        "        num_layers=lstm_config[\"num_layers\"]\n",
        "    )\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lstm_config[\"lr\"])\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # ------------------ Training ------------------\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_path = f\"drive/My Drive/DSSM-Models/LSTM_best_{split_name}.pt\"\n",
        "    print(\"Training started...\")\n",
        "\n",
        "    for epoch in range(1, lstm_config[\"epochs\"] + 1):\n",
        "        model.train()\n",
        "        for X_batch, static_batch, Y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X_batch, static_batch)\n",
        "            loss = criterion(preds, Y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_samples = 0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, static_batch, Y_batch in val_loader:\n",
        "                preds = model(X_batch, static_batch)\n",
        "                batch_loss = criterion(preds, Y_batch).item()\n",
        "                val_loss += batch_loss * X_batch.size(0)\n",
        "                val_samples += X_batch.size(0)\n",
        "        avg_val_loss = val_loss / val_samples\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"[Epoch {epoch}] Training Loss: {loss.item():.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "    # ------------------ Evaluation ------------------\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "    model.eval()\n",
        "    total_mse = 0.0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, static_batch, Y_batch in test_loader:\n",
        "            outputs = model(X_batch, static_batch)\n",
        "            batch_mse = criterion(outputs, Y_batch).item()\n",
        "            total_mse += batch_mse * X_batch.size(0)\n",
        "            total_samples += X_batch.size(0)\n",
        "\n",
        "    avg_mse = total_mse / total_samples\n",
        "    print(f\"Final Test MSE for LSTM ({split_name}): {avg_mse:.6f}\")\n",
        "    LSTM_mse.append({'Split': split_name, 'Test_MSE': avg_mse})\n",
        "\n",
        "# Save result as DataFrame\n",
        "LSTM_mse = pd.DataFrame(LSTM_mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7yv3m0nFqsh",
        "outputId": "2ade66f8-3315-4113-fbd0-b7a92092b0b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====== Split: 80_20 | X (input): 80 AND Y (output): 21 ======\n",
            "Train X shape: torch.Size([2162, 80, 1]), Train Y shape: torch.Size([2162, 21]), Static: torch.Size([2162, 14])\n",
            "Test  X shape: torch.Size([541, 80, 1]), Test  Y shape: torch.Size([541, 21]), Static: torch.Size([541, 14])\n",
            "Training started...\n",
            "[Epoch 10] Training Loss: 0.130054 | Val Loss: 0.182844\n",
            "[Epoch 20] Training Loss: 0.168539 | Val Loss: 0.188108\n",
            "[Epoch 30] Training Loss: 0.147180 | Val Loss: 0.194307\n",
            "[Epoch 40] Training Loss: 0.136666 | Val Loss: 0.180678\n",
            "[Epoch 50] Training Loss: 0.131765 | Val Loss: 0.180540\n",
            "[Epoch 60] Training Loss: 0.113578 | Val Loss: 0.194123\n",
            "[Epoch 70] Training Loss: 0.176367 | Val Loss: 0.181501\n",
            "[Epoch 80] Training Loss: 0.118859 | Val Loss: 0.185078\n",
            "[Epoch 90] Training Loss: 0.191583 | Val Loss: 0.181211\n",
            "[Epoch 100] Training Loss: 0.150244 | Val Loss: 0.215887\n",
            "[Epoch 110] Training Loss: 0.180040 | Val Loss: 0.181872\n",
            "[Epoch 120] Training Loss: 0.152387 | Val Loss: 0.206184\n",
            "[Epoch 130] Training Loss: 0.168289 | Val Loss: 0.183208\n",
            "[Epoch 140] Training Loss: 0.233084 | Val Loss: 0.196758\n",
            "[Epoch 150] Training Loss: 0.136322 | Val Loss: 0.186209\n",
            "[Epoch 160] Training Loss: 0.191939 | Val Loss: 0.182376\n",
            "[Epoch 170] Training Loss: 0.247988 | Val Loss: 0.182413\n",
            "[Epoch 180] Training Loss: 0.138360 | Val Loss: 0.185430\n",
            "[Epoch 190] Training Loss: 0.275716 | Val Loss: 0.181062\n",
            "[Epoch 200] Training Loss: 0.161953 | Val Loss: 0.181145\n",
            "[Epoch 210] Training Loss: 0.172888 | Val Loss: 0.181736\n",
            "[Epoch 220] Training Loss: 0.184088 | Val Loss: 0.182289\n",
            "[Epoch 230] Training Loss: 0.159261 | Val Loss: 0.188820\n",
            "[Epoch 240] Training Loss: 0.178705 | Val Loss: 0.177092\n",
            "[Epoch 250] Training Loss: 0.155154 | Val Loss: 0.179972\n",
            "[Epoch 260] Training Loss: 0.151366 | Val Loss: 0.177039\n",
            "[Epoch 270] Training Loss: 0.131935 | Val Loss: 0.176662\n",
            "[Epoch 280] Training Loss: 0.159161 | Val Loss: 0.179508\n",
            "[Epoch 290] Training Loss: 0.216211 | Val Loss: 0.177981\n",
            "[Epoch 300] Training Loss: 0.180392 | Val Loss: 0.175085\n",
            "[Epoch 310] Training Loss: 0.116637 | Val Loss: 0.177851\n",
            "[Epoch 320] Training Loss: 0.157628 | Val Loss: 0.192211\n",
            "[Epoch 330] Training Loss: 0.133262 | Val Loss: 0.178142\n",
            "[Epoch 340] Training Loss: 0.131153 | Val Loss: 0.194562\n",
            "[Epoch 350] Training Loss: 0.177429 | Val Loss: 0.178965\n",
            "[Epoch 360] Training Loss: 0.160261 | Val Loss: 0.189389\n",
            "[Epoch 370] Training Loss: 0.163812 | Val Loss: 0.183035\n",
            "[Epoch 380] Training Loss: 0.212847 | Val Loss: 0.180958\n",
            "[Epoch 390] Training Loss: 0.146607 | Val Loss: 0.177733\n",
            "[Epoch 400] Training Loss: 0.182354 | Val Loss: 0.179345\n",
            "[Epoch 410] Training Loss: 0.099446 | Val Loss: 0.174090\n",
            "[Epoch 420] Training Loss: 0.168117 | Val Loss: 0.183885\n",
            "[Epoch 430] Training Loss: 0.128381 | Val Loss: 0.176580\n",
            "[Epoch 440] Training Loss: 0.151759 | Val Loss: 0.179871\n",
            "[Epoch 450] Training Loss: 0.159833 | Val Loss: 0.180352\n",
            "[Epoch 460] Training Loss: 0.181634 | Val Loss: 0.177518\n",
            "[Epoch 470] Training Loss: 0.237494 | Val Loss: 0.180784\n",
            "[Epoch 480] Training Loss: 0.121158 | Val Loss: 0.185995\n",
            "[Epoch 490] Training Loss: 0.214401 | Val Loss: 0.176277\n",
            "[Epoch 500] Training Loss: 0.178213 | Val Loss: 0.177152\n",
            "Final Test MSE for LSTM (80_20): 0.153122\n",
            "\n",
            "====== Split: 60_40 | X (input): 60 AND Y (output): 41 ======\n",
            "Train X shape: torch.Size([2162, 60, 1]), Train Y shape: torch.Size([2162, 41]), Static: torch.Size([2162, 14])\n",
            "Test  X shape: torch.Size([541, 60, 1]), Test  Y shape: torch.Size([541, 41]), Static: torch.Size([541, 14])\n",
            "Training started...\n",
            "[Epoch 10] Training Loss: 0.160072 | Val Loss: 0.193764\n",
            "[Epoch 20] Training Loss: 0.125812 | Val Loss: 0.178252\n",
            "[Epoch 30] Training Loss: 0.168223 | Val Loss: 0.175086\n",
            "[Epoch 40] Training Loss: 0.227602 | Val Loss: 0.191558\n",
            "[Epoch 50] Training Loss: 0.141959 | Val Loss: 0.182942\n",
            "[Epoch 60] Training Loss: 0.160954 | Val Loss: 0.203810\n",
            "[Epoch 70] Training Loss: 0.129270 | Val Loss: 0.179528\n",
            "[Epoch 80] Training Loss: 0.123232 | Val Loss: 0.183649\n",
            "[Epoch 90] Training Loss: 0.153448 | Val Loss: 0.178136\n",
            "[Epoch 100] Training Loss: 0.215124 | Val Loss: 0.182221\n",
            "[Epoch 110] Training Loss: 0.191127 | Val Loss: 0.176271\n",
            "[Epoch 120] Training Loss: 0.158821 | Val Loss: 0.178799\n",
            "[Epoch 130] Training Loss: 0.215110 | Val Loss: 0.178688\n",
            "[Epoch 140] Training Loss: 0.137982 | Val Loss: 0.182029\n",
            "[Epoch 150] Training Loss: 0.115638 | Val Loss: 0.181434\n",
            "[Epoch 160] Training Loss: 0.119009 | Val Loss: 0.175922\n",
            "[Epoch 170] Training Loss: 0.116394 | Val Loss: 0.177343\n",
            "[Epoch 180] Training Loss: 0.156786 | Val Loss: 0.177376\n",
            "[Epoch 190] Training Loss: 0.236222 | Val Loss: 0.185280\n",
            "[Epoch 200] Training Loss: 0.089129 | Val Loss: 0.184610\n",
            "[Epoch 210] Training Loss: 0.133877 | Val Loss: 0.178632\n",
            "[Epoch 220] Training Loss: 0.171737 | Val Loss: 0.182194\n",
            "[Epoch 230] Training Loss: 0.141890 | Val Loss: 0.226532\n",
            "[Epoch 240] Training Loss: 0.142569 | Val Loss: 0.175702\n",
            "[Epoch 250] Training Loss: 0.161455 | Val Loss: 0.178996\n",
            "[Epoch 260] Training Loss: 0.163516 | Val Loss: 0.174766\n",
            "[Epoch 270] Training Loss: 0.193141 | Val Loss: 0.181462\n",
            "[Epoch 280] Training Loss: 0.179065 | Val Loss: 0.178853\n",
            "[Epoch 290] Training Loss: 0.181671 | Val Loss: 0.176590\n",
            "[Epoch 300] Training Loss: 0.103555 | Val Loss: 0.174670\n",
            "[Epoch 310] Training Loss: 0.156414 | Val Loss: 0.173884\n",
            "[Epoch 320] Training Loss: 0.141869 | Val Loss: 0.175494\n",
            "[Epoch 330] Training Loss: 0.119068 | Val Loss: 0.174858\n",
            "[Epoch 340] Training Loss: 0.144569 | Val Loss: 0.179761\n",
            "[Epoch 350] Training Loss: 0.238539 | Val Loss: 0.177972\n",
            "[Epoch 360] Training Loss: 0.227724 | Val Loss: 0.180355\n",
            "[Epoch 370] Training Loss: 0.124783 | Val Loss: 0.183063\n",
            "[Epoch 380] Training Loss: 0.122255 | Val Loss: 0.176406\n",
            "[Epoch 390] Training Loss: 0.153552 | Val Loss: 0.177496\n",
            "[Epoch 400] Training Loss: 0.144052 | Val Loss: 0.178864\n",
            "[Epoch 410] Training Loss: 0.160516 | Val Loss: 0.177436\n",
            "[Epoch 420] Training Loss: 0.219854 | Val Loss: 0.176965\n",
            "[Epoch 430] Training Loss: 0.166266 | Val Loss: 0.175137\n",
            "[Epoch 440] Training Loss: 0.142171 | Val Loss: 0.175772\n",
            "[Epoch 450] Training Loss: 0.198673 | Val Loss: 0.184647\n",
            "[Epoch 460] Training Loss: 0.143398 | Val Loss: 0.179500\n",
            "[Epoch 470] Training Loss: 0.131239 | Val Loss: 0.174886\n",
            "[Epoch 480] Training Loss: 0.107916 | Val Loss: 0.172953\n",
            "[Epoch 490] Training Loss: 0.207248 | Val Loss: 0.176964\n",
            "[Epoch 500] Training Loss: 0.188606 | Val Loss: 0.174465\n",
            "Final Test MSE for LSTM (60_40): 0.148808\n",
            "\n",
            "====== Split: 50_50 | X (input): 50 AND Y (output): 51 ======\n",
            "Train X shape: torch.Size([2162, 50, 1]), Train Y shape: torch.Size([2162, 51]), Static: torch.Size([2162, 14])\n",
            "Test  X shape: torch.Size([541, 50, 1]), Test  Y shape: torch.Size([541, 51]), Static: torch.Size([541, 14])\n",
            "Training started...\n",
            "[Epoch 10] Training Loss: 0.120351 | Val Loss: 0.184325\n",
            "[Epoch 20] Training Loss: 0.138013 | Val Loss: 0.183777\n",
            "[Epoch 30] Training Loss: 0.169825 | Val Loss: 0.180166\n",
            "[Epoch 40] Training Loss: 0.162222 | Val Loss: 0.180648\n",
            "[Epoch 50] Training Loss: 0.264132 | Val Loss: 0.204817\n",
            "[Epoch 60] Training Loss: 0.185253 | Val Loss: 0.180567\n",
            "[Epoch 70] Training Loss: 0.208975 | Val Loss: 0.184599\n",
            "[Epoch 80] Training Loss: 0.091614 | Val Loss: 0.182196\n",
            "[Epoch 90] Training Loss: 0.165012 | Val Loss: 0.181188\n",
            "[Epoch 100] Training Loss: 0.157406 | Val Loss: 0.203149\n",
            "[Epoch 110] Training Loss: 0.185506 | Val Loss: 0.180246\n",
            "[Epoch 120] Training Loss: 0.147940 | Val Loss: 0.186491\n",
            "[Epoch 130] Training Loss: 0.177132 | Val Loss: 0.183373\n",
            "[Epoch 140] Training Loss: 0.219566 | Val Loss: 0.184590\n",
            "[Epoch 150] Training Loss: 0.172550 | Val Loss: 0.180267\n",
            "[Epoch 160] Training Loss: 0.279481 | Val Loss: 0.184463\n",
            "[Epoch 170] Training Loss: 0.140572 | Val Loss: 0.181939\n",
            "[Epoch 180] Training Loss: 0.157553 | Val Loss: 0.183904\n",
            "[Epoch 190] Training Loss: 0.183318 | Val Loss: 0.180345\n",
            "[Epoch 200] Training Loss: 0.105506 | Val Loss: 0.182650\n",
            "[Epoch 210] Training Loss: 0.162551 | Val Loss: 0.184959\n",
            "[Epoch 220] Training Loss: 0.109322 | Val Loss: 0.180418\n",
            "[Epoch 230] Training Loss: 0.164931 | Val Loss: 0.180621\n",
            "[Epoch 240] Training Loss: 0.188361 | Val Loss: 0.181473\n",
            "[Epoch 250] Training Loss: 0.137044 | Val Loss: 0.180252\n",
            "[Epoch 260] Training Loss: 0.149427 | Val Loss: 0.182187\n",
            "[Epoch 270] Training Loss: 0.188873 | Val Loss: 0.180373\n",
            "[Epoch 280] Training Loss: 0.105123 | Val Loss: 0.188056\n",
            "[Epoch 290] Training Loss: 0.132859 | Val Loss: 0.183950\n",
            "[Epoch 300] Training Loss: 0.150485 | Val Loss: 0.182950\n",
            "[Epoch 310] Training Loss: 0.122216 | Val Loss: 0.185470\n",
            "[Epoch 320] Training Loss: 0.165920 | Val Loss: 0.180402\n",
            "[Epoch 330] Training Loss: 0.203997 | Val Loss: 0.180310\n",
            "[Epoch 340] Training Loss: 0.205143 | Val Loss: 0.187167\n",
            "[Epoch 350] Training Loss: 0.161939 | Val Loss: 0.181002\n",
            "[Epoch 360] Training Loss: 0.122171 | Val Loss: 0.181155\n",
            "[Epoch 370] Training Loss: 0.151626 | Val Loss: 0.183497\n",
            "[Epoch 380] Training Loss: 0.181879 | Val Loss: 0.180674\n",
            "[Epoch 390] Training Loss: 0.285030 | Val Loss: 0.181458\n",
            "[Epoch 400] Training Loss: 0.199296 | Val Loss: 0.180667\n",
            "[Epoch 410] Training Loss: 0.148234 | Val Loss: 0.184078\n",
            "[Epoch 420] Training Loss: 0.182901 | Val Loss: 0.185698\n",
            "[Epoch 430] Training Loss: 0.129643 | Val Loss: 0.181477\n",
            "[Epoch 440] Training Loss: 0.168556 | Val Loss: 0.180429\n",
            "[Epoch 450] Training Loss: 0.137146 | Val Loss: 0.183526\n",
            "[Epoch 460] Training Loss: 0.143662 | Val Loss: 0.180256\n",
            "[Epoch 470] Training Loss: 0.145814 | Val Loss: 0.184451\n",
            "[Epoch 480] Training Loss: 0.151230 | Val Loss: 0.184467\n",
            "[Epoch 490] Training Loss: 0.143046 | Val Loss: 0.190393\n",
            "[Epoch 500] Training Loss: 0.189471 | Val Loss: 0.180806\n",
            "Final Test MSE for LSTM (50_50): 0.148966\n",
            "\n",
            "====== Split: 40_60 | X (input): 40 AND Y (output): 61 ======\n",
            "Train X shape: torch.Size([2162, 40, 1]), Train Y shape: torch.Size([2162, 61]), Static: torch.Size([2162, 14])\n",
            "Test  X shape: torch.Size([541, 40, 1]), Test  Y shape: torch.Size([541, 61]), Static: torch.Size([541, 14])\n",
            "Training started...\n",
            "[Epoch 10] Training Loss: 0.180008 | Val Loss: 0.181269\n",
            "[Epoch 20] Training Loss: 0.152491 | Val Loss: 0.180552\n",
            "[Epoch 30] Training Loss: 0.181940 | Val Loss: 0.181970\n",
            "[Epoch 40] Training Loss: 0.127330 | Val Loss: 0.181782\n",
            "[Epoch 50] Training Loss: 0.162361 | Val Loss: 0.187304\n",
            "[Epoch 60] Training Loss: 0.141276 | Val Loss: 0.187234\n",
            "[Epoch 70] Training Loss: 0.129580 | Val Loss: 0.189584\n",
            "[Epoch 80] Training Loss: 0.161692 | Val Loss: 0.191419\n",
            "[Epoch 90] Training Loss: 0.108884 | Val Loss: 0.180624\n",
            "[Epoch 100] Training Loss: 0.207027 | Val Loss: 0.180096\n",
            "[Epoch 110] Training Loss: 0.133101 | Val Loss: 0.187665\n",
            "[Epoch 120] Training Loss: 0.248004 | Val Loss: 0.188339\n",
            "[Epoch 130] Training Loss: 0.182399 | Val Loss: 0.180625\n",
            "[Epoch 140] Training Loss: 0.125548 | Val Loss: 0.186765\n",
            "[Epoch 150] Training Loss: 0.126791 | Val Loss: 0.181131\n",
            "[Epoch 160] Training Loss: 0.117263 | Val Loss: 0.184270\n",
            "[Epoch 170] Training Loss: 0.185533 | Val Loss: 0.180096\n",
            "[Epoch 180] Training Loss: 0.140987 | Val Loss: 0.183735\n",
            "[Epoch 190] Training Loss: 0.143710 | Val Loss: 0.181371\n",
            "[Epoch 200] Training Loss: 0.159519 | Val Loss: 0.180091\n",
            "[Epoch 210] Training Loss: 0.188343 | Val Loss: 0.187461\n",
            "[Epoch 220] Training Loss: 0.129773 | Val Loss: 0.181405\n",
            "[Epoch 230] Training Loss: 0.158355 | Val Loss: 0.189299\n",
            "[Epoch 240] Training Loss: 0.125780 | Val Loss: 0.185658\n",
            "[Epoch 250] Training Loss: 0.135441 | Val Loss: 0.182679\n",
            "[Epoch 260] Training Loss: 0.122299 | Val Loss: 0.185339\n",
            "[Epoch 270] Training Loss: 0.145624 | Val Loss: 0.179870\n",
            "[Epoch 280] Training Loss: 0.155466 | Val Loss: 0.181286\n",
            "[Epoch 290] Training Loss: 0.277890 | Val Loss: 0.181701\n",
            "[Epoch 300] Training Loss: 0.176435 | Val Loss: 0.179996\n",
            "[Epoch 310] Training Loss: 0.270231 | Val Loss: 0.182824\n",
            "[Epoch 320] Training Loss: 0.160485 | Val Loss: 0.180145\n",
            "[Epoch 330] Training Loss: 0.099351 | Val Loss: 0.182558\n",
            "[Epoch 340] Training Loss: 0.145320 | Val Loss: 0.184637\n",
            "[Epoch 350] Training Loss: 0.109160 | Val Loss: 0.181628\n",
            "[Epoch 360] Training Loss: 0.208614 | Val Loss: 0.190679\n",
            "[Epoch 370] Training Loss: 0.128647 | Val Loss: 0.187127\n",
            "[Epoch 380] Training Loss: 0.138668 | Val Loss: 0.182144\n",
            "[Epoch 390] Training Loss: 0.244349 | Val Loss: 0.180076\n",
            "[Epoch 400] Training Loss: 0.161325 | Val Loss: 0.180267\n",
            "[Epoch 410] Training Loss: 0.170622 | Val Loss: 0.179857\n",
            "[Epoch 420] Training Loss: 0.113342 | Val Loss: 0.183829\n",
            "[Epoch 430] Training Loss: 0.150992 | Val Loss: 0.194486\n",
            "[Epoch 440] Training Loss: 0.168362 | Val Loss: 0.180464\n",
            "[Epoch 450] Training Loss: 0.205638 | Val Loss: 0.179863\n",
            "[Epoch 460] Training Loss: 0.240210 | Val Loss: 0.185080\n",
            "[Epoch 470] Training Loss: 0.106904 | Val Loss: 0.179907\n",
            "[Epoch 480] Training Loss: 0.166365 | Val Loss: 0.179975\n",
            "[Epoch 490] Training Loss: 0.153643 | Val Loss: 0.182816\n",
            "[Epoch 500] Training Loss: 0.090555 | Val Loss: 0.183110\n",
            "Final Test MSE for LSTM (40_60): 0.147135\n",
            "\n",
            "====== Split: 20_80 | X (input): 20 AND Y (output): 81 ======\n",
            "Train X shape: torch.Size([2162, 20, 1]), Train Y shape: torch.Size([2162, 81]), Static: torch.Size([2162, 14])\n",
            "Test  X shape: torch.Size([541, 20, 1]), Test  Y shape: torch.Size([541, 81]), Static: torch.Size([541, 14])\n",
            "Training started...\n",
            "[Epoch 10] Training Loss: 0.236163 | Val Loss: 0.182870\n",
            "[Epoch 20] Training Loss: 0.149765 | Val Loss: 0.182644\n",
            "[Epoch 30] Training Loss: 0.179281 | Val Loss: 0.185740\n",
            "[Epoch 40] Training Loss: 0.137037 | Val Loss: 0.185792\n",
            "[Epoch 50] Training Loss: 0.227783 | Val Loss: 0.187598\n",
            "[Epoch 60] Training Loss: 0.116878 | Val Loss: 0.180411\n",
            "[Epoch 70] Training Loss: 0.156524 | Val Loss: 0.178401\n",
            "[Epoch 80] Training Loss: 0.146130 | Val Loss: 0.180396\n",
            "[Epoch 90] Training Loss: 0.160742 | Val Loss: 0.189302\n",
            "[Epoch 100] Training Loss: 0.267661 | Val Loss: 0.180084\n",
            "[Epoch 110] Training Loss: 0.235908 | Val Loss: 0.193355\n",
            "[Epoch 120] Training Loss: 0.135111 | Val Loss: 0.225386\n",
            "[Epoch 130] Training Loss: 0.208993 | Val Loss: 0.183016\n",
            "[Epoch 140] Training Loss: 0.148236 | Val Loss: 0.184246\n",
            "[Epoch 150] Training Loss: 0.137144 | Val Loss: 0.219365\n",
            "[Epoch 160] Training Loss: 0.162060 | Val Loss: 0.189279\n",
            "[Epoch 170] Training Loss: 0.145946 | Val Loss: 0.178807\n",
            "[Epoch 180] Training Loss: 0.152611 | Val Loss: 0.181601\n",
            "[Epoch 190] Training Loss: 0.153994 | Val Loss: 0.198997\n",
            "[Epoch 200] Training Loss: 0.166053 | Val Loss: 0.193186\n",
            "[Epoch 210] Training Loss: 0.147969 | Val Loss: 0.178967\n",
            "[Epoch 220] Training Loss: 0.165523 | Val Loss: 0.188057\n",
            "[Epoch 230] Training Loss: 0.207770 | Val Loss: 0.189953\n",
            "[Epoch 240] Training Loss: 0.121162 | Val Loss: 0.182931\n",
            "[Epoch 250] Training Loss: 0.114927 | Val Loss: 0.182007\n",
            "[Epoch 260] Training Loss: 0.091437 | Val Loss: 0.178963\n",
            "[Epoch 270] Training Loss: 0.152170 | Val Loss: 0.180986\n",
            "[Epoch 280] Training Loss: 0.088058 | Val Loss: 0.178394\n",
            "[Epoch 290] Training Loss: 0.135246 | Val Loss: 0.178684\n",
            "[Epoch 300] Training Loss: 0.140059 | Val Loss: 0.185214\n",
            "[Epoch 310] Training Loss: 0.130816 | Val Loss: 0.179151\n",
            "[Epoch 320] Training Loss: 0.139881 | Val Loss: 0.178562\n",
            "[Epoch 330] Training Loss: 0.163008 | Val Loss: 0.179730\n",
            "[Epoch 340] Training Loss: 0.124598 | Val Loss: 0.180007\n",
            "[Epoch 350] Training Loss: 0.095193 | Val Loss: 0.185537\n",
            "[Epoch 360] Training Loss: 0.247224 | Val Loss: 0.178852\n",
            "[Epoch 370] Training Loss: 0.182558 | Val Loss: 0.179045\n",
            "[Epoch 380] Training Loss: 0.160441 | Val Loss: 0.182226\n",
            "[Epoch 390] Training Loss: 0.126716 | Val Loss: 0.188925\n",
            "[Epoch 400] Training Loss: 0.144945 | Val Loss: 0.180864\n",
            "[Epoch 410] Training Loss: 0.118208 | Val Loss: 0.178398\n",
            "[Epoch 420] Training Loss: 0.148328 | Val Loss: 0.179062\n",
            "[Epoch 430] Training Loss: 0.143791 | Val Loss: 0.179510\n",
            "[Epoch 440] Training Loss: 0.109832 | Val Loss: 0.179257\n",
            "[Epoch 450] Training Loss: 0.119045 | Val Loss: 0.178388\n",
            "[Epoch 460] Training Loss: 0.134593 | Val Loss: 0.181490\n",
            "[Epoch 470] Training Loss: 0.195368 | Val Loss: 0.178469\n",
            "[Epoch 480] Training Loss: 0.130776 | Val Loss: 0.180971\n",
            "[Epoch 490] Training Loss: 0.145865 | Val Loss: 0.178336\n",
            "[Epoch 500] Training Loss: 0.209052 | Val Loss: 0.178594\n",
            "Final Test MSE for LSTM (20_80): 0.145206\n",
            "\n",
            "====== Split: 10_90 | X (input): 10 AND Y (output): 91 ======\n",
            "Train X shape: torch.Size([2162, 10, 1]), Train Y shape: torch.Size([2162, 91]), Static: torch.Size([2162, 14])\n",
            "Test  X shape: torch.Size([541, 10, 1]), Test  Y shape: torch.Size([541, 91]), Static: torch.Size([541, 14])\n",
            "Training started...\n",
            "[Epoch 10] Training Loss: 0.116431 | Val Loss: 0.181869\n",
            "[Epoch 20] Training Loss: 0.156683 | Val Loss: 0.183059\n",
            "[Epoch 30] Training Loss: 0.143229 | Val Loss: 0.193210\n",
            "[Epoch 40] Training Loss: 0.099567 | Val Loss: 0.181508\n",
            "[Epoch 50] Training Loss: 0.116972 | Val Loss: 0.197156\n",
            "[Epoch 60] Training Loss: 0.092787 | Val Loss: 0.187728\n",
            "[Epoch 70] Training Loss: 0.161602 | Val Loss: 0.183011\n",
            "[Epoch 80] Training Loss: 0.186237 | Val Loss: 0.187852\n",
            "[Epoch 90] Training Loss: 0.142693 | Val Loss: 0.184020\n",
            "[Epoch 100] Training Loss: 0.099944 | Val Loss: 0.179754\n",
            "[Epoch 110] Training Loss: 0.239075 | Val Loss: 0.180362\n",
            "[Epoch 120] Training Loss: 0.144622 | Val Loss: 0.179919\n",
            "[Epoch 130] Training Loss: 0.169141 | Val Loss: 0.184004\n",
            "[Epoch 140] Training Loss: 0.173837 | Val Loss: 0.181222\n",
            "[Epoch 150] Training Loss: 0.150496 | Val Loss: 0.179797\n",
            "[Epoch 160] Training Loss: 0.105810 | Val Loss: 0.181065\n",
            "[Epoch 170] Training Loss: 0.147605 | Val Loss: 0.179886\n",
            "[Epoch 180] Training Loss: 0.141076 | Val Loss: 0.188728\n",
            "[Epoch 190] Training Loss: 0.149107 | Val Loss: 0.180831\n",
            "[Epoch 200] Training Loss: 0.131362 | Val Loss: 0.181594\n",
            "[Epoch 210] Training Loss: 0.190946 | Val Loss: 0.184167\n",
            "[Epoch 220] Training Loss: 0.095245 | Val Loss: 0.179719\n",
            "[Epoch 230] Training Loss: 0.167342 | Val Loss: 0.182214\n",
            "[Epoch 240] Training Loss: 0.250771 | Val Loss: 0.182918\n",
            "[Epoch 250] Training Loss: 0.219967 | Val Loss: 0.182058\n",
            "[Epoch 260] Training Loss: 0.162550 | Val Loss: 0.179825\n",
            "[Epoch 270] Training Loss: 0.174156 | Val Loss: 0.179871\n",
            "[Epoch 280] Training Loss: 0.140301 | Val Loss: 0.182853\n",
            "[Epoch 290] Training Loss: 0.166705 | Val Loss: 0.180474\n",
            "[Epoch 300] Training Loss: 0.195281 | Val Loss: 0.181055\n",
            "[Epoch 310] Training Loss: 0.162441 | Val Loss: 0.181196\n",
            "[Epoch 320] Training Loss: 0.132609 | Val Loss: 0.182533\n",
            "[Epoch 330] Training Loss: 0.206256 | Val Loss: 0.183536\n",
            "[Epoch 340] Training Loss: 0.160386 | Val Loss: 0.179993\n",
            "[Epoch 350] Training Loss: 0.156253 | Val Loss: 0.179688\n",
            "[Epoch 360] Training Loss: 0.123260 | Val Loss: 0.184409\n",
            "[Epoch 370] Training Loss: 0.152177 | Val Loss: 0.180369\n",
            "[Epoch 380] Training Loss: 0.116192 | Val Loss: 0.179998\n",
            "[Epoch 390] Training Loss: 0.222615 | Val Loss: 0.180120\n",
            "[Epoch 400] Training Loss: 0.207683 | Val Loss: 0.187137\n",
            "[Epoch 410] Training Loss: 0.090277 | Val Loss: 0.192857\n",
            "[Epoch 420] Training Loss: 0.130514 | Val Loss: 0.184663\n",
            "[Epoch 430] Training Loss: 0.146519 | Val Loss: 0.182651\n",
            "[Epoch 440] Training Loss: 0.176085 | Val Loss: 0.180230\n",
            "[Epoch 450] Training Loss: 0.101754 | Val Loss: 0.182593\n",
            "[Epoch 460] Training Loss: 0.143481 | Val Loss: 0.176709\n",
            "[Epoch 470] Training Loss: 0.220244 | Val Loss: 0.180593\n",
            "[Epoch 480] Training Loss: 0.157272 | Val Loss: 0.175555\n",
            "[Epoch 490] Training Loss: 0.186582 | Val Loss: 0.174193\n",
            "[Epoch 500] Training Loss: 0.131061 | Val Loss: 0.177295\n",
            "Final Test MSE for LSTM (10_90): 0.144972\n",
            "\n",
            "====== Split: 5_95 | X (input): 5 AND Y (output): 96 ======\n",
            "Train X shape: torch.Size([2162, 5, 1]), Train Y shape: torch.Size([2162, 96]), Static: torch.Size([2162, 14])\n",
            "Test  X shape: torch.Size([541, 5, 1]), Test  Y shape: torch.Size([541, 96]), Static: torch.Size([541, 14])\n",
            "Training started...\n",
            "[Epoch 10] Training Loss: 0.189218 | Val Loss: 0.187077\n",
            "[Epoch 20] Training Loss: 0.179196 | Val Loss: 0.184753\n",
            "[Epoch 30] Training Loss: 0.165638 | Val Loss: 0.185648\n",
            "[Epoch 40] Training Loss: 0.161949 | Val Loss: 0.189279\n",
            "[Epoch 50] Training Loss: 0.135924 | Val Loss: 0.189129\n",
            "[Epoch 60] Training Loss: 0.121838 | Val Loss: 0.187025\n",
            "[Epoch 70] Training Loss: 0.139179 | Val Loss: 0.186013\n",
            "[Epoch 80] Training Loss: 0.157256 | Val Loss: 0.183428\n",
            "[Epoch 90] Training Loss: 0.152110 | Val Loss: 0.210112\n",
            "[Epoch 100] Training Loss: 0.112753 | Val Loss: 0.182255\n",
            "[Epoch 110] Training Loss: 0.187806 | Val Loss: 0.182239\n",
            "[Epoch 120] Training Loss: 0.122418 | Val Loss: 0.181600\n",
            "[Epoch 130] Training Loss: 0.099032 | Val Loss: 0.184785\n",
            "[Epoch 140] Training Loss: 0.185445 | Val Loss: 0.178883\n",
            "[Epoch 150] Training Loss: 0.155949 | Val Loss: 0.180888\n",
            "[Epoch 160] Training Loss: 0.158681 | Val Loss: 0.189388\n",
            "[Epoch 170] Training Loss: 0.104062 | Val Loss: 0.185196\n",
            "[Epoch 180] Training Loss: 0.130726 | Val Loss: 0.187882\n",
            "[Epoch 190] Training Loss: 0.177086 | Val Loss: 0.183010\n",
            "[Epoch 200] Training Loss: 0.101176 | Val Loss: 0.195844\n",
            "[Epoch 210] Training Loss: 0.242358 | Val Loss: 0.178867\n",
            "[Epoch 220] Training Loss: 0.154287 | Val Loss: 0.199103\n",
            "[Epoch 230] Training Loss: 0.113690 | Val Loss: 0.178637\n",
            "[Epoch 240] Training Loss: 0.154624 | Val Loss: 0.185216\n",
            "[Epoch 250] Training Loss: 0.142369 | Val Loss: 0.182816\n",
            "[Epoch 260] Training Loss: 0.190177 | Val Loss: 0.186048\n",
            "[Epoch 270] Training Loss: 0.144620 | Val Loss: 0.179382\n",
            "[Epoch 280] Training Loss: 0.151122 | Val Loss: 0.182574\n",
            "[Epoch 290] Training Loss: 0.210915 | Val Loss: 0.181716\n",
            "[Epoch 300] Training Loss: 0.100267 | Val Loss: 0.180015\n",
            "[Epoch 310] Training Loss: 0.136663 | Val Loss: 0.177422\n",
            "[Epoch 320] Training Loss: 0.115312 | Val Loss: 0.177744\n",
            "[Epoch 330] Training Loss: 0.192332 | Val Loss: 0.178981\n",
            "[Epoch 340] Training Loss: 0.173358 | Val Loss: 0.178269\n",
            "[Epoch 350] Training Loss: 0.231273 | Val Loss: 0.179339\n",
            "[Epoch 360] Training Loss: 0.133418 | Val Loss: 0.176871\n",
            "[Epoch 370] Training Loss: 0.198401 | Val Loss: 0.176772\n",
            "[Epoch 380] Training Loss: 0.122398 | Val Loss: 0.179653\n",
            "[Epoch 390] Training Loss: 0.228964 | Val Loss: 0.177573\n",
            "[Epoch 400] Training Loss: 0.093180 | Val Loss: 0.189760\n",
            "[Epoch 410] Training Loss: 0.138303 | Val Loss: 0.181257\n",
            "[Epoch 420] Training Loss: 0.132086 | Val Loss: 0.180407\n",
            "[Epoch 430] Training Loss: 0.144400 | Val Loss: 0.185790\n",
            "[Epoch 440] Training Loss: 0.156230 | Val Loss: 0.179088\n",
            "[Epoch 450] Training Loss: 0.143470 | Val Loss: 0.183984\n",
            "[Epoch 460] Training Loss: 0.147259 | Val Loss: 0.173259\n",
            "[Epoch 470] Training Loss: 0.151928 | Val Loss: 0.179952\n",
            "[Epoch 480] Training Loss: 0.160316 | Val Loss: 0.181429\n",
            "[Epoch 490] Training Loss: 0.158209 | Val Loss: 0.185748\n",
            "[Epoch 500] Training Loss: 0.166888 | Val Loss: 0.179615\n",
            "Final Test MSE for LSTM (5_95): 0.146895\n",
            "\n",
            "====== Split: 3_97 | X (input): 3 AND Y (output): 98 ======\n",
            "Train X shape: torch.Size([2162, 3, 1]), Train Y shape: torch.Size([2162, 98]), Static: torch.Size([2162, 14])\n",
            "Test  X shape: torch.Size([541, 3, 1]), Test  Y shape: torch.Size([541, 98]), Static: torch.Size([541, 14])\n",
            "Training started...\n",
            "[Epoch 10] Training Loss: 0.144494 | Val Loss: 0.190059\n",
            "[Epoch 20] Training Loss: 0.161226 | Val Loss: 0.201130\n",
            "[Epoch 30] Training Loss: 0.153759 | Val Loss: 0.190813\n",
            "[Epoch 40] Training Loss: 0.205179 | Val Loss: 0.200772\n",
            "[Epoch 50] Training Loss: 0.140360 | Val Loss: 0.187171\n",
            "[Epoch 60] Training Loss: 0.121795 | Val Loss: 0.187208\n",
            "[Epoch 70] Training Loss: 0.138902 | Val Loss: 0.216713\n",
            "[Epoch 80] Training Loss: 0.193794 | Val Loss: 0.187172\n",
            "[Epoch 90] Training Loss: 0.118526 | Val Loss: 0.193485\n",
            "[Epoch 100] Training Loss: 0.202818 | Val Loss: 0.188667\n",
            "[Epoch 110] Training Loss: 0.139429 | Val Loss: 0.187495\n",
            "[Epoch 120] Training Loss: 0.176946 | Val Loss: 0.194496\n",
            "[Epoch 130] Training Loss: 0.142994 | Val Loss: 0.191036\n",
            "[Epoch 140] Training Loss: 0.202925 | Val Loss: 0.188029\n",
            "[Epoch 150] Training Loss: 0.105997 | Val Loss: 0.190551\n",
            "[Epoch 160] Training Loss: 0.184648 | Val Loss: 0.182404\n",
            "[Epoch 170] Training Loss: 0.212688 | Val Loss: 0.188710\n",
            "[Epoch 180] Training Loss: 0.158958 | Val Loss: 0.180706\n",
            "[Epoch 190] Training Loss: 0.160017 | Val Loss: 0.186192\n",
            "[Epoch 200] Training Loss: 0.129904 | Val Loss: 0.179186\n",
            "[Epoch 210] Training Loss: 0.088654 | Val Loss: 0.186718\n",
            "[Epoch 220] Training Loss: 0.154724 | Val Loss: 0.178726\n",
            "[Epoch 230] Training Loss: 0.112049 | Val Loss: 0.184311\n",
            "[Epoch 240] Training Loss: 0.247466 | Val Loss: 0.179971\n",
            "[Epoch 250] Training Loss: 0.200469 | Val Loss: 0.183628\n",
            "[Epoch 260] Training Loss: 0.240643 | Val Loss: 0.180540\n",
            "[Epoch 270] Training Loss: 0.140576 | Val Loss: 0.180084\n",
            "[Epoch 280] Training Loss: 0.162913 | Val Loss: 0.181688\n",
            "[Epoch 290] Training Loss: 0.192014 | Val Loss: 0.178738\n",
            "[Epoch 300] Training Loss: 0.126706 | Val Loss: 0.174913\n",
            "[Epoch 310] Training Loss: 0.075512 | Val Loss: 0.141782\n",
            "[Epoch 320] Training Loss: 0.113797 | Val Loss: 0.140214\n",
            "[Epoch 330] Training Loss: 0.050871 | Val Loss: 0.131360\n",
            "[Epoch 340] Training Loss: 0.129410 | Val Loss: 0.160663\n",
            "[Epoch 350] Training Loss: 0.103549 | Val Loss: 0.120884\n",
            "[Epoch 360] Training Loss: 0.106299 | Val Loss: 0.109749\n",
            "[Epoch 370] Training Loss: 0.056649 | Val Loss: 0.097521\n",
            "[Epoch 380] Training Loss: 0.067017 | Val Loss: 0.088649\n",
            "[Epoch 390] Training Loss: 0.020470 | Val Loss: 0.074551\n",
            "[Epoch 400] Training Loss: 0.023809 | Val Loss: 0.067591\n",
            "[Epoch 410] Training Loss: 0.031638 | Val Loss: 0.074786\n",
            "[Epoch 420] Training Loss: 0.025322 | Val Loss: 0.071982\n",
            "[Epoch 430] Training Loss: 0.017775 | Val Loss: 0.063785\n",
            "[Epoch 440] Training Loss: 0.014443 | Val Loss: 0.059788\n",
            "[Epoch 450] Training Loss: 0.013015 | Val Loss: 0.059740\n",
            "[Epoch 460] Training Loss: 0.021161 | Val Loss: 0.058382\n",
            "[Epoch 470] Training Loss: 0.016909 | Val Loss: 0.057417\n",
            "[Epoch 480] Training Loss: 0.023800 | Val Loss: 0.058782\n",
            "[Epoch 490] Training Loss: 0.020392 | Val Loss: 0.057857\n",
            "[Epoch 500] Training Loss: 0.014351 | Val Loss: 0.061552\n",
            "Final Test MSE for LSTM (3_97): 0.049812\n",
            "\n",
            "====== Split: 1_99 | X (input): 1 AND Y (output): 100 ======\n",
            "Train X shape: torch.Size([2162, 1, 1]), Train Y shape: torch.Size([2162, 100]), Static: torch.Size([2162, 14])\n",
            "Test  X shape: torch.Size([541, 1, 1]), Test  Y shape: torch.Size([541, 100]), Static: torch.Size([541, 14])\n",
            "Training started...\n",
            "[Epoch 10] Training Loss: 0.116860 | Val Loss: 0.188274\n",
            "[Epoch 20] Training Loss: 0.113143 | Val Loss: 0.188562\n",
            "[Epoch 30] Training Loss: 0.142519 | Val Loss: 0.195624\n",
            "[Epoch 40] Training Loss: 0.171004 | Val Loss: 0.188342\n",
            "[Epoch 50] Training Loss: 0.167474 | Val Loss: 0.191169\n",
            "[Epoch 60] Training Loss: 0.160417 | Val Loss: 0.189844\n",
            "[Epoch 70] Training Loss: 0.165786 | Val Loss: 0.192626\n",
            "[Epoch 80] Training Loss: 0.188553 | Val Loss: 0.190849\n",
            "[Epoch 90] Training Loss: 0.186907 | Val Loss: 0.188358\n",
            "[Epoch 100] Training Loss: 0.109540 | Val Loss: 0.188279\n",
            "[Epoch 110] Training Loss: 0.143351 | Val Loss: 0.190810\n",
            "[Epoch 120] Training Loss: 0.198583 | Val Loss: 0.198118\n",
            "[Epoch 130] Training Loss: 0.103133 | Val Loss: 0.194054\n",
            "[Epoch 140] Training Loss: 0.161425 | Val Loss: 0.195268\n",
            "[Epoch 150] Training Loss: 0.163176 | Val Loss: 0.189909\n",
            "[Epoch 160] Training Loss: 0.157965 | Val Loss: 0.188229\n",
            "[Epoch 170] Training Loss: 0.148694 | Val Loss: 0.189085\n",
            "[Epoch 180] Training Loss: 0.162572 | Val Loss: 0.203044\n",
            "[Epoch 190] Training Loss: 0.213274 | Val Loss: 0.186964\n",
            "[Epoch 200] Training Loss: 0.160518 | Val Loss: 0.183694\n",
            "[Epoch 210] Training Loss: 0.190710 | Val Loss: 0.184658\n",
            "[Epoch 220] Training Loss: 0.149080 | Val Loss: 0.184391\n",
            "[Epoch 230] Training Loss: 0.181158 | Val Loss: 0.183608\n",
            "[Epoch 240] Training Loss: 0.165773 | Val Loss: 0.182125\n",
            "[Epoch 250] Training Loss: 0.145016 | Val Loss: 0.183694\n",
            "[Epoch 260] Training Loss: 0.162377 | Val Loss: 0.183576\n",
            "[Epoch 270] Training Loss: 0.156157 | Val Loss: 0.190559\n",
            "[Epoch 280] Training Loss: 0.127549 | Val Loss: 0.183702\n",
            "[Epoch 290] Training Loss: 0.200646 | Val Loss: 0.184760\n",
            "[Epoch 300] Training Loss: 0.120397 | Val Loss: 0.180370\n",
            "[Epoch 310] Training Loss: 0.269307 | Val Loss: 0.182393\n",
            "[Epoch 320] Training Loss: 0.135421 | Val Loss: 0.183924\n",
            "[Epoch 330] Training Loss: 0.131648 | Val Loss: 0.201629\n",
            "[Epoch 340] Training Loss: 0.117566 | Val Loss: 0.183630\n",
            "[Epoch 350] Training Loss: 0.209190 | Val Loss: 0.181393\n",
            "[Epoch 360] Training Loss: 0.245244 | Val Loss: 0.187389\n",
            "[Epoch 370] Training Loss: 0.147197 | Val Loss: 0.181285\n",
            "[Epoch 380] Training Loss: 0.141871 | Val Loss: 0.186195\n",
            "[Epoch 390] Training Loss: 0.134055 | Val Loss: 0.187624\n",
            "[Epoch 400] Training Loss: 0.142117 | Val Loss: 0.183019\n",
            "[Epoch 410] Training Loss: 0.216182 | Val Loss: 0.183364\n",
            "[Epoch 420] Training Loss: 0.146856 | Val Loss: 0.184392\n",
            "[Epoch 430] Training Loss: 0.163456 | Val Loss: 0.184942\n",
            "[Epoch 440] Training Loss: 0.184299 | Val Loss: 0.184286\n",
            "[Epoch 450] Training Loss: 0.149814 | Val Loss: 0.182502\n",
            "[Epoch 460] Training Loss: 0.213774 | Val Loss: 0.183444\n",
            "[Epoch 470] Training Loss: 0.125725 | Val Loss: 0.183542\n",
            "[Epoch 480] Training Loss: 0.147876 | Val Loss: 0.183555\n",
            "[Epoch 490] Training Loss: 0.165399 | Val Loss: 0.183459\n",
            "[Epoch 500] Training Loss: 0.161522 | Val Loss: 0.183192\n",
            "Final Test MSE for LSTM (1_99): 0.152032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**New TCN model**"
      ],
      "metadata": {
        "id": "hIIgAWRw8o_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# ========== TCN-MIMO Hyperparameters ==========\n",
        "tcn_mimo_config = {\n",
        "    'num_channels': [64, 64, 64],\n",
        "    'kernel_size': 3,\n",
        "    'dropout': 0.2,\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 500,\n",
        "    'model_name': 'TCN_MIMO'\n",
        "}\n",
        "\n",
        "# ========== MSE Storage ==========\n",
        "TCN_MIMO_mse = pd.DataFrame(columns=['Split', 'Test_MSE'])\n",
        "\n",
        "# ========== Model Save Path ==========\n",
        "drive_path = '/content/drive/MyDrive/DSSM-Figures'\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "# ========== Define Splits ==========\n",
        "splits = [(80, 20), (60, 40), (50, 50), (40, 60), (20, 80), (10, 90), (5, 95), (3, 97), (1, 99)]\n",
        "\n",
        "# ========== Seed ==========\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# ========== Main Loop ==========\n",
        "for train_pct, test_pct in splits:\n",
        "    split_name = f\"{train_pct}_{test_pct}\"\n",
        "    train_ids, test_ids = train_test_split(df_output['file_id'].unique(), test_size=0.2, random_state=42)\n",
        "    df_train = df_output[df_output['file_id'].isin(train_ids)]\n",
        "    df_val = df_output[df_output['file_id'].isin(test_ids[:len(test_ids)//2])]\n",
        "    df_test = df_output[df_output['file_id'].isin(test_ids[len(test_ids)//2:])]\n",
        "\n",
        "    train_timestep = int(train_pct / 100 * 101)\n",
        "    pred_timestep = 101 - train_timestep\n",
        "    print(f\"\\n====== Split: {split_name} | X (input): {train_timestep} AND Y (output): {pred_timestep} ======\")\n",
        "\n",
        "    def get_xy_static(df, ids):\n",
        "        df_ = df[df['file_id'].isin(ids)]\n",
        "        X = df_.pivot(index='file_id', columns='timestep', values='CO2').values[:, :train_timestep]\n",
        "        Y = df_.pivot(index='file_id', columns='timestep', values='CO2').values[:, train_timestep:]\n",
        "        static = merged_df[merged_df['file_id'].isin(ids)].drop(columns=['file_id', 'cluster']).values\n",
        "        return X[:, :, None], Y, static\n",
        "\n",
        "    X_train, Y_train, static_train = get_xy_static(df_train, train_ids)\n",
        "    X_val, Y_val, static_val = get_xy_static(df_val, test_ids[:len(test_ids)//2])\n",
        "    X_test, Y_test, static_test = get_xy_static(df_test, test_ids[len(test_ids)//2:])\n",
        "\n",
        "    # Tensor conversion\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
        "    static_train_tensor = torch.tensor(static_train, dtype=torch.float32)\n",
        "\n",
        "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "    Y_val_tensor = torch.tensor(Y_val, dtype=torch.float32)\n",
        "    static_val_tensor = torch.tensor(static_val, dtype=torch.float32)\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
        "    static_test_tensor = torch.tensor(static_test, dtype=torch.float32)\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(TensorDataset(X_train_tensor, static_train_tensor, Y_train_tensor), batch_size=tcn_mimo_config['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(TensorDataset(X_val_tensor, static_val_tensor, Y_val_tensor), batch_size=tcn_mimo_config['batch_size'])\n",
        "    test_loader = DataLoader(TensorDataset(X_test_tensor, static_test_tensor, Y_test_tensor), batch_size=tcn_mimo_config['batch_size'])\n",
        "\n",
        "    # Model\n",
        "    model = TCN_MIMO(\n",
        "        input_len=train_timestep,\n",
        "        output_len=pred_timestep,\n",
        "        static_dim=static_train.shape[1],\n",
        "        num_channels=tcn_mimo_config['num_channels'],\n",
        "        kernel_size=tcn_mimo_config['kernel_size'],\n",
        "        dropout=tcn_mimo_config['dropout']\n",
        "    )\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=tcn_mimo_config['learning_rate'])\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_path = os.path.join(drive_path, f\"best_model_TCN_MIMO_{split_name}.pt\")\n",
        "\n",
        "    print(\"Training started...\")\n",
        "    for epoch in range(tcn_mimo_config['epochs']):\n",
        "        model.train()\n",
        "        for X_batch, static_batch, Y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X_batch, static_batch)\n",
        "            loss = criterion(preds, Y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, static_batch, Y_batch in val_loader:\n",
        "                preds = model(X_batch, static_batch)\n",
        "                val_loss += criterion(preds, Y_batch).item()\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}: Train Loss = {loss.item():.6f}, Val Loss = {val_loss:.6f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            #print(f\"Best model updated at epoch {epoch+1} with val loss {val_loss:.6f}\")\n",
        "\n",
        "    # Load and evaluate\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "    model.eval()\n",
        "    total_mse = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, static_batch, Y_batch in test_loader:\n",
        "            outputs = model(X_batch, static_batch)\n",
        "            batch_mse = criterion(outputs, Y_batch).item()\n",
        "            total_mse += batch_mse * X_batch.size(0)\n",
        "            total_samples += X_batch.size(0)\n",
        "\n",
        "    avg_mse = total_mse / total_samples\n",
        "    print(f\"✅ TCN_MIMO Final Test MSE for split {split_name}: {avg_mse:.6f}\")\n",
        "    TCN_MIMO_mse.loc[len(TCN_MIMO_mse)] = [split_name, avg_mse]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaiU-aY3Fquj",
        "outputId": "476138af-d7b6-44ca-959e-e36f6fe22fb7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====== Split: 80_20 | X (input): 80 AND Y (output): 21 ======\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.008023, Val Loss = 0.006117\n",
            "Epoch 20: Train Loss = 0.003023, Val Loss = 0.002582\n",
            "Epoch 30: Train Loss = 0.002007, Val Loss = 0.001840\n",
            "Epoch 40: Train Loss = 0.003558, Val Loss = 0.004436\n",
            "Epoch 50: Train Loss = 0.002107, Val Loss = 0.001406\n",
            "Epoch 60: Train Loss = 0.001418, Val Loss = 0.001573\n",
            "Epoch 70: Train Loss = 0.003597, Val Loss = 0.001043\n",
            "Epoch 80: Train Loss = 0.000921, Val Loss = 0.000836\n",
            "Epoch 90: Train Loss = 0.002337, Val Loss = 0.001455\n",
            "Epoch 100: Train Loss = 0.000890, Val Loss = 0.000964\n",
            "Epoch 110: Train Loss = 0.002451, Val Loss = 0.003616\n",
            "Epoch 120: Train Loss = 0.000463, Val Loss = 0.001024\n",
            "Epoch 130: Train Loss = 0.001080, Val Loss = 0.000867\n",
            "Epoch 140: Train Loss = 0.001432, Val Loss = 0.000715\n",
            "Epoch 150: Train Loss = 0.000584, Val Loss = 0.000574\n",
            "Epoch 160: Train Loss = 0.000661, Val Loss = 0.000500\n",
            "Epoch 170: Train Loss = 0.000351, Val Loss = 0.000496\n",
            "Epoch 180: Train Loss = 0.000488, Val Loss = 0.000353\n",
            "Epoch 190: Train Loss = 0.001167, Val Loss = 0.000696\n",
            "Epoch 200: Train Loss = 0.000761, Val Loss = 0.000695\n",
            "Epoch 210: Train Loss = 0.000469, Val Loss = 0.000586\n",
            "Epoch 220: Train Loss = 0.000730, Val Loss = 0.000667\n",
            "Epoch 230: Train Loss = 0.000775, Val Loss = 0.000628\n",
            "Epoch 240: Train Loss = 0.000318, Val Loss = 0.000315\n",
            "Epoch 250: Train Loss = 0.000459, Val Loss = 0.000490\n",
            "Epoch 260: Train Loss = 0.000398, Val Loss = 0.000423\n",
            "Epoch 270: Train Loss = 0.000350, Val Loss = 0.000326\n",
            "Epoch 280: Train Loss = 0.000367, Val Loss = 0.000273\n",
            "Epoch 290: Train Loss = 0.000394, Val Loss = 0.000412\n",
            "Epoch 300: Train Loss = 0.000233, Val Loss = 0.000215\n",
            "Epoch 310: Train Loss = 0.000205, Val Loss = 0.000198\n",
            "Epoch 320: Train Loss = 0.000431, Val Loss = 0.000181\n",
            "Epoch 330: Train Loss = 0.000432, Val Loss = 0.000237\n",
            "Epoch 340: Train Loss = 0.000180, Val Loss = 0.000148\n",
            "Epoch 350: Train Loss = 0.000293, Val Loss = 0.000229\n",
            "Epoch 360: Train Loss = 0.000142, Val Loss = 0.000163\n",
            "Epoch 370: Train Loss = 0.000109, Val Loss = 0.000195\n",
            "Epoch 380: Train Loss = 0.000775, Val Loss = 0.000424\n",
            "Epoch 390: Train Loss = 0.000302, Val Loss = 0.000117\n",
            "Epoch 400: Train Loss = 0.000149, Val Loss = 0.000118\n",
            "Epoch 410: Train Loss = 0.000200, Val Loss = 0.000263\n",
            "Epoch 420: Train Loss = 0.000125, Val Loss = 0.000138\n",
            "Epoch 430: Train Loss = 0.002254, Val Loss = 0.001807\n",
            "Epoch 440: Train Loss = 0.000230, Val Loss = 0.000168\n",
            "Epoch 450: Train Loss = 0.000192, Val Loss = 0.000108\n",
            "Epoch 460: Train Loss = 0.000490, Val Loss = 0.000427\n",
            "Epoch 470: Train Loss = 0.000134, Val Loss = 0.000112\n",
            "Epoch 480: Train Loss = 0.000202, Val Loss = 0.000306\n",
            "Epoch 490: Train Loss = 0.000140, Val Loss = 0.000123\n",
            "Epoch 500: Train Loss = 0.000109, Val Loss = 0.000171\n",
            "✅ TCN_MIMO Final Test MSE for split 80_20: 0.000067\n",
            "\n",
            "====== Split: 60_40 | X (input): 60 AND Y (output): 41 ======\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.013119, Val Loss = 0.007513\n",
            "Epoch 20: Train Loss = 0.006278, Val Loss = 0.004638\n",
            "Epoch 30: Train Loss = 0.007873, Val Loss = 0.003533\n",
            "Epoch 40: Train Loss = 0.003147, Val Loss = 0.003591\n",
            "Epoch 50: Train Loss = 0.001721, Val Loss = 0.001979\n",
            "Epoch 60: Train Loss = 0.002613, Val Loss = 0.001662\n",
            "Epoch 70: Train Loss = 0.002026, Val Loss = 0.002664\n",
            "Epoch 80: Train Loss = 0.001674, Val Loss = 0.001410\n",
            "Epoch 90: Train Loss = 0.001400, Val Loss = 0.001024\n",
            "Epoch 100: Train Loss = 0.001477, Val Loss = 0.001710\n",
            "Epoch 110: Train Loss = 0.000726, Val Loss = 0.000788\n",
            "Epoch 120: Train Loss = 0.002529, Val Loss = 0.001530\n",
            "Epoch 130: Train Loss = 0.001519, Val Loss = 0.000592\n",
            "Epoch 140: Train Loss = 0.002551, Val Loss = 0.002707\n",
            "Epoch 150: Train Loss = 0.000985, Val Loss = 0.000915\n",
            "Epoch 160: Train Loss = 0.000688, Val Loss = 0.000355\n",
            "Epoch 170: Train Loss = 0.000729, Val Loss = 0.000341\n",
            "Epoch 180: Train Loss = 0.000669, Val Loss = 0.000281\n",
            "Epoch 190: Train Loss = 0.000540, Val Loss = 0.000267\n",
            "Epoch 200: Train Loss = 0.000779, Val Loss = 0.000299\n",
            "Epoch 210: Train Loss = 0.000890, Val Loss = 0.000357\n",
            "Epoch 220: Train Loss = 0.000885, Val Loss = 0.000283\n",
            "Epoch 230: Train Loss = 0.000960, Val Loss = 0.000350\n",
            "Epoch 240: Train Loss = 0.000766, Val Loss = 0.000508\n",
            "Epoch 250: Train Loss = 0.000967, Val Loss = 0.000631\n",
            "Epoch 260: Train Loss = 0.000644, Val Loss = 0.000305\n",
            "Epoch 270: Train Loss = 0.001096, Val Loss = 0.000400\n",
            "Epoch 280: Train Loss = 0.000535, Val Loss = 0.000476\n",
            "Epoch 290: Train Loss = 0.000430, Val Loss = 0.000234\n",
            "Epoch 300: Train Loss = 0.000746, Val Loss = 0.000341\n",
            "Epoch 310: Train Loss = 0.000414, Val Loss = 0.000280\n",
            "Epoch 320: Train Loss = 0.000169, Val Loss = 0.000382\n",
            "Epoch 330: Train Loss = 0.000414, Val Loss = 0.000218\n",
            "Epoch 340: Train Loss = 0.000746, Val Loss = 0.000471\n",
            "Epoch 350: Train Loss = 0.000597, Val Loss = 0.000203\n",
            "Epoch 360: Train Loss = 0.000880, Val Loss = 0.000967\n",
            "Epoch 370: Train Loss = 0.001276, Val Loss = 0.000475\n",
            "Epoch 380: Train Loss = 0.000313, Val Loss = 0.000282\n",
            "Epoch 390: Train Loss = 0.000407, Val Loss = 0.000265\n",
            "Epoch 400: Train Loss = 0.000605, Val Loss = 0.000400\n",
            "Epoch 410: Train Loss = 0.000587, Val Loss = 0.000505\n",
            "Epoch 420: Train Loss = 0.000360, Val Loss = 0.000336\n",
            "Epoch 430: Train Loss = 0.000651, Val Loss = 0.000557\n",
            "Epoch 440: Train Loss = 0.000374, Val Loss = 0.000270\n",
            "Epoch 450: Train Loss = 0.000452, Val Loss = 0.000276\n",
            "Epoch 460: Train Loss = 0.000455, Val Loss = 0.000422\n",
            "Epoch 470: Train Loss = 0.000382, Val Loss = 0.000521\n",
            "Epoch 480: Train Loss = 0.001154, Val Loss = 0.001749\n",
            "Epoch 490: Train Loss = 0.000505, Val Loss = 0.000384\n",
            "Epoch 500: Train Loss = 0.000541, Val Loss = 0.000737\n",
            "✅ TCN_MIMO Final Test MSE for split 60_40: 0.000177\n",
            "\n",
            "====== Split: 50_50 | X (input): 50 AND Y (output): 51 ======\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.014756, Val Loss = 0.012155\n",
            "Epoch 20: Train Loss = 0.015008, Val Loss = 0.007266\n",
            "Epoch 30: Train Loss = 0.006851, Val Loss = 0.006888\n",
            "Epoch 40: Train Loss = 0.012048, Val Loss = 0.006260\n",
            "Epoch 50: Train Loss = 0.010222, Val Loss = 0.003735\n",
            "Epoch 60: Train Loss = 0.004254, Val Loss = 0.003544\n",
            "Epoch 70: Train Loss = 0.002867, Val Loss = 0.005072\n",
            "Epoch 80: Train Loss = 0.003545, Val Loss = 0.002258\n",
            "Epoch 90: Train Loss = 0.001722, Val Loss = 0.001319\n",
            "Epoch 100: Train Loss = 0.002825, Val Loss = 0.002257\n",
            "Epoch 110: Train Loss = 0.001709, Val Loss = 0.002122\n",
            "Epoch 120: Train Loss = 0.002895, Val Loss = 0.002121\n",
            "Epoch 130: Train Loss = 0.002661, Val Loss = 0.003553\n",
            "Epoch 140: Train Loss = 0.001403, Val Loss = 0.001051\n",
            "Epoch 150: Train Loss = 0.001665, Val Loss = 0.001965\n",
            "Epoch 160: Train Loss = 0.001195, Val Loss = 0.000768\n",
            "Epoch 170: Train Loss = 0.001840, Val Loss = 0.000791\n",
            "Epoch 180: Train Loss = 0.000876, Val Loss = 0.000894\n",
            "Epoch 190: Train Loss = 0.001282, Val Loss = 0.001294\n",
            "Epoch 200: Train Loss = 0.000929, Val Loss = 0.000672\n",
            "Epoch 210: Train Loss = 0.001476, Val Loss = 0.000606\n",
            "Epoch 220: Train Loss = 0.001258, Val Loss = 0.000635\n",
            "Epoch 230: Train Loss = 0.003101, Val Loss = 0.000751\n",
            "Epoch 240: Train Loss = 0.000733, Val Loss = 0.000592\n",
            "Epoch 250: Train Loss = 0.000812, Val Loss = 0.001629\n",
            "Epoch 260: Train Loss = 0.001042, Val Loss = 0.000484\n",
            "Epoch 270: Train Loss = 0.000725, Val Loss = 0.000484\n",
            "Epoch 280: Train Loss = 0.000974, Val Loss = 0.000758\n",
            "Epoch 290: Train Loss = 0.000641, Val Loss = 0.000395\n",
            "Epoch 300: Train Loss = 0.000520, Val Loss = 0.000487\n",
            "Epoch 310: Train Loss = 0.000869, Val Loss = 0.000666\n",
            "Epoch 320: Train Loss = 0.001152, Val Loss = 0.000784\n",
            "Epoch 330: Train Loss = 0.000553, Val Loss = 0.000287\n",
            "Epoch 340: Train Loss = 0.000559, Val Loss = 0.000369\n",
            "Epoch 350: Train Loss = 0.003312, Val Loss = 0.001903\n",
            "Epoch 360: Train Loss = 0.000557, Val Loss = 0.000510\n",
            "Epoch 370: Train Loss = 0.001595, Val Loss = 0.000921\n",
            "Epoch 380: Train Loss = 0.000429, Val Loss = 0.000318\n",
            "Epoch 390: Train Loss = 0.000589, Val Loss = 0.000353\n",
            "Epoch 400: Train Loss = 0.005178, Val Loss = 0.005082\n",
            "Epoch 410: Train Loss = 0.000867, Val Loss = 0.000810\n",
            "Epoch 420: Train Loss = 0.001071, Val Loss = 0.000515\n",
            "Epoch 430: Train Loss = 0.001094, Val Loss = 0.000559\n",
            "Epoch 440: Train Loss = 0.000889, Val Loss = 0.000471\n",
            "Epoch 450: Train Loss = 0.000672, Val Loss = 0.000440\n",
            "Epoch 460: Train Loss = 0.000462, Val Loss = 0.000501\n",
            "Epoch 470: Train Loss = 0.000823, Val Loss = 0.000823\n",
            "Epoch 480: Train Loss = 0.001312, Val Loss = 0.000485\n",
            "Epoch 490: Train Loss = 0.000718, Val Loss = 0.000343\n",
            "Epoch 500: Train Loss = 0.000670, Val Loss = 0.000465\n",
            "✅ TCN_MIMO Final Test MSE for split 50_50: 0.000272\n",
            "\n",
            "====== Split: 40_60 | X (input): 40 AND Y (output): 61 ======\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.020081, Val Loss = 0.013546\n",
            "Epoch 20: Train Loss = 0.013410, Val Loss = 0.013833\n",
            "Epoch 30: Train Loss = 0.006344, Val Loss = 0.008652\n",
            "Epoch 40: Train Loss = 0.006126, Val Loss = 0.006180\n",
            "Epoch 50: Train Loss = 0.006705, Val Loss = 0.006999\n",
            "Epoch 60: Train Loss = 0.003865, Val Loss = 0.002580\n",
            "Epoch 70: Train Loss = 0.005423, Val Loss = 0.002557\n",
            "Epoch 80: Train Loss = 0.002285, Val Loss = 0.001909\n",
            "Epoch 90: Train Loss = 0.002461, Val Loss = 0.001344\n",
            "Epoch 100: Train Loss = 0.002923, Val Loss = 0.001582\n",
            "Epoch 110: Train Loss = 0.001462, Val Loss = 0.001372\n",
            "Epoch 120: Train Loss = 0.001220, Val Loss = 0.000971\n",
            "Epoch 130: Train Loss = 0.001966, Val Loss = 0.002331\n",
            "Epoch 140: Train Loss = 0.001748, Val Loss = 0.001368\n",
            "Epoch 150: Train Loss = 0.002593, Val Loss = 0.000973\n",
            "Epoch 160: Train Loss = 0.001063, Val Loss = 0.000773\n",
            "Epoch 170: Train Loss = 0.002544, Val Loss = 0.000985\n",
            "Epoch 180: Train Loss = 0.001445, Val Loss = 0.001065\n",
            "Epoch 190: Train Loss = 0.001243, Val Loss = 0.000777\n",
            "Epoch 200: Train Loss = 0.001059, Val Loss = 0.000744\n",
            "Epoch 210: Train Loss = 0.001271, Val Loss = 0.001157\n",
            "Epoch 220: Train Loss = 0.001931, Val Loss = 0.000872\n",
            "Epoch 230: Train Loss = 0.001452, Val Loss = 0.001017\n",
            "Epoch 240: Train Loss = 0.001029, Val Loss = 0.002075\n",
            "Epoch 250: Train Loss = 0.000810, Val Loss = 0.001054\n",
            "Epoch 260: Train Loss = 0.001579, Val Loss = 0.000739\n",
            "Epoch 270: Train Loss = 0.001180, Val Loss = 0.001343\n",
            "Epoch 280: Train Loss = 0.001209, Val Loss = 0.000878\n",
            "Epoch 290: Train Loss = 0.002470, Val Loss = 0.001433\n",
            "Epoch 300: Train Loss = 0.000691, Val Loss = 0.000807\n",
            "Epoch 310: Train Loss = 0.001488, Val Loss = 0.001198\n",
            "Epoch 320: Train Loss = 0.001448, Val Loss = 0.000948\n",
            "Epoch 330: Train Loss = 0.000791, Val Loss = 0.000819\n",
            "Epoch 340: Train Loss = 0.001577, Val Loss = 0.001163\n",
            "Epoch 350: Train Loss = 0.003060, Val Loss = 0.008892\n",
            "Epoch 360: Train Loss = 0.000935, Val Loss = 0.000785\n",
            "Epoch 370: Train Loss = 0.000774, Val Loss = 0.001323\n",
            "Epoch 380: Train Loss = 0.001017, Val Loss = 0.000847\n",
            "Epoch 390: Train Loss = 0.001606, Val Loss = 0.000958\n",
            "Epoch 400: Train Loss = 0.000447, Val Loss = 0.000755\n",
            "Epoch 410: Train Loss = 0.000842, Val Loss = 0.000885\n",
            "Epoch 420: Train Loss = 0.001374, Val Loss = 0.001611\n",
            "Epoch 430: Train Loss = 0.001053, Val Loss = 0.000766\n",
            "Epoch 440: Train Loss = 0.000612, Val Loss = 0.000783\n",
            "Epoch 450: Train Loss = 0.001698, Val Loss = 0.001007\n",
            "Epoch 460: Train Loss = 0.001868, Val Loss = 0.000867\n",
            "Epoch 470: Train Loss = 0.000685, Val Loss = 0.000693\n",
            "Epoch 480: Train Loss = 0.001358, Val Loss = 0.001000\n",
            "Epoch 490: Train Loss = 0.001079, Val Loss = 0.000676\n",
            "Epoch 500: Train Loss = 0.000559, Val Loss = 0.001209\n",
            "✅ TCN_MIMO Final Test MSE for split 40_60: 0.000749\n",
            "\n",
            "====== Split: 20_80 | X (input): 20 AND Y (output): 81 ======\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.043397, Val Loss = 0.019458\n",
            "Epoch 20: Train Loss = 0.015015, Val Loss = 0.024211\n",
            "Epoch 30: Train Loss = 0.007185, Val Loss = 0.007662\n",
            "Epoch 40: Train Loss = 0.007921, Val Loss = 0.006393\n",
            "Epoch 50: Train Loss = 0.010934, Val Loss = 0.004038\n",
            "Epoch 60: Train Loss = 0.003325, Val Loss = 0.003497\n",
            "Epoch 70: Train Loss = 0.006203, Val Loss = 0.003559\n",
            "Epoch 80: Train Loss = 0.010550, Val Loss = 0.004786\n",
            "Epoch 90: Train Loss = 0.008028, Val Loss = 0.003761\n",
            "Epoch 100: Train Loss = 0.003227, Val Loss = 0.002843\n",
            "Epoch 110: Train Loss = 0.005479, Val Loss = 0.003430\n",
            "Epoch 120: Train Loss = 0.007343, Val Loss = 0.006600\n",
            "Epoch 130: Train Loss = 0.004238, Val Loss = 0.003018\n",
            "Epoch 140: Train Loss = 0.011420, Val Loss = 0.004023\n",
            "Epoch 150: Train Loss = 0.005130, Val Loss = 0.003175\n",
            "Epoch 160: Train Loss = 0.005666, Val Loss = 0.004195\n",
            "Epoch 170: Train Loss = 0.005342, Val Loss = 0.002757\n",
            "Epoch 180: Train Loss = 0.002878, Val Loss = 0.002896\n",
            "Epoch 190: Train Loss = 0.003906, Val Loss = 0.003123\n",
            "Epoch 200: Train Loss = 0.005437, Val Loss = 0.003295\n",
            "Epoch 210: Train Loss = 0.008301, Val Loss = 0.005086\n",
            "Epoch 220: Train Loss = 0.003185, Val Loss = 0.002323\n",
            "Epoch 230: Train Loss = 0.003677, Val Loss = 0.002580\n",
            "Epoch 240: Train Loss = 0.003837, Val Loss = 0.003222\n",
            "Epoch 250: Train Loss = 0.003028, Val Loss = 0.002086\n",
            "Epoch 260: Train Loss = 0.002341, Val Loss = 0.002403\n",
            "Epoch 270: Train Loss = 0.004612, Val Loss = 0.002914\n",
            "Epoch 280: Train Loss = 0.003816, Val Loss = 0.002015\n",
            "Epoch 290: Train Loss = 0.008464, Val Loss = 0.002269\n",
            "Epoch 300: Train Loss = 0.004944, Val Loss = 0.002317\n",
            "Epoch 310: Train Loss = 0.004054, Val Loss = 0.002085\n",
            "Epoch 320: Train Loss = 0.002187, Val Loss = 0.001808\n",
            "Epoch 330: Train Loss = 0.004233, Val Loss = 0.001893\n",
            "Epoch 340: Train Loss = 0.001931, Val Loss = 0.001676\n",
            "Epoch 350: Train Loss = 0.002040, Val Loss = 0.001534\n",
            "Epoch 360: Train Loss = 0.001417, Val Loss = 0.001429\n",
            "Epoch 370: Train Loss = 0.004013, Val Loss = 0.002406\n",
            "Epoch 380: Train Loss = 0.002869, Val Loss = 0.001785\n",
            "Epoch 390: Train Loss = 0.003507, Val Loss = 0.001581\n",
            "Epoch 400: Train Loss = 0.002232, Val Loss = 0.002261\n",
            "Epoch 410: Train Loss = 0.003046, Val Loss = 0.001413\n",
            "Epoch 420: Train Loss = 0.002909, Val Loss = 0.001753\n",
            "Epoch 430: Train Loss = 0.001771, Val Loss = 0.001208\n",
            "Epoch 440: Train Loss = 0.002972, Val Loss = 0.001293\n",
            "Epoch 450: Train Loss = 0.002643, Val Loss = 0.001527\n",
            "Epoch 460: Train Loss = 0.001301, Val Loss = 0.001920\n",
            "Epoch 470: Train Loss = 0.002155, Val Loss = 0.001901\n",
            "Epoch 480: Train Loss = 0.000857, Val Loss = 0.001418\n",
            "Epoch 490: Train Loss = 0.002103, Val Loss = 0.001265\n",
            "Epoch 500: Train Loss = 0.001376, Val Loss = 0.001486\n",
            "✅ TCN_MIMO Final Test MSE for split 20_80: 0.001572\n",
            "\n",
            "====== Split: 10_90 | X (input): 10 AND Y (output): 91 ======\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.037941, Val Loss = 0.025725\n",
            "Epoch 20: Train Loss = 0.024128, Val Loss = 0.021172\n",
            "Epoch 30: Train Loss = 0.021908, Val Loss = 0.019696\n",
            "Epoch 40: Train Loss = 0.022844, Val Loss = 0.014973\n",
            "Epoch 50: Train Loss = 0.015885, Val Loss = 0.014803\n",
            "Epoch 60: Train Loss = 0.005668, Val Loss = 0.006879\n",
            "Epoch 70: Train Loss = 0.010386, Val Loss = 0.006491\n",
            "Epoch 80: Train Loss = 0.010453, Val Loss = 0.006233\n",
            "Epoch 90: Train Loss = 0.008219, Val Loss = 0.005746\n",
            "Epoch 100: Train Loss = 0.011411, Val Loss = 0.005962\n",
            "Epoch 110: Train Loss = 0.009494, Val Loss = 0.005021\n",
            "Epoch 120: Train Loss = 0.009988, Val Loss = 0.004592\n",
            "Epoch 130: Train Loss = 0.012386, Val Loss = 0.006440\n",
            "Epoch 140: Train Loss = 0.014110, Val Loss = 0.004465\n",
            "Epoch 150: Train Loss = 0.003937, Val Loss = 0.006381\n",
            "Epoch 160: Train Loss = 0.010762, Val Loss = 0.004369\n",
            "Epoch 170: Train Loss = 0.007836, Val Loss = 0.004936\n",
            "Epoch 180: Train Loss = 0.004586, Val Loss = 0.004723\n",
            "Epoch 190: Train Loss = 0.009728, Val Loss = 0.003790\n",
            "Epoch 200: Train Loss = 0.007825, Val Loss = 0.003564\n",
            "Epoch 210: Train Loss = 0.004258, Val Loss = 0.006369\n",
            "Epoch 220: Train Loss = 0.008496, Val Loss = 0.004244\n",
            "Epoch 230: Train Loss = 0.003452, Val Loss = 0.004681\n",
            "Epoch 240: Train Loss = 0.006119, Val Loss = 0.004047\n",
            "Epoch 250: Train Loss = 0.007000, Val Loss = 0.003596\n",
            "Epoch 260: Train Loss = 0.004971, Val Loss = 0.003794\n",
            "Epoch 270: Train Loss = 0.011654, Val Loss = 0.004797\n",
            "Epoch 280: Train Loss = 0.011490, Val Loss = 0.005062\n",
            "Epoch 290: Train Loss = 0.003291, Val Loss = 0.004511\n",
            "Epoch 300: Train Loss = 0.004001, Val Loss = 0.003823\n",
            "Epoch 310: Train Loss = 0.006830, Val Loss = 0.003902\n",
            "Epoch 320: Train Loss = 0.005013, Val Loss = 0.003742\n",
            "Epoch 330: Train Loss = 0.002060, Val Loss = 0.003510\n",
            "Epoch 340: Train Loss = 0.015078, Val Loss = 0.003825\n",
            "Epoch 350: Train Loss = 0.003217, Val Loss = 0.003790\n",
            "Epoch 360: Train Loss = 0.009633, Val Loss = 0.005891\n",
            "Epoch 370: Train Loss = 0.007120, Val Loss = 0.004756\n",
            "Epoch 380: Train Loss = 0.002969, Val Loss = 0.003698\n",
            "Epoch 390: Train Loss = 0.002575, Val Loss = 0.003383\n",
            "Epoch 400: Train Loss = 0.004271, Val Loss = 0.003782\n",
            "Epoch 410: Train Loss = 0.003868, Val Loss = 0.003744\n",
            "Epoch 420: Train Loss = 0.004321, Val Loss = 0.002985\n",
            "Epoch 430: Train Loss = 0.002555, Val Loss = 0.003419\n",
            "Epoch 440: Train Loss = 0.006185, Val Loss = 0.004475\n",
            "Epoch 450: Train Loss = 0.005535, Val Loss = 0.003167\n",
            "Epoch 460: Train Loss = 0.005695, Val Loss = 0.003418\n",
            "Epoch 470: Train Loss = 0.004277, Val Loss = 0.003465\n",
            "Epoch 480: Train Loss = 0.002435, Val Loss = 0.003054\n",
            "Epoch 490: Train Loss = 0.004570, Val Loss = 0.003021\n",
            "Epoch 500: Train Loss = 0.008815, Val Loss = 0.003431\n",
            "✅ TCN_MIMO Final Test MSE for split 10_90: 0.003825\n",
            "\n",
            "====== Split: 5_95 | X (input): 5 AND Y (output): 96 ======\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.039220, Val Loss = 0.034353\n",
            "Epoch 20: Train Loss = 0.043084, Val Loss = 0.024565\n",
            "Epoch 30: Train Loss = 0.021438, Val Loss = 0.039551\n",
            "Epoch 40: Train Loss = 0.062005, Val Loss = 0.020848\n",
            "Epoch 50: Train Loss = 0.012226, Val Loss = 0.013267\n",
            "Epoch 60: Train Loss = 0.027459, Val Loss = 0.017411\n",
            "Epoch 70: Train Loss = 0.018207, Val Loss = 0.012205\n",
            "Epoch 80: Train Loss = 0.015590, Val Loss = 0.011466\n",
            "Epoch 90: Train Loss = 0.031260, Val Loss = 0.012812\n",
            "Epoch 100: Train Loss = 0.008567, Val Loss = 0.011118\n",
            "Epoch 110: Train Loss = 0.005076, Val Loss = 0.008770\n",
            "Epoch 120: Train Loss = 0.005327, Val Loss = 0.007581\n",
            "Epoch 130: Train Loss = 0.030807, Val Loss = 0.008344\n",
            "Epoch 140: Train Loss = 0.015841, Val Loss = 0.007090\n",
            "Epoch 150: Train Loss = 0.009398, Val Loss = 0.007373\n",
            "Epoch 160: Train Loss = 0.009738, Val Loss = 0.008844\n",
            "Epoch 170: Train Loss = 0.013462, Val Loss = 0.010325\n",
            "Epoch 180: Train Loss = 0.017769, Val Loss = 0.008589\n",
            "Epoch 190: Train Loss = 0.006637, Val Loss = 0.005769\n",
            "Epoch 200: Train Loss = 0.010841, Val Loss = 0.007098\n",
            "Epoch 210: Train Loss = 0.008950, Val Loss = 0.005974\n",
            "Epoch 220: Train Loss = 0.013770, Val Loss = 0.009346\n",
            "Epoch 230: Train Loss = 0.012632, Val Loss = 0.009640\n",
            "Epoch 240: Train Loss = 0.012351, Val Loss = 0.006997\n",
            "Epoch 250: Train Loss = 0.007254, Val Loss = 0.006031\n",
            "Epoch 260: Train Loss = 0.004811, Val Loss = 0.005850\n",
            "Epoch 270: Train Loss = 0.002535, Val Loss = 0.006206\n",
            "Epoch 280: Train Loss = 0.016221, Val Loss = 0.005979\n",
            "Epoch 290: Train Loss = 0.005131, Val Loss = 0.006529\n",
            "Epoch 300: Train Loss = 0.008560, Val Loss = 0.006391\n",
            "Epoch 310: Train Loss = 0.004984, Val Loss = 0.006669\n",
            "Epoch 320: Train Loss = 0.015603, Val Loss = 0.006152\n",
            "Epoch 330: Train Loss = 0.011587, Val Loss = 0.009999\n",
            "Epoch 340: Train Loss = 0.006124, Val Loss = 0.005626\n",
            "Epoch 350: Train Loss = 0.014952, Val Loss = 0.005754\n",
            "Epoch 360: Train Loss = 0.007263, Val Loss = 0.005080\n",
            "Epoch 370: Train Loss = 0.007865, Val Loss = 0.005444\n",
            "Epoch 380: Train Loss = 0.007575, Val Loss = 0.005906\n",
            "Epoch 390: Train Loss = 0.005539, Val Loss = 0.005516\n",
            "Epoch 400: Train Loss = 0.010862, Val Loss = 0.006084\n",
            "Epoch 410: Train Loss = 0.005533, Val Loss = 0.006225\n",
            "Epoch 420: Train Loss = 0.005812, Val Loss = 0.005544\n",
            "Epoch 430: Train Loss = 0.007615, Val Loss = 0.009040\n",
            "Epoch 440: Train Loss = 0.007477, Val Loss = 0.007117\n",
            "Epoch 450: Train Loss = 0.005884, Val Loss = 0.005496\n",
            "Epoch 460: Train Loss = 0.011105, Val Loss = 0.006426\n",
            "Epoch 470: Train Loss = 0.006347, Val Loss = 0.006836\n",
            "Epoch 480: Train Loss = 0.007651, Val Loss = 0.005744\n",
            "Epoch 490: Train Loss = 0.008753, Val Loss = 0.005507\n",
            "Epoch 500: Train Loss = 0.004659, Val Loss = 0.007202\n",
            "✅ TCN_MIMO Final Test MSE for split 5_95: 0.008719\n",
            "\n",
            "====== Split: 3_97 | X (input): 3 AND Y (output): 98 ======\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.071621, Val Loss = 0.040652\n",
            "Epoch 20: Train Loss = 0.053131, Val Loss = 0.026873\n",
            "Epoch 30: Train Loss = 0.060323, Val Loss = 0.023626\n",
            "Epoch 40: Train Loss = 0.021706, Val Loss = 0.022868\n",
            "Epoch 50: Train Loss = 0.034384, Val Loss = 0.020734\n",
            "Epoch 60: Train Loss = 0.024833, Val Loss = 0.018179\n",
            "Epoch 70: Train Loss = 0.017503, Val Loss = 0.016594\n",
            "Epoch 80: Train Loss = 0.023437, Val Loss = 0.018088\n",
            "Epoch 90: Train Loss = 0.023464, Val Loss = 0.014963\n",
            "Epoch 100: Train Loss = 0.020613, Val Loss = 0.012780\n",
            "Epoch 110: Train Loss = 0.012262, Val Loss = 0.012909\n",
            "Epoch 120: Train Loss = 0.019982, Val Loss = 0.011724\n",
            "Epoch 130: Train Loss = 0.021485, Val Loss = 0.010653\n",
            "Epoch 140: Train Loss = 0.021669, Val Loss = 0.011624\n",
            "Epoch 150: Train Loss = 0.015238, Val Loss = 0.009577\n",
            "Epoch 160: Train Loss = 0.009937, Val Loss = 0.009149\n",
            "Epoch 170: Train Loss = 0.013455, Val Loss = 0.010336\n",
            "Epoch 180: Train Loss = 0.018632, Val Loss = 0.010485\n",
            "Epoch 190: Train Loss = 0.013031, Val Loss = 0.008850\n",
            "Epoch 200: Train Loss = 0.018005, Val Loss = 0.014330\n",
            "Epoch 210: Train Loss = 0.006561, Val Loss = 0.009343\n",
            "Epoch 220: Train Loss = 0.010463, Val Loss = 0.009390\n",
            "Epoch 230: Train Loss = 0.017302, Val Loss = 0.008448\n",
            "Epoch 240: Train Loss = 0.004024, Val Loss = 0.008670\n",
            "Epoch 250: Train Loss = 0.009103, Val Loss = 0.008887\n",
            "Epoch 260: Train Loss = 0.007641, Val Loss = 0.011369\n",
            "Epoch 270: Train Loss = 0.014939, Val Loss = 0.010049\n",
            "Epoch 280: Train Loss = 0.025928, Val Loss = 0.009875\n",
            "Epoch 290: Train Loss = 0.012969, Val Loss = 0.010814\n",
            "Epoch 300: Train Loss = 0.008266, Val Loss = 0.011117\n",
            "Epoch 310: Train Loss = 0.010783, Val Loss = 0.009684\n",
            "Epoch 320: Train Loss = 0.024007, Val Loss = 0.009502\n",
            "Epoch 330: Train Loss = 0.013344, Val Loss = 0.009081\n",
            "Epoch 340: Train Loss = 0.027359, Val Loss = 0.009581\n",
            "Epoch 350: Train Loss = 0.005512, Val Loss = 0.008588\n",
            "Epoch 360: Train Loss = 0.012838, Val Loss = 0.008295\n",
            "Epoch 370: Train Loss = 0.021780, Val Loss = 0.009875\n",
            "Epoch 380: Train Loss = 0.008024, Val Loss = 0.009581\n",
            "Epoch 390: Train Loss = 0.020106, Val Loss = 0.008463\n",
            "Epoch 400: Train Loss = 0.016178, Val Loss = 0.010222\n",
            "Epoch 410: Train Loss = 0.019398, Val Loss = 0.008468\n",
            "Epoch 420: Train Loss = 0.009298, Val Loss = 0.008093\n",
            "Epoch 430: Train Loss = 0.006932, Val Loss = 0.008154\n",
            "Epoch 440: Train Loss = 0.004562, Val Loss = 0.010037\n",
            "Epoch 450: Train Loss = 0.012595, Val Loss = 0.008860\n",
            "Epoch 460: Train Loss = 0.007725, Val Loss = 0.009421\n",
            "Epoch 470: Train Loss = 0.016218, Val Loss = 0.012093\n",
            "Epoch 480: Train Loss = 0.010022, Val Loss = 0.009811\n",
            "Epoch 490: Train Loss = 0.004956, Val Loss = 0.007301\n",
            "Epoch 500: Train Loss = 0.008543, Val Loss = 0.008476\n",
            "✅ TCN_MIMO Final Test MSE for split 3_97: 0.014294\n",
            "\n",
            "====== Split: 1_99 | X (input): 1 AND Y (output): 100 ======\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.126580, Val Loss = 0.058652\n",
            "Epoch 20: Train Loss = 0.052797, Val Loss = 0.055082\n",
            "Epoch 30: Train Loss = 0.057457, Val Loss = 0.053016\n",
            "Epoch 40: Train Loss = 0.041187, Val Loss = 0.053377\n",
            "Epoch 50: Train Loss = 0.051219, Val Loss = 0.062055\n",
            "Epoch 60: Train Loss = 0.031253, Val Loss = 0.045954\n",
            "Epoch 70: Train Loss = 0.046429, Val Loss = 0.047656\n",
            "Epoch 80: Train Loss = 0.055277, Val Loss = 0.041915\n",
            "Epoch 90: Train Loss = 0.063371, Val Loss = 0.048902\n",
            "Epoch 100: Train Loss = 0.086486, Val Loss = 0.042113\n",
            "Epoch 110: Train Loss = 0.020182, Val Loss = 0.044174\n",
            "Epoch 120: Train Loss = 0.091054, Val Loss = 0.042011\n",
            "Epoch 130: Train Loss = 0.060599, Val Loss = 0.045157\n",
            "Epoch 140: Train Loss = 0.062910, Val Loss = 0.040415\n",
            "Epoch 150: Train Loss = 0.035673, Val Loss = 0.043368\n",
            "Epoch 160: Train Loss = 0.027802, Val Loss = 0.042759\n",
            "Epoch 170: Train Loss = 0.035541, Val Loss = 0.043368\n",
            "Epoch 180: Train Loss = 0.066865, Val Loss = 0.040206\n",
            "Epoch 190: Train Loss = 0.031409, Val Loss = 0.041204\n",
            "Epoch 200: Train Loss = 0.049582, Val Loss = 0.038684\n",
            "Epoch 210: Train Loss = 0.073741, Val Loss = 0.037963\n",
            "Epoch 220: Train Loss = 0.122525, Val Loss = 0.038459\n",
            "Epoch 230: Train Loss = 0.076329, Val Loss = 0.036320\n",
            "Epoch 240: Train Loss = 0.030795, Val Loss = 0.039523\n",
            "Epoch 250: Train Loss = 0.039226, Val Loss = 0.038211\n",
            "Epoch 260: Train Loss = 0.023694, Val Loss = 0.038611\n",
            "Epoch 270: Train Loss = 0.030057, Val Loss = 0.039492\n",
            "Epoch 280: Train Loss = 0.037877, Val Loss = 0.037340\n",
            "Epoch 290: Train Loss = 0.025154, Val Loss = 0.041523\n",
            "Epoch 300: Train Loss = 0.029580, Val Loss = 0.034394\n",
            "Epoch 310: Train Loss = 0.027459, Val Loss = 0.035458\n",
            "Epoch 320: Train Loss = 0.021217, Val Loss = 0.038698\n",
            "Epoch 330: Train Loss = 0.041562, Val Loss = 0.036944\n",
            "Epoch 340: Train Loss = 0.024634, Val Loss = 0.037655\n",
            "Epoch 350: Train Loss = 0.040262, Val Loss = 0.033618\n",
            "Epoch 360: Train Loss = 0.024213, Val Loss = 0.034708\n",
            "Epoch 370: Train Loss = 0.099396, Val Loss = 0.035777\n",
            "Epoch 380: Train Loss = 0.037315, Val Loss = 0.037119\n",
            "Epoch 390: Train Loss = 0.038310, Val Loss = 0.036672\n",
            "Epoch 400: Train Loss = 0.041516, Val Loss = 0.039346\n",
            "Epoch 410: Train Loss = 0.023092, Val Loss = 0.034930\n",
            "Epoch 420: Train Loss = 0.030897, Val Loss = 0.037999\n",
            "Epoch 430: Train Loss = 0.022953, Val Loss = 0.035651\n",
            "Epoch 440: Train Loss = 0.021406, Val Loss = 0.039159\n",
            "Epoch 450: Train Loss = 0.089427, Val Loss = 0.038746\n",
            "Epoch 460: Train Loss = 0.039791, Val Loss = 0.036057\n",
            "Epoch 470: Train Loss = 0.019678, Val Loss = 0.037545\n",
            "Epoch 480: Train Loss = 0.029773, Val Loss = 0.035431\n",
            "Epoch 490: Train Loss = 0.022002, Val Loss = 0.041688\n",
            "Epoch 500: Train Loss = 0.031830, Val Loss = 0.037151\n",
            "✅ TCN_MIMO Final Test MSE for split 1_99: 0.045860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "U0pAb_09H8uD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BASELINE VISUALIZATION - MSE Bar and line**"
      ],
      "metadata": {
        "id": "-3fEW-xWH8wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "# ---------------- SETUP ----------------\n",
        "mpl.rcParams.update({\n",
        "    'font.size': 20,\n",
        "    'axes.titlesize': 24,\n",
        "    'axes.labelsize': 22,\n",
        "    'xtick.labelsize': 18,\n",
        "    'ytick.labelsize': 18,\n",
        "    'legend.fontsize': 20,\n",
        "    'figure.dpi': 300,\n",
        "    'savefig.dpi': 300,\n",
        "    'figure.autolayout': True,\n",
        "})\n",
        "\n",
        "# ---------------- FIX COLUMN NAMES ----------------\n",
        "DSSM_Advanced_mse.rename(columns={'Test_MSE': 'MSE'}, inplace=True)\n",
        "DSSM_BASIC_MSE.rename(columns={'Test_MSE': 'MSE'}, inplace=True)\n",
        "TFT_mse.rename(columns={'Test_MSE': 'MSE'}, inplace=True)\n",
        "NLinear_mse.rename(columns={'Test_MSE': 'MSE'}, inplace=True)\n",
        "TCN_MIMO_mse.rename(columns={'Test_MSE': 'MSE'}, inplace=True)\n",
        "\n",
        "# ---------------- ASSIGN MODEL NAMES ----------------\n",
        "DSSM_Advanced_mse['Model'] = 'DSSM-advanced'\n",
        "DSSM_BASIC_MSE['Model'] = 'DSSM-basic'\n",
        "TFT_mse['Model'] = 'TFT'\n",
        "NLinear_mse['Model'] = 'NLinear'\n",
        "TCN_MIMO_mse['Model'] = 'TCN'\n",
        "LSTM_mse['Model'] = 'BiLSTM'\n",
        "\n",
        "# ---------------- COMBINE MODEL MSE ----------------\n",
        "mse_df = pd.concat([\n",
        "    DSSM_Advanced_mse,\n",
        "    DSSM_BASIC_MSE,\n",
        "    TFT_mse,\n",
        "    NLinear_mse,\n",
        "    TCN_MIMO_mse,\n",
        "    LSTM_mse\n",
        "], ignore_index=True)\n",
        "\n",
        "# ---------------- PIVOT ----------------\n",
        "splits = ['80_20', '60_40', '50_50', '40_60', '20_80', '10_90', '5_95', '3_97']\n",
        "mse_pivot = mse_df.pivot(index='Model', columns='Split', values='MSE').reset_index()\n",
        "mse_pivot = mse_pivot[['Model'] + [s for s in splits if s in mse_pivot.columns]]\n",
        "\n",
        "# ---------------- PLOTTING ----------------\n",
        "output_folder = 'drive/My Drive/DSSM-Figures'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# --- BAR PLOT ---\n",
        "x = range(len(mse_pivot['Model']))\n",
        "width = 0.09\n",
        "plt.figure(figsize=(16, 10))\n",
        "for i, split in enumerate(mse_pivot.columns[1:]):\n",
        "    plt.bar(\n",
        "        [pos + i * width for pos in x],\n",
        "        mse_pivot[split],\n",
        "        width=width,\n",
        "        label=split.replace('_', ':')\n",
        "    )\n",
        "\n",
        "plt.xticks([pos + (len(mse_pivot.columns[1:]) // 2) * width for pos in x],\n",
        "           mse_pivot['Model'], rotation=30, ha='right')\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"MSE Score\")\n",
        "plt.title(\"MSE Scores across Models and X-Y Splits\")\n",
        "plt.legend(title=\"X-Y Splits\")\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.savefig(f'{output_folder}/Figure_X_MSE_Barplot.pdf', format='pdf', bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# --- LINE PLOT ---\n",
        "plt.figure(figsize=(16, 10))\n",
        "for idx, row in mse_pivot.iterrows():\n",
        "    model = row['Model']\n",
        "    plt.plot(\n",
        "        [s.replace('_', ':') for s in mse_pivot.columns[1:]],\n",
        "        row[mse_pivot.columns[1:]].values,\n",
        "        marker='o',\n",
        "        label=model\n",
        "    )\n",
        "\n",
        "plt.xlabel(\"X-Y Split\")\n",
        "plt.ylabel(\"MSE Score\")\n",
        "plt.title(\"MSE Scores across X-Y Splits for Each Model\")\n",
        "plt.legend(title=\"Model\", loc='upper left')\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.savefig(f'{output_folder}/Figure_Y_MSE_Lineplot.pdf', format='pdf', bbox_inches='tight')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "wwnGXT2KBa5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Log (MSE) - Bar and Line**"
      ],
      "metadata": {
        "id": "ty6v8aGT4HVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BAR PLOT with log scale ---\n",
        "x = range(len(mse_pivot['Model']))\n",
        "width = 0.09\n",
        "plt.figure(figsize=(16, 10))\n",
        "for i, split in enumerate(mse_pivot.columns[1:]):\n",
        "    plt.bar(\n",
        "        [pos + i * width for pos in x],\n",
        "        mse_pivot[split],\n",
        "        width=width,\n",
        "        label=split.replace('_', ':')\n",
        "    )\n",
        "\n",
        "plt.xticks([pos + (len(mse_pivot.columns[1:]) // 2) * width for pos in x],\n",
        "           mse_pivot['Model'], rotation=30, ha='right')\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"MSE Score (log scale)\")\n",
        "plt.title(\"Log-Scaled MSE Scores across Models and X-Y Splits\")\n",
        "plt.yscale('log')  # <--- Apply log scale here\n",
        "plt.legend(title=\"X-Y Splits\")\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.savefig(f'{output_folder}/Figure_X_MSE_Barplot_log-1.0.pdf', format='pdf', bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# --- LINE PLOT with log scale ---\n",
        "plt.figure(figsize=(16, 10))\n",
        "for idx, row in mse_pivot.iterrows():\n",
        "    model = row['Model']\n",
        "    plt.plot(\n",
        "        [s.replace('_', ':') for s in mse_pivot.columns[1:]],\n",
        "        row[mse_pivot.columns[1:]].values,\n",
        "        marker='o',\n",
        "        label=model\n",
        "    )\n",
        "\n",
        "plt.xlabel(\"X-Y Split\")\n",
        "plt.ylabel(\"MSE Score (log scale)\")\n",
        "plt.title(\"Log-Scaled MSE Scores across X-Y Splits for Each Model\")\n",
        "plt.yscale('log')  # <--- Apply log scale here\n",
        "plt.legend(title=\"Model\", loc='upper left')\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.savefig(f'{output_folder}/Figure_Y_MSE_Lineplot_log-1.0.pdf', format='pdf', bbox_inches='tight')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "NUrjy5aj4HwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving dataframe as csv"
      ],
      "metadata": {
        "id": "rXCTfq4rFxxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- SAVE MSE TABLE TO CSV ----------------\n",
        "csv_output_path = 'drive/My Drive/DSSM-Figures/All_Model_MSE_Table-1.0.csv'\n",
        "mse_pivot.to_csv(csv_output_path, index=False)\n",
        "print(f\"MSE table saved to: {csv_output_path}\")"
      ],
      "metadata": {
        "id": "ehdS5GCBFzvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac21c852-cbd4-4272-8100-b38fad2c3a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE table saved to: drive/My Drive/DSSM-Figures/All_Model_MSE_Table-1.0.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import matplotlib as mpl\n",
        "from google.colab import drive\n",
        "# --------------------- Load Data ---------------------\n",
        "print(\"Mounting Google Drive and loading dataset...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ---------------- LOAD MSE TABLE ----------------\n",
        "csv_input_path = 'drive/My Drive/DSSM-Figures/All_Model_MSE_Table-1.0.csv'\n",
        "mse_pivot = pd.read_csv(csv_input_path)"
      ],
      "metadata": {
        "id": "HvLInBgPF0iE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47e30653-50cf-4ede-8e8d-7a982620e456"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive and loading dataset...\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse_pivot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ayNtwsIBKOnh",
        "outputId": "14d7e749-59c1-470a-aae3-133c6e9791da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Model     80_20     60_40     50_50     40_60     20_80     10_90  \\\n",
              "0  DSSM-advanced  0.000045  0.000172  0.000832  0.000721  0.002107  0.004962   \n",
              "1     DSSM-basic  0.001168  0.002288  0.003769  0.005900  0.010188  0.016807   \n",
              "2        NLinear  0.000010  0.000140  0.000340  0.001077  0.009894  0.024607   \n",
              "3            TFT  0.000209  0.000446  0.000590  0.002430  0.003448  0.010680   \n",
              "4           LSTM  0.001343  0.005442  0.008847  0.017022  0.054216  0.112788   \n",
              "\n",
              "       5_95  \n",
              "0  0.007876  \n",
              "1  0.021219  \n",
              "2  0.033830  \n",
              "3  0.016687  \n",
              "4  0.118881  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f4f36ad9-8168-4fd8-a5bc-d0fb68a0211d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>80_20</th>\n",
              "      <th>60_40</th>\n",
              "      <th>50_50</th>\n",
              "      <th>40_60</th>\n",
              "      <th>20_80</th>\n",
              "      <th>10_90</th>\n",
              "      <th>5_95</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DSSM-advanced</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000172</td>\n",
              "      <td>0.000832</td>\n",
              "      <td>0.000721</td>\n",
              "      <td>0.002107</td>\n",
              "      <td>0.004962</td>\n",
              "      <td>0.007876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DSSM-basic</td>\n",
              "      <td>0.001168</td>\n",
              "      <td>0.002288</td>\n",
              "      <td>0.003769</td>\n",
              "      <td>0.005900</td>\n",
              "      <td>0.010188</td>\n",
              "      <td>0.016807</td>\n",
              "      <td>0.021219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NLinear</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000140</td>\n",
              "      <td>0.000340</td>\n",
              "      <td>0.001077</td>\n",
              "      <td>0.009894</td>\n",
              "      <td>0.024607</td>\n",
              "      <td>0.033830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TFT</td>\n",
              "      <td>0.000209</td>\n",
              "      <td>0.000446</td>\n",
              "      <td>0.000590</td>\n",
              "      <td>0.002430</td>\n",
              "      <td>0.003448</td>\n",
              "      <td>0.010680</td>\n",
              "      <td>0.016687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LSTM</td>\n",
              "      <td>0.001343</td>\n",
              "      <td>0.005442</td>\n",
              "      <td>0.008847</td>\n",
              "      <td>0.017022</td>\n",
              "      <td>0.054216</td>\n",
              "      <td>0.112788</td>\n",
              "      <td>0.118881</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f4f36ad9-8168-4fd8-a5bc-d0fb68a0211d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f4f36ad9-8168-4fd8-a5bc-d0fb68a0211d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f4f36ad9-8168-4fd8-a5bc-d0fb68a0211d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0ff968c9-6e22-47b9-9cb3-56a19d771d7f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0ff968c9-6e22-47b9-9cb3-56a19d771d7f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0ff968c9-6e22-47b9-9cb3-56a19d771d7f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_ed943e7a-859a-4793-b13b-4d8dc1b429a5\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('mse_pivot')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ed943e7a-859a-4793-b13b-4d8dc1b429a5 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('mse_pivot');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "mse_pivot",
              "summary": "{\n  \"name\": \"mse_pivot\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"DSSM-basic\",\n          \"LSTM\",\n          \"NLinear\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"80_20\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0006469509177702986,\n        \"min\": 9.52671711133673e-06,\n        \"max\": 0.001343,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0011679192272474,\n          0.001343,\n          9.52671711133673e-06\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"60_40\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.002274298777712079,\n        \"min\": 0.000140341934664,\n        \"max\": 0.005442,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0022882376048651,\n          0.005442,\n          0.000140341934664\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"50_50\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.003615339370633996,\n        \"min\": 0.0003403409741076,\n        \"max\": 0.008847,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0037688044908623,\n          0.008847,\n          0.0003403409741076\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"40_60\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.006795525763927179,\n        \"min\": 0.0007210617186501,\n        \"max\": 0.017022,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0058995280259886,\n          0.017022,\n          0.001076823337582\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"20_80\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.021691539360537096,\n        \"min\": 0.0021068532951176,\n        \"max\": 0.054216,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0101884744145568,\n          0.054216,\n          0.0098935659887017\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"10_90\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04466106568303433,\n        \"min\": 0.0049617495387792,\n        \"max\": 0.112788,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0168068958394738,\n          0.112788,\n          0.0246066142664619\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"5_95\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04524381034941084,\n        \"min\": 0.0078759929165244,\n        \"max\": 0.118881,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0212187672794801,\n          0.118881,\n          0.0338297481708875\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- LINE PLOT ---\n",
        "output_folder = 'drive/My Drive/DSSM-Figures'\n",
        "plt.figure(figsize=(16, 10))\n",
        "for idx, row in mse_pivot.iterrows():\n",
        "    model = row['Model']\n",
        "    plt.plot(\n",
        "        [s.replace('_', ':') for s in mse_pivot.columns[1:]],\n",
        "        row[mse_pivot.columns[1:]].values,\n",
        "        marker='o',\n",
        "        label=model\n",
        "    )\n",
        "\n",
        "plt.xlabel(\"X-Y Split\")\n",
        "plt.ylabel(\"MSE Score\")\n",
        "plt.title(\"MSE Scores across X-Y Splits for Each Model\")\n",
        "plt.legend(title=\"Model\", loc='upper left')\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.savefig(f'{output_folder}/Figure_Y_MSE_Lineplot-final.pdf', format='pdf', bbox_inches='tight')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "isJRl05wKh8_"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}