{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PREPERATION"
      ],
      "metadata": {
        "id": "uHubmQ-gpS0u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCxMIiA1fexX",
        "outputId": "c16b38fa-1e82-41a6-9dea-a687f13e93b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive and loading dataset...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loaded dataset with shape: (1192157, 17)\n",
            "Static feature table created: Input_Link_Table.shape = (2703, 15)\n",
            "Time series matrix constructed: relevant_data.shape = (2703, 101)\n",
            "Performed KMeans clustering into 8 clusters\n",
            "Cluster boundary stats calculated: cluster_boundaries.shape = (8, 4, 101)\n",
            "Final input features (static + cluster): merged_df.shape = (2703, 16)\n",
            "Final output time series: df_output.shape = (273003, 3)\n",
            "Data Preparation Summary:\n",
            "Static Input Table: merged_df [2703 rows × 16 columns]\n",
            "Time Series Output: df_output [273003 rows × 3 columns]\n",
            "Cluster Boundaries: cluster_boundaries [(8, 4, 101)]\n"
          ]
        }
      ],
      "source": [
        "# --------------------- Imports ---------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from google.colab import drive\n",
        "\n",
        "# --------------------- Matplotlib Setup ---------------------\n",
        "mpl.rcParams.update({\n",
        "    'font.size': 14,\n",
        "    'axes.titlesize': 15,\n",
        "    'axes.labelsize': 12,\n",
        "    'xtick.labelsize': 11,\n",
        "    'ytick.labelsize': 11,\n",
        "    'legend.fontsize': 11,\n",
        "    'figure.dpi': 300,\n",
        "    'savefig.dpi': 300,\n",
        "    'figure.autolayout': True,\n",
        "})\n",
        "\n",
        "# --------------------- Load Data ---------------------\n",
        "print(\"Mounting Google Drive and loading dataset...\")\n",
        "drive.mount('/content/drive')\n",
        "total_capture_7k = pd.read_csv('drive/My Drive/correlation_wide.csv')\n",
        "print(f\"Loaded dataset with shape: {total_capture_7k.shape}\")\n",
        "\n",
        "# --------------------- Identify Unique Static Parameter Sets ---------------------\n",
        "static_cols = [\n",
        "    'MikeSorghum', 'Quartz', 'Plagioclase', 'Apatite', 'Ilmenite',\n",
        "    'Diopside_Mn', 'Diopside', 'Olivine', 'Alkali-feldspar',\n",
        "    'Montmorillonite', 'Glass', 'temp', 'shift', 'year'\n",
        "]\n",
        "\n",
        "# Add timestep count per file_id\n",
        "file_lengths = total_capture_7k.groupby('file_id').size().rename(\"num_timesteps\").reset_index()\n",
        "static_rows = total_capture_7k.groupby('file_id')[static_cols].first().reset_index()\n",
        "static_rows = static_rows.merge(file_lengths, on='file_id')\n",
        "\n",
        "# Filter only unique static parameter sets\n",
        "unique_static_rows = static_rows.drop_duplicates(subset=static_cols)\n",
        "unique_file_ids = unique_static_rows['file_id'].tolist()\n",
        "\n",
        "# --------------------- Extract Time Series Data ---------------------\n",
        "filtered_df = total_capture_7k[total_capture_7k['file_id'].isin(unique_file_ids)].copy()\n",
        "\n",
        "# Truncate each group to 101 timesteps\n",
        "filtered_df = filtered_df.groupby('file_id').head(101).reset_index(drop=True)\n",
        "\n",
        "# --------------------- Static Feature Table ---------------------\n",
        "Input_Link_Table = filtered_df.groupby('file_id').agg({col: 'first' for col in static_cols}).reset_index()\n",
        "print(f\"Static feature table created: Input_Link_Table.shape = {Input_Link_Table.shape}\")\n",
        "\n",
        "# --------------------- Time Series Structuring ---------------------\n",
        "result = filtered_df[['Total_CO2_capture', 'year', 'file_id']]\n",
        "file_ids = result['file_id'].unique()\n",
        "num_file_ids = len(file_ids)\n",
        "max_timesteps = 101\n",
        "relevant_data = np.zeros((num_file_ids, max_timesteps))\n",
        "file_id_order = np.zeros(num_file_ids)\n",
        "\n",
        "for i, file_id in enumerate(file_ids):\n",
        "    file_data = result[result['file_id'] == file_id]['Total_CO2_capture'].values\n",
        "    relevant_data[i, :len(file_data)] = file_data\n",
        "    file_id_order[i] = file_id\n",
        "print(f\"Time series matrix constructed: relevant_data.shape = {relevant_data.shape}\")\n",
        "\n",
        "# --------------------- Clustering ---------------------\n",
        "scaler = StandardScaler()\n",
        "normalized_data = scaler.fit_transform(relevant_data)\n",
        "kmeans = KMeans(n_clusters=8, random_state=42)\n",
        "clusters = kmeans.fit_predict(normalized_data)\n",
        "print(\"Performed KMeans clustering into 8 clusters\")\n",
        "\n",
        "# Compute boundary stats\n",
        "cluster_boundaries = []\n",
        "for cluster_id in range(8):\n",
        "    cluster_data = normalized_data[clusters == cluster_id]\n",
        "    min_v = scaler.inverse_transform(np.min(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    median_v = scaler.inverse_transform(np.median(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    mean_v = scaler.inverse_transform(np.mean(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    max_v = scaler.inverse_transform(np.max(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    cluster_boundaries.append((min_v, median_v, mean_v, max_v))\n",
        "cluster_boundaries = np.array(cluster_boundaries)\n",
        "print(f\"Cluster boundary stats calculated: cluster_boundaries.shape = {cluster_boundaries.shape}\")\n",
        "\n",
        "# --------------------- Merge Static Features with Clusters ---------------------\n",
        "Clustering_link_table = pd.DataFrame({'file_id': file_id_order.astype(int), 'cluster': clusters})\n",
        "Clustering_link_table = Clustering_link_table.sort_values(by='file_id').reset_index(drop=True)\n",
        "merged_df = pd.merge(Input_Link_Table, Clustering_link_table, on='file_id')\n",
        "print(f\"Final input features (static + cluster): merged_df.shape = {merged_df.shape}\")\n",
        "\n",
        "# --------------------- Create Output Time Series DataFrame ---------------------\n",
        "data = [[file_id_order[i].astype(int), t, relevant_data[i, t]] for i in range(len(file_id_order)) for t in range(max_timesteps)]\n",
        "df_output = pd.DataFrame(data, columns=['file_id', 'timestep', 'CO2']).sort_values(by=['file_id', 'timestep'])\n",
        "print(f\"Final output time series: df_output.shape = {df_output.shape}\")\n",
        "\n",
        "# --------------------- Summary ---------------------\n",
        "print(\"Data Preparation Summary:\")\n",
        "print(f\"Static Input Table: merged_df [{merged_df.shape[0]} rows × {merged_df.shape[1]} columns]\")\n",
        "print(f\"Time Series Output: df_output [{df_output.shape[0]} rows × 3 columns]\")\n",
        "print(f\"Cluster Boundaries: cluster_boundaries [{cluster_boundaries.shape}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLinear**"
      ],
      "metadata": {
        "id": "TToXRA993wzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NLinear(nn.Module):\n",
        "    def __init__(self, seq_len, pred_len, individual=False):\n",
        "        super(NLinear, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.individual = individual\n",
        "\n",
        "        if self.individual:\n",
        "            self.Linear = nn.ModuleList()\n",
        "            for i in range(1):  # Univariate case\n",
        "                self.Linear.append(nn.Linear(self.seq_len, self.pred_len))\n",
        "        else:\n",
        "            self.Linear = nn.Linear(self.seq_len, self.pred_len)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) == 3:\n",
        "            x = x.squeeze(-1)\n",
        "\n",
        "        seq_last = x[:, -1:].detach()\n",
        "        x = x - seq_last\n",
        "\n",
        "        if self.individual:\n",
        "            out = torch.zeros([x.size(0), self.pred_len], dtype=x.dtype).to(x.device)\n",
        "            for i in range(1):\n",
        "                out[:, :] = self.Linear[i](x)\n",
        "        else:\n",
        "            out = self.Linear(x)\n",
        "\n",
        "        out = out + seq_last\n",
        "        return out.unsqueeze(-1)"
      ],
      "metadata": {
        "id": "GRtDheQx3zj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nlinear experiment**"
      ],
      "metadata": {
        "id": "BVQUzali2Ebe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Model Definition\n",
        "class NLinear(nn.Module):\n",
        "    def __init__(self, seq_len, pred_len, individual=False):\n",
        "        super(NLinear, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.individual = individual\n",
        "        if self.individual:\n",
        "            self.Linear = nn.ModuleList([nn.Linear(self.seq_len, self.pred_len)])\n",
        "        else:\n",
        "            self.Linear = nn.Linear(self.seq_len, self.pred_len)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) == 3:\n",
        "            x = x.squeeze(-1)\n",
        "        seq_last = x[:, -1:].detach()\n",
        "        x = x - seq_last\n",
        "        if self.individual:\n",
        "            out = torch.zeros([x.size(0), self.pred_len], dtype=x.dtype).to(x.device)\n",
        "            for i in range(1):\n",
        "                out[:, :] = self.Linear[i](x)\n",
        "        else:\n",
        "            out = self.Linear(x)\n",
        "        out = out + seq_last\n",
        "        return out.unsqueeze(-1)\n",
        "\n",
        "# Hyperparameters and Output Storage\n",
        "nlinear_config = {\n",
        "    'epochs': 500,\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 0.001,\n",
        "    'individual': False,\n",
        "    'model_name': 'NLinear'\n",
        "}\n",
        "\n",
        "NLinear_mse = pd.DataFrame(columns=['Split', 'Test_MSE'])\n",
        "splits = [(80, 20), (60, 40), (50, 50), (40, 60), (20, 80), (10, 90), (5, 95), (3, 97), (1, 99)]\n",
        "drive_path = '/content/drive/MyDrive/DSSM-Figures'\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "for train_pct, test_pct in splits:\n",
        "    split_name = f\"{train_pct}_{test_pct}\"\n",
        "    print(f\"\\n==== Running Split: {split_name} ====\")\n",
        "\n",
        "    # Split file_ids\n",
        "    file_ids = df_output['file_id'].unique()\n",
        "    trainval_ids, test_ids = train_test_split(file_ids, test_size=0.2, random_state=42)\n",
        "    train_ids, val_ids = train_test_split(trainval_ids, test_size=0.2, random_state=42)\n",
        "\n",
        "    def extract_X_Y(ids, pct):\n",
        "        df_subset = df_output[df_output['file_id'].isin(ids)]\n",
        "        pivoted = df_subset.pivot(index='file_id', columns='timestep', values='CO2').values\n",
        "        split_idx = int(pct / 100 * 101)\n",
        "        X = pivoted[:, :split_idx]\n",
        "        Y = pivoted[:, split_idx:]\n",
        "        return X, Y\n",
        "\n",
        "    X_train, Y_train = extract_X_Y(train_ids, train_pct)\n",
        "    X_val, Y_val = extract_X_Y(val_ids, train_pct)\n",
        "    X_test, Y_test = extract_X_Y(test_ids, train_pct)\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train[:, :, None], dtype=torch.float32)\n",
        "    Y_train_tensor = torch.tensor(Y_train[:, :, None], dtype=torch.float32)\n",
        "    X_val_tensor = torch.tensor(X_val[:, :, None], dtype=torch.float32)\n",
        "    Y_val_tensor = torch.tensor(Y_val[:, :, None], dtype=torch.float32)\n",
        "    X_test_tensor = torch.tensor(X_test[:, :, None], dtype=torch.float32)\n",
        "    Y_test_tensor = torch.tensor(Y_test[:, :, None], dtype=torch.float32)\n",
        "\n",
        "    print(f\"Split {split_name} — X (INPUT): {X_train.shape[1]}, Y (OUTPUT): {Y_train.shape[1]}\")\n",
        "    print(f\"Train: {X_train_tensor.shape}, Val: {X_val_tensor.shape}, Test: {X_test_tensor.shape}\")\n",
        "\n",
        "    train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
        "    val_dataset = TensorDataset(X_val_tensor, Y_val_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=nlinear_config['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=nlinear_config['batch_size'])\n",
        "    test_loader = DataLoader(test_dataset, batch_size=nlinear_config['batch_size'])\n",
        "\n",
        "    model = NLinear(seq_len=X_train.shape[1], pred_len=Y_train.shape[1], individual=nlinear_config['individual'])\n",
        "    optimizer = optim.Adam(model.parameters(), lr=nlinear_config['learning_rate'])\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "\n",
        "    print(\"Training started...\")\n",
        "    for epoch in range(nlinear_config['epochs']):\n",
        "        model.train()\n",
        "        for X_batch, Y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X_batch)\n",
        "            loss = criterion(preds, Y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, Y_batch in val_loader:\n",
        "                preds = model(X_batch)\n",
        "                val_loss += criterion(preds, Y_batch).item()\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}: Train Loss = {loss.item():.6f}, Val Loss = {val_loss:.6f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict()\n",
        "            #print(f\"New best model found at epoch {epoch+1} with val loss {val_loss:.6f}\")\n",
        "            torch.save(best_model_state, os.path.join(drive_path, f\"best_model_{split_name}.pt\"))\n",
        "\n",
        "    model.load_state_dict(torch.load(os.path.join(drive_path, f\"best_model_{split_name}.pt\")))\n",
        "    model.eval()\n",
        "\n",
        "    total_mse = 0.0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, Y_batch in test_loader:\n",
        "            outputs = model(X_batch)\n",
        "            batch_mse = criterion(outputs, Y_batch).item()\n",
        "            total_mse += batch_mse * X_batch.size(0)\n",
        "            total_samples += X_batch.size(0)\n",
        "\n",
        "    avg_mse = total_mse / total_samples\n",
        "    print(f\"Final Test MSE ({split_name}): {avg_mse:.6f}\")\n",
        "    NLinear_mse.loc[len(NLinear_mse)] = [split_name, avg_mse]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eojVeGCUFqoi",
        "outputId": "3752a039-15c0-4e18-cff9-5f256936ede3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Running Split: 80_20 ====\n",
            "Split 80_20 — X (INPUT): 80, Y (OUTPUT): 21\n",
            "Train: torch.Size([1729, 80, 1]), Val: torch.Size([433, 80, 1]), Test: torch.Size([541, 80, 1])\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.000252, Val Loss = 0.000861\n",
            "Epoch 20: Train Loss = 0.000139, Val Loss = 0.000282\n",
            "Epoch 30: Train Loss = 0.000080, Val Loss = 0.000136\n",
            "Epoch 40: Train Loss = 0.000057, Val Loss = 0.000084\n",
            "Epoch 50: Train Loss = 0.000020, Val Loss = 0.000071\n",
            "Epoch 60: Train Loss = 0.000014, Val Loss = 0.000048\n",
            "Epoch 70: Train Loss = 0.000070, Val Loss = 0.000042\n",
            "Epoch 80: Train Loss = 0.000024, Val Loss = 0.000035\n",
            "Epoch 90: Train Loss = 0.000076, Val Loss = 0.000053\n",
            "Epoch 100: Train Loss = 0.000030, Val Loss = 0.000031\n",
            "Epoch 110: Train Loss = 0.000014, Val Loss = 0.000035\n",
            "Epoch 120: Train Loss = 0.000027, Val Loss = 0.000030\n",
            "Epoch 130: Train Loss = 0.000064, Val Loss = 0.000019\n",
            "Epoch 140: Train Loss = 0.000017, Val Loss = 0.000017\n",
            "Epoch 150: Train Loss = 0.000003, Val Loss = 0.000017\n",
            "Epoch 160: Train Loss = 0.000052, Val Loss = 0.000035\n",
            "Epoch 170: Train Loss = 0.000014, Val Loss = 0.000016\n",
            "Epoch 180: Train Loss = 0.000003, Val Loss = 0.000024\n",
            "Epoch 190: Train Loss = 0.000014, Val Loss = 0.000034\n",
            "Epoch 200: Train Loss = 0.000015, Val Loss = 0.000037\n",
            "Epoch 210: Train Loss = 0.000009, Val Loss = 0.000016\n",
            "Epoch 220: Train Loss = 0.000002, Val Loss = 0.000014\n",
            "Epoch 230: Train Loss = 0.000003, Val Loss = 0.000017\n",
            "Epoch 240: Train Loss = 0.000004, Val Loss = 0.000015\n",
            "Epoch 250: Train Loss = 0.000029, Val Loss = 0.000016\n",
            "Epoch 260: Train Loss = 0.000015, Val Loss = 0.000019\n",
            "Epoch 270: Train Loss = 0.000001, Val Loss = 0.000013\n",
            "Epoch 280: Train Loss = 0.000005, Val Loss = 0.000015\n",
            "Epoch 290: Train Loss = 0.000004, Val Loss = 0.000024\n",
            "Epoch 300: Train Loss = 0.000007, Val Loss = 0.000022\n",
            "Epoch 310: Train Loss = 0.000004, Val Loss = 0.000023\n",
            "Epoch 320: Train Loss = 0.000004, Val Loss = 0.000014\n",
            "Epoch 330: Train Loss = 0.000006, Val Loss = 0.000023\n",
            "Epoch 340: Train Loss = 0.000009, Val Loss = 0.000028\n",
            "Epoch 350: Train Loss = 0.000008, Val Loss = 0.000014\n",
            "Epoch 360: Train Loss = 0.000004, Val Loss = 0.000016\n",
            "Epoch 370: Train Loss = 0.000007, Val Loss = 0.000020\n",
            "Epoch 380: Train Loss = 0.000002, Val Loss = 0.000022\n",
            "Epoch 390: Train Loss = 0.000008, Val Loss = 0.000015\n",
            "Epoch 400: Train Loss = 0.000012, Val Loss = 0.000025\n",
            "Epoch 410: Train Loss = 0.000003, Val Loss = 0.000016\n",
            "Epoch 420: Train Loss = 0.000001, Val Loss = 0.000017\n",
            "Epoch 430: Train Loss = 0.000005, Val Loss = 0.000014\n",
            "Epoch 440: Train Loss = 0.000003, Val Loss = 0.000011\n",
            "Epoch 450: Train Loss = 0.000001, Val Loss = 0.000016\n",
            "Epoch 460: Train Loss = 0.000011, Val Loss = 0.000012\n",
            "Epoch 470: Train Loss = 0.000002, Val Loss = 0.000013\n",
            "Epoch 480: Train Loss = 0.000222, Val Loss = 0.000412\n",
            "Epoch 490: Train Loss = 0.000009, Val Loss = 0.000027\n",
            "Epoch 500: Train Loss = 0.000020, Val Loss = 0.000026\n",
            "Final Test MSE (80_20): 0.000010\n",
            "\n",
            "==== Running Split: 60_40 ====\n",
            "Split 60_40 — X (INPUT): 60, Y (OUTPUT): 41\n",
            "Train: torch.Size([1729, 60, 1]), Val: torch.Size([433, 60, 1]), Test: torch.Size([541, 60, 1])\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.000303, Val Loss = 0.001777\n",
            "Epoch 20: Train Loss = 0.000478, Val Loss = 0.000875\n",
            "Epoch 30: Train Loss = 0.000122, Val Loss = 0.000629\n",
            "Epoch 40: Train Loss = 0.000196, Val Loss = 0.000503\n",
            "Epoch 50: Train Loss = 0.000311, Val Loss = 0.000461\n",
            "Epoch 60: Train Loss = 0.000213, Val Loss = 0.000428\n",
            "Epoch 70: Train Loss = 0.000062, Val Loss = 0.000411\n",
            "Epoch 80: Train Loss = 0.000071, Val Loss = 0.000396\n",
            "Epoch 90: Train Loss = 0.000207, Val Loss = 0.000387\n",
            "Epoch 100: Train Loss = 0.000888, Val Loss = 0.000381\n",
            "Epoch 110: Train Loss = 0.001096, Val Loss = 0.000414\n",
            "Epoch 120: Train Loss = 0.000604, Val Loss = 0.000339\n",
            "Epoch 130: Train Loss = 0.000318, Val Loss = 0.000323\n",
            "Epoch 140: Train Loss = 0.000548, Val Loss = 0.000320\n",
            "Epoch 150: Train Loss = 0.000121, Val Loss = 0.000314\n",
            "Epoch 160: Train Loss = 0.000180, Val Loss = 0.000316\n",
            "Epoch 170: Train Loss = 0.000014, Val Loss = 0.000328\n",
            "Epoch 180: Train Loss = 0.000014, Val Loss = 0.000318\n",
            "Epoch 190: Train Loss = 0.000072, Val Loss = 0.000380\n",
            "Epoch 200: Train Loss = 0.000030, Val Loss = 0.000287\n",
            "Epoch 210: Train Loss = 0.000363, Val Loss = 0.000333\n",
            "Epoch 220: Train Loss = 0.000511, Val Loss = 0.000315\n",
            "Epoch 230: Train Loss = 0.000299, Val Loss = 0.000278\n",
            "Epoch 240: Train Loss = 0.000356, Val Loss = 0.000280\n",
            "Epoch 250: Train Loss = 0.000037, Val Loss = 0.000279\n",
            "Epoch 260: Train Loss = 0.000216, Val Loss = 0.000397\n",
            "Epoch 270: Train Loss = 0.000277, Val Loss = 0.000258\n",
            "Epoch 280: Train Loss = 0.000325, Val Loss = 0.000258\n",
            "Epoch 290: Train Loss = 0.000013, Val Loss = 0.000264\n",
            "Epoch 300: Train Loss = 0.000101, Val Loss = 0.000252\n",
            "Epoch 310: Train Loss = 0.000035, Val Loss = 0.000226\n",
            "Epoch 320: Train Loss = 0.000012, Val Loss = 0.000256\n",
            "Epoch 330: Train Loss = 0.000083, Val Loss = 0.000250\n",
            "Epoch 340: Train Loss = 0.000234, Val Loss = 0.000247\n",
            "Epoch 350: Train Loss = 0.000197, Val Loss = 0.000258\n",
            "Epoch 360: Train Loss = 0.000056, Val Loss = 0.000212\n",
            "Epoch 370: Train Loss = 0.000118, Val Loss = 0.000214\n",
            "Epoch 380: Train Loss = 0.000138, Val Loss = 0.000228\n",
            "Epoch 390: Train Loss = 0.000005, Val Loss = 0.000209\n",
            "Epoch 400: Train Loss = 0.000005, Val Loss = 0.000246\n",
            "Epoch 410: Train Loss = 0.000046, Val Loss = 0.000219\n",
            "Epoch 420: Train Loss = 0.000111, Val Loss = 0.000255\n",
            "Epoch 430: Train Loss = 0.000009, Val Loss = 0.000217\n",
            "Epoch 440: Train Loss = 0.000211, Val Loss = 0.000216\n",
            "Epoch 450: Train Loss = 0.000012, Val Loss = 0.000200\n",
            "Epoch 460: Train Loss = 0.000170, Val Loss = 0.000192\n",
            "Epoch 470: Train Loss = 0.000263, Val Loss = 0.000206\n",
            "Epoch 480: Train Loss = 0.000407, Val Loss = 0.000205\n",
            "Epoch 490: Train Loss = 0.000707, Val Loss = 0.000207\n",
            "Epoch 500: Train Loss = 0.000289, Val Loss = 0.000255\n",
            "Final Test MSE (60_40): 0.000140\n",
            "\n",
            "==== Running Split: 50_50 ====\n",
            "Split 50_50 — X (INPUT): 50, Y (OUTPUT): 51\n",
            "Train: torch.Size([1729, 50, 1]), Val: torch.Size([433, 50, 1]), Test: torch.Size([541, 50, 1])\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.009217, Val Loss = 0.002435\n",
            "Epoch 20: Train Loss = 0.000424, Val Loss = 0.001495\n",
            "Epoch 30: Train Loss = 0.000320, Val Loss = 0.001156\n",
            "Epoch 40: Train Loss = 0.001463, Val Loss = 0.001039\n",
            "Epoch 50: Train Loss = 0.001377, Val Loss = 0.000943\n",
            "Epoch 60: Train Loss = 0.000545, Val Loss = 0.000888\n",
            "Epoch 70: Train Loss = 0.000376, Val Loss = 0.000860\n",
            "Epoch 80: Train Loss = 0.000060, Val Loss = 0.000797\n",
            "Epoch 90: Train Loss = 0.000243, Val Loss = 0.000773\n",
            "Epoch 100: Train Loss = 0.000771, Val Loss = 0.000746\n",
            "Epoch 110: Train Loss = 0.000166, Val Loss = 0.000735\n",
            "Epoch 120: Train Loss = 0.000120, Val Loss = 0.000699\n",
            "Epoch 130: Train Loss = 0.000037, Val Loss = 0.000725\n",
            "Epoch 140: Train Loss = 0.000043, Val Loss = 0.000703\n",
            "Epoch 150: Train Loss = 0.003217, Val Loss = 0.000756\n",
            "Epoch 160: Train Loss = 0.001669, Val Loss = 0.000681\n",
            "Epoch 170: Train Loss = 0.000431, Val Loss = 0.000630\n",
            "Epoch 180: Train Loss = 0.000084, Val Loss = 0.000623\n",
            "Epoch 190: Train Loss = 0.000221, Val Loss = 0.000693\n",
            "Epoch 200: Train Loss = 0.000023, Val Loss = 0.000598\n",
            "Epoch 210: Train Loss = 0.001004, Val Loss = 0.000586\n",
            "Epoch 220: Train Loss = 0.000060, Val Loss = 0.000574\n",
            "Epoch 230: Train Loss = 0.000265, Val Loss = 0.000588\n",
            "Epoch 240: Train Loss = 0.000082, Val Loss = 0.000562\n",
            "Epoch 250: Train Loss = 0.000074, Val Loss = 0.000572\n",
            "Epoch 260: Train Loss = 0.000048, Val Loss = 0.000564\n",
            "Epoch 270: Train Loss = 0.000026, Val Loss = 0.000551\n",
            "Epoch 280: Train Loss = 0.000230, Val Loss = 0.000550\n",
            "Epoch 290: Train Loss = 0.000282, Val Loss = 0.000545\n",
            "Epoch 300: Train Loss = 0.000478, Val Loss = 0.000690\n",
            "Epoch 310: Train Loss = 0.000010, Val Loss = 0.000521\n",
            "Epoch 320: Train Loss = 0.000023, Val Loss = 0.000525\n",
            "Epoch 330: Train Loss = 0.000140, Val Loss = 0.000533\n",
            "Epoch 340: Train Loss = 0.000984, Val Loss = 0.000555\n",
            "Epoch 350: Train Loss = 0.000016, Val Loss = 0.000523\n",
            "Epoch 360: Train Loss = 0.000068, Val Loss = 0.000523\n",
            "Epoch 370: Train Loss = 0.000216, Val Loss = 0.000530\n",
            "Epoch 380: Train Loss = 0.000005, Val Loss = 0.000524\n",
            "Epoch 390: Train Loss = 0.000013, Val Loss = 0.000504\n",
            "Epoch 400: Train Loss = 0.000028, Val Loss = 0.000526\n",
            "Epoch 410: Train Loss = 0.000067, Val Loss = 0.000518\n",
            "Epoch 420: Train Loss = 0.000218, Val Loss = 0.000509\n",
            "Epoch 430: Train Loss = 0.000119, Val Loss = 0.000489\n",
            "Epoch 440: Train Loss = 0.000037, Val Loss = 0.000488\n",
            "Epoch 450: Train Loss = 0.000089, Val Loss = 0.000507\n",
            "Epoch 460: Train Loss = 0.000646, Val Loss = 0.000518\n",
            "Epoch 470: Train Loss = 0.000612, Val Loss = 0.000485\n",
            "Epoch 480: Train Loss = 0.000573, Val Loss = 0.000487\n",
            "Epoch 490: Train Loss = 0.000040, Val Loss = 0.000477\n",
            "Epoch 500: Train Loss = 0.000027, Val Loss = 0.000492\n",
            "Final Test MSE (50_50): 0.000340\n",
            "\n",
            "==== Running Split: 40_60 ====\n",
            "Split 40_60 — X (INPUT): 40, Y (OUTPUT): 61\n",
            "Train: torch.Size([1729, 40, 1]), Val: torch.Size([433, 40, 1]), Test: torch.Size([541, 40, 1])\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.000360, Val Loss = 0.005304\n",
            "Epoch 20: Train Loss = 0.000665, Val Loss = 0.003680\n",
            "Epoch 30: Train Loss = 0.002173, Val Loss = 0.002856\n",
            "Epoch 40: Train Loss = 0.000146, Val Loss = 0.002396\n",
            "Epoch 50: Train Loss = 0.001300, Val Loss = 0.002165\n",
            "Epoch 60: Train Loss = 0.000249, Val Loss = 0.001992\n",
            "Epoch 70: Train Loss = 0.000246, Val Loss = 0.001925\n",
            "Epoch 80: Train Loss = 0.000042, Val Loss = 0.001835\n",
            "Epoch 90: Train Loss = 0.000391, Val Loss = 0.001790\n",
            "Epoch 100: Train Loss = 0.000064, Val Loss = 0.001757\n",
            "Epoch 110: Train Loss = 0.002834, Val Loss = 0.001835\n",
            "Epoch 120: Train Loss = 0.000073, Val Loss = 0.001727\n",
            "Epoch 130: Train Loss = 0.000165, Val Loss = 0.001738\n",
            "Epoch 140: Train Loss = 0.003266, Val Loss = 0.001797\n",
            "Epoch 150: Train Loss = 0.001253, Val Loss = 0.001702\n",
            "Epoch 160: Train Loss = 0.006600, Val Loss = 0.001833\n",
            "Epoch 170: Train Loss = 0.000102, Val Loss = 0.001726\n",
            "Epoch 180: Train Loss = 0.000350, Val Loss = 0.001717\n",
            "Epoch 190: Train Loss = 0.002583, Val Loss = 0.001727\n",
            "Epoch 200: Train Loss = 0.000088, Val Loss = 0.001710\n",
            "Epoch 210: Train Loss = 0.000551, Val Loss = 0.001711\n",
            "Epoch 220: Train Loss = 0.000192, Val Loss = 0.001712\n",
            "Epoch 230: Train Loss = 0.000046, Val Loss = 0.001731\n",
            "Epoch 240: Train Loss = 0.001374, Val Loss = 0.001765\n",
            "Epoch 250: Train Loss = 0.002295, Val Loss = 0.001773\n",
            "Epoch 260: Train Loss = 0.001603, Val Loss = 0.001739\n",
            "Epoch 270: Train Loss = 0.000045, Val Loss = 0.001703\n",
            "Epoch 280: Train Loss = 0.000116, Val Loss = 0.001728\n",
            "Epoch 290: Train Loss = 0.000525, Val Loss = 0.001705\n",
            "Epoch 300: Train Loss = 0.000066, Val Loss = 0.001708\n",
            "Epoch 310: Train Loss = 0.020099, Val Loss = 0.002051\n",
            "Epoch 320: Train Loss = 0.000184, Val Loss = 0.001761\n",
            "Epoch 330: Train Loss = 0.000024, Val Loss = 0.001751\n",
            "Epoch 340: Train Loss = 0.000207, Val Loss = 0.001738\n",
            "Epoch 350: Train Loss = 0.000143, Val Loss = 0.001737\n",
            "Epoch 360: Train Loss = 0.000109, Val Loss = 0.001751\n",
            "Epoch 370: Train Loss = 0.000518, Val Loss = 0.001731\n",
            "Epoch 380: Train Loss = 0.000092, Val Loss = 0.001768\n",
            "Epoch 390: Train Loss = 0.001094, Val Loss = 0.001830\n",
            "Epoch 400: Train Loss = 0.000054, Val Loss = 0.001763\n",
            "Epoch 410: Train Loss = 0.002475, Val Loss = 0.001891\n",
            "Epoch 420: Train Loss = 0.001794, Val Loss = 0.001818\n",
            "Epoch 430: Train Loss = 0.005196, Val Loss = 0.001904\n",
            "Epoch 440: Train Loss = 0.000043, Val Loss = 0.001745\n",
            "Epoch 450: Train Loss = 0.000018, Val Loss = 0.001765\n",
            "Epoch 460: Train Loss = 0.002093, Val Loss = 0.001815\n",
            "Epoch 470: Train Loss = 0.001090, Val Loss = 0.001789\n",
            "Epoch 480: Train Loss = 0.000189, Val Loss = 0.001762\n",
            "Epoch 490: Train Loss = 0.001055, Val Loss = 0.001763\n",
            "Epoch 500: Train Loss = 0.002784, Val Loss = 0.001834\n",
            "Final Test MSE (40_60): 0.001077\n",
            "\n",
            "==== Running Split: 20_80 ====\n",
            "Split 20_80 — X (INPUT): 20, Y (OUTPUT): 81\n",
            "Train: torch.Size([1729, 20, 1]), Val: torch.Size([433, 20, 1]), Test: torch.Size([541, 20, 1])\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.019322, Val Loss = 0.024558\n",
            "Epoch 20: Train Loss = 0.005497, Val Loss = 0.021164\n",
            "Epoch 30: Train Loss = 0.000463, Val Loss = 0.019535\n",
            "Epoch 40: Train Loss = 0.008600, Val Loss = 0.018393\n",
            "Epoch 50: Train Loss = 0.004983, Val Loss = 0.017545\n",
            "Epoch 60: Train Loss = 0.045409, Val Loss = 0.016755\n",
            "Epoch 70: Train Loss = 0.080640, Val Loss = 0.015990\n",
            "Epoch 80: Train Loss = 0.000391, Val Loss = 0.015407\n",
            "Epoch 90: Train Loss = 0.001378, Val Loss = 0.014898\n",
            "Epoch 100: Train Loss = 0.004271, Val Loss = 0.014529\n",
            "Epoch 110: Train Loss = 0.002601, Val Loss = 0.014101\n",
            "Epoch 120: Train Loss = 0.000520, Val Loss = 0.013684\n",
            "Epoch 130: Train Loss = 0.020001, Val Loss = 0.013416\n",
            "Epoch 140: Train Loss = 0.023815, Val Loss = 0.013147\n",
            "Epoch 150: Train Loss = 0.032610, Val Loss = 0.012946\n",
            "Epoch 160: Train Loss = 0.000888, Val Loss = 0.012833\n",
            "Epoch 170: Train Loss = 0.043859, Val Loss = 0.012724\n",
            "Epoch 180: Train Loss = 0.000679, Val Loss = 0.012525\n",
            "Epoch 190: Train Loss = 0.000873, Val Loss = 0.012377\n",
            "Epoch 200: Train Loss = 0.000109, Val Loss = 0.012272\n",
            "Epoch 210: Train Loss = 0.002830, Val Loss = 0.012296\n",
            "Epoch 220: Train Loss = 0.003989, Val Loss = 0.012170\n",
            "Epoch 230: Train Loss = 0.001545, Val Loss = 0.012163\n",
            "Epoch 240: Train Loss = 0.003911, Val Loss = 0.012134\n",
            "Epoch 250: Train Loss = 0.003007, Val Loss = 0.012067\n",
            "Epoch 260: Train Loss = 0.000229, Val Loss = 0.012017\n",
            "Epoch 270: Train Loss = 0.000029, Val Loss = 0.012004\n",
            "Epoch 280: Train Loss = 0.114010, Val Loss = 0.012017\n",
            "Epoch 290: Train Loss = 0.000605, Val Loss = 0.011987\n",
            "Epoch 300: Train Loss = 0.033819, Val Loss = 0.011875\n",
            "Epoch 310: Train Loss = 0.001588, Val Loss = 0.011865\n",
            "Epoch 320: Train Loss = 0.000077, Val Loss = 0.011839\n",
            "Epoch 330: Train Loss = 0.000723, Val Loss = 0.011808\n",
            "Epoch 340: Train Loss = 0.028437, Val Loss = 0.011853\n",
            "Epoch 350: Train Loss = 0.003750, Val Loss = 0.011887\n",
            "Epoch 360: Train Loss = 0.000397, Val Loss = 0.011737\n",
            "Epoch 370: Train Loss = 0.005432, Val Loss = 0.011687\n",
            "Epoch 380: Train Loss = 0.004813, Val Loss = 0.011682\n",
            "Epoch 390: Train Loss = 0.000125, Val Loss = 0.011653\n",
            "Epoch 400: Train Loss = 0.020263, Val Loss = 0.011658\n",
            "Epoch 410: Train Loss = 0.002075, Val Loss = 0.011709\n",
            "Epoch 420: Train Loss = 0.032754, Val Loss = 0.011606\n",
            "Epoch 430: Train Loss = 0.002564, Val Loss = 0.011570\n",
            "Epoch 440: Train Loss = 0.000378, Val Loss = 0.011603\n",
            "Epoch 450: Train Loss = 0.004954, Val Loss = 0.011564\n",
            "Epoch 460: Train Loss = 0.074457, Val Loss = 0.011528\n",
            "Epoch 470: Train Loss = 0.003963, Val Loss = 0.011504\n",
            "Epoch 480: Train Loss = 0.035914, Val Loss = 0.011592\n",
            "Epoch 490: Train Loss = 0.001799, Val Loss = 0.011476\n",
            "Epoch 500: Train Loss = 0.000039, Val Loss = 0.011466\n",
            "Final Test MSE (20_80): 0.009894\n",
            "\n",
            "==== Running Split: 10_90 ====\n",
            "Split 10_90 — X (INPUT): 10, Y (OUTPUT): 91\n",
            "Train: torch.Size([1729, 10, 1]), Val: torch.Size([433, 10, 1]), Test: torch.Size([541, 10, 1])\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.047875, Val Loss = 0.067081\n",
            "Epoch 20: Train Loss = 0.024967, Val Loss = 0.049318\n",
            "Epoch 30: Train Loss = 0.017496, Val Loss = 0.046169\n",
            "Epoch 40: Train Loss = 0.002188, Val Loss = 0.044183\n",
            "Epoch 50: Train Loss = 0.180149, Val Loss = 0.042576\n",
            "Epoch 60: Train Loss = 0.001022, Val Loss = 0.041428\n",
            "Epoch 70: Train Loss = 0.002233, Val Loss = 0.040392\n",
            "Epoch 80: Train Loss = 0.023350, Val Loss = 0.039407\n",
            "Epoch 90: Train Loss = 0.056711, Val Loss = 0.038593\n",
            "Epoch 100: Train Loss = 0.006760, Val Loss = 0.037812\n",
            "Epoch 110: Train Loss = 0.011377, Val Loss = 0.036962\n",
            "Epoch 120: Train Loss = 0.000432, Val Loss = 0.036322\n",
            "Epoch 130: Train Loss = 0.206308, Val Loss = 0.035777\n",
            "Epoch 140: Train Loss = 0.026053, Val Loss = 0.035515\n",
            "Epoch 150: Train Loss = 0.003525, Val Loss = 0.035114\n",
            "Epoch 160: Train Loss = 0.010922, Val Loss = 0.034754\n",
            "Epoch 170: Train Loss = 0.012301, Val Loss = 0.034411\n",
            "Epoch 180: Train Loss = 0.001933, Val Loss = 0.034131\n",
            "Epoch 190: Train Loss = 0.001557, Val Loss = 0.033797\n",
            "Epoch 200: Train Loss = 0.008552, Val Loss = 0.033490\n",
            "Epoch 210: Train Loss = 0.000892, Val Loss = 0.033547\n",
            "Epoch 220: Train Loss = 0.016115, Val Loss = 0.033521\n",
            "Epoch 230: Train Loss = 0.029911, Val Loss = 0.033135\n",
            "Epoch 240: Train Loss = 0.004804, Val Loss = 0.033031\n",
            "Epoch 250: Train Loss = 0.054639, Val Loss = 0.032786\n",
            "Epoch 260: Train Loss = 0.000962, Val Loss = 0.032945\n",
            "Epoch 270: Train Loss = 0.004995, Val Loss = 0.032703\n",
            "Epoch 280: Train Loss = 0.001762, Val Loss = 0.032729\n",
            "Epoch 290: Train Loss = 0.020403, Val Loss = 0.032460\n",
            "Epoch 300: Train Loss = 0.000193, Val Loss = 0.032355\n",
            "Epoch 310: Train Loss = 0.002003, Val Loss = 0.032469\n",
            "Epoch 320: Train Loss = 0.031551, Val Loss = 0.032440\n",
            "Epoch 330: Train Loss = 0.006501, Val Loss = 0.032324\n",
            "Epoch 340: Train Loss = 0.339192, Val Loss = 0.032099\n",
            "Epoch 350: Train Loss = 0.000650, Val Loss = 0.032063\n",
            "Epoch 360: Train Loss = 0.013626, Val Loss = 0.032075\n",
            "Epoch 370: Train Loss = 0.013767, Val Loss = 0.032135\n",
            "Epoch 380: Train Loss = 0.022344, Val Loss = 0.031985\n",
            "Epoch 390: Train Loss = 0.000268, Val Loss = 0.031859\n",
            "Epoch 400: Train Loss = 0.062157, Val Loss = 0.031887\n",
            "Epoch 410: Train Loss = 0.033968, Val Loss = 0.031903\n",
            "Epoch 420: Train Loss = 0.006639, Val Loss = 0.031916\n",
            "Epoch 430: Train Loss = 0.001139, Val Loss = 0.031742\n",
            "Epoch 440: Train Loss = 0.082944, Val Loss = 0.031886\n",
            "Epoch 450: Train Loss = 0.001169, Val Loss = 0.031643\n",
            "Epoch 460: Train Loss = 0.017865, Val Loss = 0.031672\n",
            "Epoch 470: Train Loss = 0.005394, Val Loss = 0.031521\n",
            "Epoch 480: Train Loss = 0.019450, Val Loss = 0.031510\n",
            "Epoch 490: Train Loss = 0.011208, Val Loss = 0.031622\n",
            "Epoch 500: Train Loss = 0.014290, Val Loss = 0.031501\n",
            "Final Test MSE (10_90): 0.024607\n",
            "\n",
            "==== Running Split: 5_95 ====\n",
            "Split 5_95 — X (INPUT): 5, Y (OUTPUT): 96\n",
            "Train: torch.Size([1729, 5, 1]), Val: torch.Size([433, 5, 1]), Test: torch.Size([541, 5, 1])\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.149409, Val Loss = 0.269215\n",
            "Epoch 20: Train Loss = 0.050573, Val Loss = 0.160866\n",
            "Epoch 30: Train Loss = 0.206517, Val Loss = 0.135086\n",
            "Epoch 40: Train Loss = 0.009569, Val Loss = 0.117364\n",
            "Epoch 50: Train Loss = 0.004146, Val Loss = 0.102821\n",
            "Epoch 60: Train Loss = 0.022807, Val Loss = 0.090316\n",
            "Epoch 70: Train Loss = 0.030980, Val Loss = 0.079928\n",
            "Epoch 80: Train Loss = 0.221336, Val Loss = 0.071386\n",
            "Epoch 90: Train Loss = 0.336359, Val Loss = 0.064476\n",
            "Epoch 100: Train Loss = 0.191531, Val Loss = 0.059035\n",
            "Epoch 110: Train Loss = 0.006188, Val Loss = 0.054802\n",
            "Epoch 120: Train Loss = 0.138467, Val Loss = 0.051655\n",
            "Epoch 130: Train Loss = 0.009110, Val Loss = 0.049151\n",
            "Epoch 140: Train Loss = 0.002170, Val Loss = 0.047389\n",
            "Epoch 150: Train Loss = 0.001793, Val Loss = 0.046206\n",
            "Epoch 160: Train Loss = 0.035448, Val Loss = 0.044990\n",
            "Epoch 170: Train Loss = 0.005896, Val Loss = 0.044221\n",
            "Epoch 180: Train Loss = 0.059003, Val Loss = 0.043668\n",
            "Epoch 190: Train Loss = 0.001215, Val Loss = 0.043236\n",
            "Epoch 200: Train Loss = 0.036647, Val Loss = 0.042927\n",
            "Epoch 210: Train Loss = 0.040295, Val Loss = 0.042651\n",
            "Epoch 220: Train Loss = 0.000956, Val Loss = 0.042411\n",
            "Epoch 230: Train Loss = 0.463735, Val Loss = 0.042325\n",
            "Epoch 240: Train Loss = 0.001965, Val Loss = 0.042272\n",
            "Epoch 250: Train Loss = 0.038374, Val Loss = 0.042232\n",
            "Epoch 260: Train Loss = 0.139856, Val Loss = 0.042259\n",
            "Epoch 270: Train Loss = 0.022113, Val Loss = 0.042036\n",
            "Epoch 280: Train Loss = 0.080109, Val Loss = 0.042044\n",
            "Epoch 290: Train Loss = 0.011134, Val Loss = 0.042102\n",
            "Epoch 300: Train Loss = 0.157143, Val Loss = 0.042127\n",
            "Epoch 310: Train Loss = 0.050749, Val Loss = 0.042016\n",
            "Epoch 320: Train Loss = 0.022145, Val Loss = 0.041950\n",
            "Epoch 330: Train Loss = 0.002654, Val Loss = 0.041954\n",
            "Epoch 340: Train Loss = 0.001361, Val Loss = 0.041920\n",
            "Epoch 350: Train Loss = 0.000893, Val Loss = 0.041964\n",
            "Epoch 360: Train Loss = 0.014047, Val Loss = 0.042188\n",
            "Epoch 370: Train Loss = 0.007947, Val Loss = 0.041833\n",
            "Epoch 380: Train Loss = 0.027890, Val Loss = 0.041865\n",
            "Epoch 390: Train Loss = 0.013295, Val Loss = 0.041803\n",
            "Epoch 400: Train Loss = 0.001158, Val Loss = 0.041829\n",
            "Epoch 410: Train Loss = 0.002743, Val Loss = 0.041919\n",
            "Epoch 420: Train Loss = 0.075192, Val Loss = 0.041789\n",
            "Epoch 430: Train Loss = 0.022611, Val Loss = 0.041818\n",
            "Epoch 440: Train Loss = 0.082120, Val Loss = 0.041780\n",
            "Epoch 450: Train Loss = 0.001141, Val Loss = 0.041839\n",
            "Epoch 460: Train Loss = 0.001129, Val Loss = 0.041757\n",
            "Epoch 470: Train Loss = 0.021197, Val Loss = 0.041877\n",
            "Epoch 480: Train Loss = 0.004778, Val Loss = 0.041758\n",
            "Epoch 490: Train Loss = 0.009462, Val Loss = 0.041731\n",
            "Epoch 500: Train Loss = 0.032683, Val Loss = 0.041865\n",
            "Final Test MSE (5_95): 0.033830\n",
            "\n",
            "==== Running Split: 3_97 ====\n",
            "Split 3_97 — X (INPUT): 3, Y (OUTPUT): 98\n",
            "Train: torch.Size([1729, 3, 1]), Val: torch.Size([433, 3, 1]), Test: torch.Size([541, 3, 1])\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 0.439172, Val Loss = 0.793093\n",
            "Epoch 20: Train Loss = 0.192938, Val Loss = 0.439939\n",
            "Epoch 30: Train Loss = 1.080538, Val Loss = 0.306984\n",
            "Epoch 40: Train Loss = 0.115938, Val Loss = 0.248151\n",
            "Epoch 50: Train Loss = 0.477082, Val Loss = 0.212468\n",
            "Epoch 60: Train Loss = 0.008043, Val Loss = 0.183942\n",
            "Epoch 70: Train Loss = 0.071708, Val Loss = 0.158916\n",
            "Epoch 80: Train Loss = 0.188892, Val Loss = 0.136296\n",
            "Epoch 90: Train Loss = 0.073290, Val Loss = 0.116632\n",
            "Epoch 100: Train Loss = 0.126030, Val Loss = 0.100250\n",
            "Epoch 110: Train Loss = 0.095941, Val Loss = 0.086522\n",
            "Epoch 120: Train Loss = 0.051778, Val Loss = 0.075409\n",
            "Epoch 130: Train Loss = 0.005434, Val Loss = 0.066813\n",
            "Epoch 140: Train Loss = 0.027820, Val Loss = 0.060116\n",
            "Epoch 150: Train Loss = 0.002948, Val Loss = 0.055255\n",
            "Epoch 160: Train Loss = 0.006832, Val Loss = 0.051930\n",
            "Epoch 170: Train Loss = 0.008154, Val Loss = 0.049569\n",
            "Epoch 180: Train Loss = 0.002416, Val Loss = 0.047902\n",
            "Epoch 190: Train Loss = 0.001425, Val Loss = 0.046921\n",
            "Epoch 200: Train Loss = 0.098315, Val Loss = 0.046124\n",
            "Epoch 210: Train Loss = 0.067348, Val Loss = 0.045720\n",
            "Epoch 220: Train Loss = 0.026311, Val Loss = 0.045391\n",
            "Epoch 230: Train Loss = 0.013232, Val Loss = 0.045066\n",
            "Epoch 240: Train Loss = 0.003320, Val Loss = 0.044943\n",
            "Epoch 250: Train Loss = 0.001431, Val Loss = 0.044817\n",
            "Epoch 260: Train Loss = 0.017837, Val Loss = 0.044751\n",
            "Epoch 270: Train Loss = 0.017259, Val Loss = 0.044743\n",
            "Epoch 280: Train Loss = 0.003094, Val Loss = 0.044641\n",
            "Epoch 290: Train Loss = 0.081044, Val Loss = 0.044459\n",
            "Epoch 300: Train Loss = 0.031680, Val Loss = 0.044436\n",
            "Epoch 310: Train Loss = 0.013557, Val Loss = 0.044294\n",
            "Epoch 320: Train Loss = 0.030568, Val Loss = 0.044332\n",
            "Epoch 330: Train Loss = 0.065260, Val Loss = 0.044432\n",
            "Epoch 340: Train Loss = 0.032685, Val Loss = 0.044476\n",
            "Epoch 350: Train Loss = 0.008260, Val Loss = 0.044141\n",
            "Epoch 360: Train Loss = 0.006010, Val Loss = 0.044066\n",
            "Epoch 370: Train Loss = 0.000638, Val Loss = 0.044058\n",
            "Epoch 380: Train Loss = 0.038889, Val Loss = 0.044078\n",
            "Epoch 390: Train Loss = 0.003854, Val Loss = 0.043960\n",
            "Epoch 400: Train Loss = 0.135179, Val Loss = 0.043967\n",
            "Epoch 410: Train Loss = 0.029453, Val Loss = 0.044142\n",
            "Epoch 420: Train Loss = 0.033882, Val Loss = 0.044273\n",
            "Epoch 430: Train Loss = 0.017138, Val Loss = 0.043941\n",
            "Epoch 440: Train Loss = 0.019586, Val Loss = 0.043884\n",
            "Epoch 450: Train Loss = 0.327251, Val Loss = 0.043805\n",
            "Epoch 460: Train Loss = 0.008424, Val Loss = 0.043828\n",
            "Epoch 470: Train Loss = 0.003467, Val Loss = 0.043807\n",
            "Epoch 480: Train Loss = 0.001203, Val Loss = 0.043761\n",
            "Epoch 490: Train Loss = 0.017687, Val Loss = 0.043734\n",
            "Epoch 500: Train Loss = 0.004750, Val Loss = 0.043733\n",
            "Final Test MSE (3_97): 0.036144\n",
            "\n",
            "==== Running Split: 1_99 ====\n",
            "Split 1_99 — X (INPUT): 1, Y (OUTPUT): 100\n",
            "Train: torch.Size([1729, 1, 1]), Val: torch.Size([433, 1, 1]), Test: torch.Size([541, 1, 1])\n",
            "Training started...\n",
            "Epoch 10: Train Loss = 4.360520, Val Loss = 3.574580\n",
            "Epoch 20: Train Loss = 1.296825, Val Loss = 2.763415\n",
            "Epoch 30: Train Loss = 2.912303, Val Loss = 2.117400\n",
            "Epoch 40: Train Loss = 1.180799, Val Loss = 1.610719\n",
            "Epoch 50: Train Loss = 1.013454, Val Loss = 1.217736\n",
            "Epoch 60: Train Loss = 1.868613, Val Loss = 0.917304\n",
            "Epoch 70: Train Loss = 0.705091, Val Loss = 0.690772\n",
            "Epoch 80: Train Loss = 0.501812, Val Loss = 0.523585\n",
            "Epoch 90: Train Loss = 0.269302, Val Loss = 0.401912\n",
            "Epoch 100: Train Loss = 0.077593, Val Loss = 0.315300\n",
            "Epoch 110: Train Loss = 0.062275, Val Loss = 0.253987\n",
            "Epoch 120: Train Loss = 0.519434, Val Loss = 0.213329\n",
            "Epoch 130: Train Loss = 0.086694, Val Loss = 0.186575\n",
            "Epoch 140: Train Loss = 0.024541, Val Loss = 0.170133\n",
            "Epoch 150: Train Loss = 0.017828, Val Loss = 0.159978\n",
            "Epoch 160: Train Loss = 0.095077, Val Loss = 0.154091\n",
            "Epoch 170: Train Loss = 0.057535, Val Loss = 0.151474\n",
            "Epoch 180: Train Loss = 0.019989, Val Loss = 0.150222\n",
            "Epoch 190: Train Loss = 0.214716, Val Loss = 0.149320\n",
            "Epoch 200: Train Loss = 0.004779, Val Loss = 0.148779\n",
            "Epoch 210: Train Loss = 0.013888, Val Loss = 0.148706\n",
            "Epoch 220: Train Loss = 0.510923, Val Loss = 0.148279\n",
            "Epoch 230: Train Loss = 0.161462, Val Loss = 0.148453\n",
            "Epoch 240: Train Loss = 0.002231, Val Loss = 0.148584\n",
            "Epoch 250: Train Loss = 0.041307, Val Loss = 0.148622\n",
            "Epoch 260: Train Loss = 0.006195, Val Loss = 0.148636\n",
            "Epoch 270: Train Loss = 0.259186, Val Loss = 0.148141\n",
            "Epoch 280: Train Loss = 0.187350, Val Loss = 0.148847\n",
            "Epoch 290: Train Loss = 0.240584, Val Loss = 0.149123\n",
            "Epoch 300: Train Loss = 0.006454, Val Loss = 0.148519\n",
            "Epoch 310: Train Loss = 0.036998, Val Loss = 0.148465\n",
            "Epoch 320: Train Loss = 0.169879, Val Loss = 0.148434\n",
            "Epoch 330: Train Loss = 0.350974, Val Loss = 0.148987\n",
            "Epoch 340: Train Loss = 0.083969, Val Loss = 0.148478\n",
            "Epoch 350: Train Loss = 0.136681, Val Loss = 0.148583\n",
            "Epoch 360: Train Loss = 0.187711, Val Loss = 0.148650\n",
            "Epoch 370: Train Loss = 0.048771, Val Loss = 0.148161\n",
            "Epoch 380: Train Loss = 0.240496, Val Loss = 0.148338\n",
            "Epoch 390: Train Loss = 0.045714, Val Loss = 0.148949\n",
            "Epoch 400: Train Loss = 0.359315, Val Loss = 0.148942\n",
            "Epoch 410: Train Loss = 0.212245, Val Loss = 0.148671\n",
            "Epoch 420: Train Loss = 0.067927, Val Loss = 0.148238\n",
            "Epoch 430: Train Loss = 0.121162, Val Loss = 0.148594\n",
            "Epoch 440: Train Loss = 0.496811, Val Loss = 0.148619\n",
            "Epoch 450: Train Loss = 0.030917, Val Loss = 0.147813\n",
            "Epoch 460: Train Loss = 0.032094, Val Loss = 0.148376\n",
            "Epoch 470: Train Loss = 0.333708, Val Loss = 0.148535\n",
            "Epoch 480: Train Loss = 0.225796, Val Loss = 0.148438\n",
            "Epoch 490: Train Loss = 0.138715, Val Loss = 0.148460\n",
            "Epoch 500: Train Loss = 0.020107, Val Loss = 0.148320\n",
            "Final Test MSE (1_99): 0.123073\n"
          ]
        }
      ]
    }
  ]
}