{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meXGsXToU6pU",
        "outputId": "92a975f7-3fa1-4d3f-a80e-90e6ca67b3f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive and loading dataset...\n",
            "Mounted at /content/drive\n",
            "Loaded dataset with shape: (1192157, 17)\n",
            "Static feature table created: Input_Link_Table.shape = (2703, 15)\n",
            "Time series matrix constructed: relevant_data.shape = (2703, 101)\n",
            "Performed KMeans clustering into 8 clusters\n",
            "Cluster boundary stats calculated: cluster_boundaries.shape = (8, 4, 101)\n",
            "Final input features (static + cluster): merged_df.shape = (2703, 16)\n",
            "Final output time series: df_output.shape = (273003, 3)\n",
            "Data Preparation Summary:\n",
            "Static Input Table: merged_df [2703 rows × 16 columns]\n",
            "Time Series Output: df_output [273003 rows × 3 columns]\n",
            "Cluster Boundaries: cluster_boundaries [(8, 4, 101)]\n"
          ]
        }
      ],
      "source": [
        "# --------------------- Imports ---------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from google.colab import drive\n",
        "\n",
        "# --------------------- Matplotlib Setup ---------------------\n",
        "mpl.rcParams.update({\n",
        "    'font.size': 14,\n",
        "    'axes.titlesize': 15,\n",
        "    'axes.labelsize': 12,\n",
        "    'xtick.labelsize': 11,\n",
        "    'ytick.labelsize': 11,\n",
        "    'legend.fontsize': 11,\n",
        "    'figure.dpi': 300,\n",
        "    'savefig.dpi': 300,\n",
        "    'figure.autolayout': True,\n",
        "})\n",
        "\n",
        "# --------------------- Load Data ---------------------\n",
        "print(\"Mounting Google Drive and loading dataset...\")\n",
        "drive.mount('/content/drive')\n",
        "total_capture_7k = pd.read_csv('drive/My Drive/C02 project/correlation_wide.csv')\n",
        "print(f\"Loaded dataset with shape: {total_capture_7k.shape}\")\n",
        "\n",
        "# --------------------- Identify Unique Static Parameter Sets ---------------------\n",
        "static_cols = [\n",
        "    'MikeSorghum', 'Quartz', 'Plagioclase', 'Apatite', 'Ilmenite',\n",
        "    'Diopside_Mn', 'Diopside', 'Olivine', 'Alkali-feldspar',\n",
        "    'Montmorillonite', 'Glass', 'temp', 'shift', 'year'\n",
        "]\n",
        "\n",
        "# Add timestep count per file_id\n",
        "file_lengths = total_capture_7k.groupby('file_id').size().rename(\"num_timesteps\").reset_index()\n",
        "static_rows = total_capture_7k.groupby('file_id')[static_cols].first().reset_index()\n",
        "static_rows = static_rows.merge(file_lengths, on='file_id')\n",
        "\n",
        "# Filter only unique static parameter sets\n",
        "unique_static_rows = static_rows.drop_duplicates(subset=static_cols)\n",
        "unique_file_ids = unique_static_rows['file_id'].tolist()\n",
        "\n",
        "# --------------------- Extract Time Series Data ---------------------\n",
        "filtered_df = total_capture_7k[total_capture_7k['file_id'].isin(unique_file_ids)].copy()\n",
        "\n",
        "# Truncate each group to 101 timesteps\n",
        "filtered_df = filtered_df.groupby('file_id').head(101).reset_index(drop=True)\n",
        "\n",
        "# --------------------- Static Feature Table ---------------------\n",
        "Input_Link_Table = filtered_df.groupby('file_id').agg({col: 'first' for col in static_cols}).reset_index()\n",
        "print(f\"Static feature table created: Input_Link_Table.shape = {Input_Link_Table.shape}\")\n",
        "\n",
        "# --------------------- Time Series Structuring ---------------------\n",
        "result = filtered_df[['Total_CO2_capture', 'year', 'file_id']]\n",
        "file_ids = result['file_id'].unique()\n",
        "num_file_ids = len(file_ids)\n",
        "max_timesteps = 101\n",
        "relevant_data = np.zeros((num_file_ids, max_timesteps))\n",
        "file_id_order = np.zeros(num_file_ids)\n",
        "\n",
        "for i, file_id in enumerate(file_ids):\n",
        "    file_data = result[result['file_id'] == file_id]['Total_CO2_capture'].values\n",
        "    relevant_data[i, :len(file_data)] = file_data\n",
        "    file_id_order[i] = file_id\n",
        "print(f\"Time series matrix constructed: relevant_data.shape = {relevant_data.shape}\")\n",
        "\n",
        "# --------------------- Clustering ---------------------\n",
        "scaler = StandardScaler()\n",
        "normalized_data = scaler.fit_transform(relevant_data)\n",
        "kmeans = KMeans(n_clusters=8, random_state=42)\n",
        "clusters = kmeans.fit_predict(normalized_data)\n",
        "print(\"Performed KMeans clustering into 8 clusters\")\n",
        "\n",
        "# Compute boundary stats\n",
        "cluster_boundaries = []\n",
        "for cluster_id in range(8):\n",
        "    cluster_data = normalized_data[clusters == cluster_id]\n",
        "    min_v = scaler.inverse_transform(np.min(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    median_v = scaler.inverse_transform(np.median(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    mean_v = scaler.inverse_transform(np.mean(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    max_v = scaler.inverse_transform(np.max(cluster_data, axis=0).reshape(1, -1)).flatten()\n",
        "    cluster_boundaries.append((min_v, median_v, mean_v, max_v))\n",
        "cluster_boundaries = np.array(cluster_boundaries)\n",
        "print(f\"Cluster boundary stats calculated: cluster_boundaries.shape = {cluster_boundaries.shape}\")\n",
        "\n",
        "# --------------------- Merge Static Features with Clusters ---------------------\n",
        "Clustering_link_table = pd.DataFrame({'file_id': file_id_order.astype(int), 'cluster': clusters})\n",
        "Clustering_link_table = Clustering_link_table.sort_values(by='file_id').reset_index(drop=True)\n",
        "merged_df = pd.merge(Input_Link_Table, Clustering_link_table, on='file_id')\n",
        "print(f\"Final input features (static + cluster): merged_df.shape = {merged_df.shape}\")\n",
        "\n",
        "# --------------------- Create Output Time Series DataFrame ---------------------\n",
        "data = [[file_id_order[i].astype(int), t, relevant_data[i, t]] for i in range(len(file_id_order)) for t in range(max_timesteps)]\n",
        "df_output = pd.DataFrame(data, columns=['file_id', 'timestep', 'CO2']).sort_values(by=['file_id', 'timestep'])\n",
        "print(f\"Final output time series: df_output.shape = {df_output.shape}\")\n",
        "\n",
        "# --------------------- Summary ---------------------\n",
        "print(\"Data Preparation Summary:\")\n",
        "print(f\"Static Input Table: merged_df [{merged_df.shape[0]} rows × {merged_df.shape[1]} columns]\")\n",
        "print(f\"Time Series Output: df_output [{df_output.shape[0]} rows × 3 columns]\")\n",
        "print(f\"Cluster Boundaries: cluster_boundaries [{cluster_boundaries.shape}]\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_tvlssm.py\n",
        "# Time-Varying Linear SSM (A_t, B_t, C_t, D_t) predicted by a Conv1D+LSTM backbone.\n",
        "# For a horizon H, the model predicts H distinct (A_t, B_t, C_t, D_t) and rolls the linear SSM forward.\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class TimeVaryingLSSMHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Predicts time-varying linear state-space parameters for each forecast step t = 1..H:\n",
        "        z_t = A_t z_{t-1} + B_t u\n",
        "        y_t = C_t z_t + D_t u\n",
        "    where u is the static embedding (same for all steps).\n",
        "\n",
        "    Parameterization:\n",
        "      - A_t is stabilized as: diag(tanh(a_t)) + uv_scale * (U_t @ V_t^T),\n",
        "        with U_t, V_t in R^{Z x r}, a_t in R^Z.\n",
        "      - B_t in R^{Z x u_dim}, C_t in R^{1 x Z}, D_t in R^{1 x u_dim}\n",
        "\n",
        "    Inputs:\n",
        "      - context_vec: [B, C]   (e.g., concat of backbone summary h_T and static embedding s)\n",
        "      - step_embeddings: [H, E] learnable or provided per-horizon embeddings (H = output_dim)\n",
        "      - u: [B, u_dim] static control vector (same at every t)\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim, u_dim, horizon, context_dim, step_emb_dim=64, low_rank=4, uv_scale=0.05):\n",
        "        super().__init__()\n",
        "        self.Z = latent_dim\n",
        "        self.u_dim = u_dim\n",
        "        self.H = horizon\n",
        "        self.Cdim = context_dim\n",
        "        self.E = step_emb_dim\n",
        "        self.r = low_rank\n",
        "        self.uv_scale = uv_scale\n",
        "\n",
        "        # Per-step learnable embeddings (one per horizon step)\n",
        "        self.step_embed = nn.Parameter(torch.randn(self.H, self.E) * 0.02)\n",
        "\n",
        "        # Small MLP to produce a compact \"param context\" per step\n",
        "        in_dim = self.Cdim + self.E\n",
        "        hidden = max(128, self.Z)  # modest width\n",
        "        self.param_ctx = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden), nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
        "        )\n",
        "        self._init_mlp(self.param_ctx)\n",
        "\n",
        "        # Heads to predict raw parameter tensors from param-context\n",
        "        # A_t parts\n",
        "        self.head_a = nn.Linear(hidden, self.Z)                     # diag part\n",
        "        self.head_U = nn.Linear(hidden, self.Z * self.r)            # U_t flattened\n",
        "        self.head_V = nn.Linear(hidden, self.Z * self.r)            # V_t flattened\n",
        "\n",
        "        # B_t, C_t, D_t\n",
        "        self.head_B = nn.Linear(hidden, self.Z * self.u_dim)        # B_t flattened\n",
        "        self.head_C = nn.Linear(hidden, self.Z)                     # C_t row (1 x Z)\n",
        "        self.head_D = nn.Linear(hidden, self.u_dim)                 # D_t row (1 x u_dim)\n",
        "\n",
        "        # z0 from the same context (no step embedding)\n",
        "        self.init_net = nn.Sequential(\n",
        "            nn.Linear(self.Cdim, 2 * self.Z), nn.ReLU(),\n",
        "            nn.Linear(2 * self.Z, self.Z)\n",
        "        )\n",
        "        self._init_mlp(self.init_net)\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_mlp(m):\n",
        "        for mod in m:\n",
        "            if isinstance(mod, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(mod.weight, a=math.sqrt(5))\n",
        "                if mod.bias is not None:\n",
        "                    nn.init.zeros_(mod.bias)\n",
        "\n",
        "    def _build_A(self, a_raw, U_raw, V_raw):\n",
        "        \"\"\"\n",
        "        a_raw: [B, Z]\n",
        "        U_raw: [B, Z, r]\n",
        "        V_raw: [B, Z, r]\n",
        "        Returns A: [B, Z, Z]\n",
        "        \"\"\"\n",
        "        B, Z, r = U_raw.shape\n",
        "        diag = torch.tanh(a_raw)                        # [B, Z], |diag|<1\n",
        "        A = torch.zeros(B, Z, Z, device=a_raw.device, dtype=a_raw.dtype)\n",
        "        A = A + torch.diag_embed(diag)                  # put diag entries\n",
        "\n",
        "        # Low-rank residual: U V^T\n",
        "        # (B, Z, r) @ (B, r, Z) -> (B, Z, Z)\n",
        "        A = A + self.uv_scale * torch.bmm(U_raw, V_raw.transpose(1, 2))\n",
        "        return A\n",
        "\n",
        "    def forward(self, context_vec, u):\n",
        "        \"\"\"\n",
        "        context_vec: [B, C]\n",
        "        u:           [B, u_dim]\n",
        "        Returns:\n",
        "          y_hat: [B, H]\n",
        "        Also returns a dict of parameters if needed (disabled in main forward for speed).\n",
        "        \"\"\"\n",
        "        B = context_vec.size(0)\n",
        "        device = context_vec.device\n",
        "        Z, H, r, u_dim = self.Z, self.H, self.r, self.u_dim\n",
        "\n",
        "        # Initial latent state z0\n",
        "        z_t = self.init_net(context_vec)                            # [B, Z]\n",
        "\n",
        "        # Repeat static 'u' as needed\n",
        "        # We'll compute per-step params and roll forward\n",
        "        y_preds = []\n",
        "\n",
        "        for t in range(H):\n",
        "            e_t = self.step_embed[t].unsqueeze(0).expand(B, -1)     # [B, E]\n",
        "            pc_in = torch.cat([context_vec, e_t], dim=1)            # [B, C+E]\n",
        "            pc = self.param_ctx(pc_in)                              # [B, hidden]\n",
        "\n",
        "            # Predict raw params\n",
        "            a_t = self.head_a(pc)                                   # [B, Z]\n",
        "            U_t = self.head_U(pc).view(B, Z, r)                     # [B, Z, r]\n",
        "            V_t = self.head_V(pc).view(B, Z, r)                     # [B, Z, r]\n",
        "            B_t = self.head_B(pc).view(B, Z, u_dim)                 # [B, Z, u_dim]\n",
        "            C_t = self.head_C(pc).view(B, 1, Z)                     # [B, 1, Z]\n",
        "            D_t = self.head_D(pc).view(B, 1, u_dim)                 # [B, 1, u_dim]\n",
        "\n",
        "            # Build A_t\n",
        "            A_t = self._build_A(a_t, U_t, V_t)                      # [B, Z, Z]\n",
        "\n",
        "            # Roll one step: z_t = A_t z_{t-1} + B_t u\n",
        "            Bu = torch.bmm(B_t, u.unsqueeze(-1)).squeeze(-1)        # [B, Z]\n",
        "            z_t = torch.bmm(A_t, z_t.unsqueeze(-1)).squeeze(-1) + Bu\n",
        "\n",
        "            # y_t = C_t z_t + D_t u  -> scalar per step\n",
        "            Cz = torch.bmm(C_t, z_t.unsqueeze(-1)).squeeze(-1)      # [B, 1]\n",
        "            Du = torch.bmm(D_t, u.unsqueeze(-1)).squeeze(-1)        # [B, 1]\n",
        "            y_t = Cz + Du                                           # [B, 1]\n",
        "            y_preds.append(y_t)\n",
        "\n",
        "        y_hat = torch.cat(y_preds, dim=1)                           # [B, H]\n",
        "        return y_hat\n",
        "\n",
        "\n",
        "class TVLSSMForecastNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Backbone: Static MLP + Conv1D + LSTM encoder over observed window.\n",
        "    Head:     TimeVaryingLSSMHead that predicts (A_t, B_t, C_t, D_t) per step and rolls the linear SSM.\n",
        "\n",
        "    time_series_input: [B, L] or [B, 1, L] observed window\n",
        "    static_input:      [B, S] static features\n",
        "    returns:           [B, H] forecast\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_len,                 # L (observed window length)\n",
        "        static_dim,                # number of static features\n",
        "        hidden_dim,                # conv channels and LSTM hidden size (also latent_dim default)\n",
        "        horizon,                   # H (forecast length)\n",
        "        latent_dim=None,           # latent state z size (default: hidden_dim)\n",
        "        step_emb_dim=64,\n",
        "        low_rank=4,\n",
        "        uv_scale=0.05,\n",
        "        dropout=0.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if latent_dim is None:\n",
        "            latent_dim = hidden_dim\n",
        "        self.input_len = input_len\n",
        "        self.static_dim = static_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.horizon = horizon\n",
        "        self.latent_dim = latent_dim\n",
        "        self.u_dim = 64  # static embedding size\n",
        "        self.dropout_p = dropout\n",
        "\n",
        "        # ----- Static encoder -----\n",
        "        self.fc_s1 = nn.Linear(static_dim, 512)\n",
        "        self.fc_s2 = nn.Linear(512, 256)\n",
        "        self.fc_s3 = nn.Linear(256, 128)\n",
        "        self.fc_s4 = nn.Linear(128, self.u_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self._init_linear(self.fc_s1)\n",
        "        self._init_linear(self.fc_s2)\n",
        "        self._init_linear(self.fc_s3)\n",
        "        self._init_linear(self.fc_s4)\n",
        "        self.drop = nn.Dropout(self.dropout_p) if self.dropout_p > 0 else nn.Identity()\n",
        "\n",
        "        # ----- Temporal encoder: Conv1D → LSTM -----\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
        "        nn.init.kaiming_uniform_(self.conv1.weight, a=math.sqrt(5))\n",
        "        if self.conv1.bias is not None:\n",
        "            nn.init.zeros_(self.conv1.bias)\n",
        "\n",
        "        self.lstm = nn.LSTM(hidden_dim + self.u_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "        # ----- Time-Varying LSSM Head -----\n",
        "        context_dim = hidden_dim + self.u_dim  # using [h_T ⊕ s] as context\n",
        "        self.tvlssm = TimeVaryingLSSMHead(\n",
        "            latent_dim=self.latent_dim,\n",
        "            u_dim=self.u_dim,\n",
        "            horizon=horizon,\n",
        "            context_dim=context_dim,\n",
        "            step_emb_dim=step_emb_dim,\n",
        "            low_rank=low_rank,\n",
        "            uv_scale=uv_scale,\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_linear(layer):\n",
        "        nn.init.kaiming_uniform_(layer.weight, a=math.sqrt(5))\n",
        "        if layer.bias is not None:\n",
        "            nn.init.zeros_(layer.bias)\n",
        "\n",
        "    def forward(self, time_series_input, static_input):\n",
        "        # Static embedding u\n",
        "        s = self.relu(self.fc_s1(static_input))\n",
        "        s = self.relu(self.fc_s2(s))\n",
        "        s = self.relu(self.fc_s3(s))\n",
        "        s = self.fc_s4(s)                         # [B, u_dim]\n",
        "        s = self.drop(s)\n",
        "\n",
        "        # Conv over observed window\n",
        "        x = time_series_input\n",
        "        if x.dim() == 2:                          # [B, L] -> [B, 1, L]\n",
        "            x = x.unsqueeze(1)\n",
        "        conv_out = self.relu(self.conv1(x))       # [B, Hc, L]\n",
        "        conv_out = conv_out.transpose(1, 2)       # [B, L, Hc]\n",
        "\n",
        "        # LSTM with static conditioning at each step of the observed window\n",
        "        s_exp = s.unsqueeze(1).expand(-1, conv_out.size(1), -1)  # [B, L, u_dim]\n",
        "        lstm_in = torch.cat([conv_out, s_exp], dim=2)            # [B, L, Hc+u]\n",
        "        lstm_out, _ = self.lstm(lstm_in)                         # [B, L, Hc]\n",
        "        h_T = lstm_out[:, -1, :]                                 # summary [B, Hc]\n",
        "\n",
        "        # Context for parameter generation\n",
        "        context_vec = torch.cat([h_T, s], dim=1)                 # [B, Hc+u]\n",
        "\n",
        "        # Forecast by rolling the time-varying linear SSM\n",
        "        y_hat = self.tvlssm(context_vec, s)                      # [B, horizon]\n",
        "        return y_hat\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def debug_inspect_single_step_params(self, time_series_input, static_input, step_idx=0):\n",
        "        \"\"\"\n",
        "        Optional: return A_t, B_t, C_t, D_t for a given step t for inspection (first element of batch).\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        x = time_series_input\n",
        "        s_in = static_input\n",
        "        if x.dim() == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "        s = self.relu(self.fc_s1(s_in))\n",
        "        s = self.relu(self.fc_s2(s))\n",
        "        s = self.relu(self.fc_s3(s))\n",
        "        s = self.fc_s4(s)\n",
        "\n",
        "        conv_out = self.relu(self.conv1(x)).transpose(1, 2)\n",
        "        s_exp = s.unsqueeze(1).expand(-1, conv_out.size(1), -1)\n",
        "        lstm_in = torch.cat([conv_out, s_exp], dim=2)\n",
        "        lstm_out, _ = self.lstm(lstm_in)\n",
        "        h_T = lstm_out[:, -1, :]\n",
        "        context_vec = torch.cat([h_T, s], dim=1)\n",
        "\n",
        "        # Build per-step params (single step)\n",
        "        B = context_vec.size(0)\n",
        "        e_t = self.tvlssm.step_embed[step_idx].unsqueeze(0).expand(B, -1)\n",
        "        pc_in = torch.cat([context_vec, e_t], dim=1)\n",
        "        pc = self.tvlssm.param_ctx(pc_in)\n",
        "\n",
        "        Z, r, u_dim = self.latent_dim, self.tvlssm.r, self.u_dim\n",
        "        a_t = self.tvlssm.head_a(pc)\n",
        "        U_t = self.tvlssm.head_U(pc).view(B, Z, r)\n",
        "        V_t = self.tvlssm.head_V(pc).view(B, Z, r)\n",
        "        B_t = self.tvlssm.head_B(pc).view(B, Z, u_dim)\n",
        "        C_t = self.tvlssm.head_C(pc).view(B, 1, Z)\n",
        "        D_t = self.tvlssm.head_D(pc).view(B, 1, u_dim)\n",
        "        A_t = self.tvlssm._build_A(a_t, U_t, V_t)\n",
        "        return A_t[0].cpu(), B_t[0].cpu(), C_t[0].cpu(), D_t[0].cpu()"
      ],
      "metadata": {
        "id": "vbXIeQMOU7gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# experiment_tvlssm.py\n",
        "# Train/evaluate the TV-LSSM model over multiple X–Y splits using your existing df_output & merged_df.\n",
        "\n",
        "import os, math, numpy as np, pandas as pd, torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# --------------------- Config ---------------------\n",
        "cfg = {\n",
        "    \"hidden_dim\": 101,\n",
        "    \"epochs\": 500,\n",
        "    \"batch_size\": 124,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"grad_clip\": 1.0,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"low_rank\": 4,\n",
        "    \"uv_scale\": 0.05,\n",
        "    \"dropout\": 0.0,\n",
        "    \"seed\": 42,\n",
        "    \"checkpoint_dir\": \"/content/drive/MyDrive/DSSM-Figures\",\n",
        "    \"log_every\": 10,\n",
        "}\n",
        "os.makedirs(cfg[\"checkpoint_dir\"], exist_ok=True)\n",
        "torch.manual_seed(cfg[\"seed\"]); np.random.seed(cfg[\"seed\"])\n",
        "\n",
        "# --------------------- Data helpers (unchanged logic) ---------------------\n",
        "def pivot_matrix(df_output, ids):\n",
        "    sub = df_output[df_output[\"file_id\"].isin(ids)]\n",
        "    piv = sub.pivot(index=\"file_id\", columns=\"timestep\", values=\"CO2\").sort_index()\n",
        "    return piv.values, piv.index.values\n",
        "\n",
        "def extract_X_Y(df_output, ids, pct):\n",
        "    mat, ordered_ids = pivot_matrix(df_output, ids)\n",
        "    finite_mask = np.all(np.isfinite(mat), axis=1)\n",
        "    mat = mat[finite_mask]\n",
        "    kept_ids = ordered_ids[finite_mask]\n",
        "    split_idx = int(pct / 100 * 101)  # total steps=101\n",
        "    X = mat[:, :split_idx]\n",
        "    Y = mat[:, split_idx:]\n",
        "    return X, Y, kept_ids\n",
        "\n",
        "def align_static(merged_df, kept_ids):\n",
        "    stat = merged_df.set_index(\"file_id\").loc[kept_ids]\n",
        "    stat = stat.drop(columns=[\"cluster\"], errors=\"ignore\")\n",
        "    S = stat.values\n",
        "    S = np.nan_to_num(S, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return S\n",
        "\n",
        "def mse_np(a, b):\n",
        "    return float(np.mean((a - b) ** 2))\n",
        "\n",
        "# --------------------- Training pipeline ---------------------\n",
        "def train_all_splits(df_output, merged_df, splits=None):\n",
        "    if splits is None:\n",
        "        splits = [(20,80),(10,90),(5,95),(3,97),(1,99),(80,20),(60,40),(50,50),(40,60)]\n",
        "\n",
        "    device = cfg[\"device\"]\n",
        "    results = pd.DataFrame(columns=[\"Split\", \"Test_MSE\"])\n",
        "\n",
        "    # Fixed series ID split across all X–Y splits\n",
        "    file_ids = df_output[\"file_id\"].unique()\n",
        "    trainval_ids, test_ids = train_test_split(file_ids, test_size=0.2, random_state=cfg[\"seed\"])\n",
        "    train_ids, val_ids = train_test_split(trainval_ids, test_size=0.2, random_state=cfg[\"seed\"])\n",
        "\n",
        "    for train_pct, test_pct in splits:\n",
        "        split_name = f\"{train_pct}_{test_pct}\"\n",
        "        print(f\"\\n==== Running Split: {split_name} ====\")\n",
        "\n",
        "        # Build matrices (these calls drop rows with any NaN/Inf in series)\n",
        "        X_train, Y_train, kept_train = extract_X_Y(df_output, train_ids, train_pct)\n",
        "        X_val,   Y_val,   kept_val   = extract_X_Y(df_output, val_ids,   train_pct)\n",
        "        X_test,  Y_test,  kept_test  = extract_X_Y(df_output, test_ids,  train_pct)\n",
        "\n",
        "        S_train = align_static(merged_df, kept_train)\n",
        "        S_val   = align_static(merged_df, kept_val)\n",
        "        S_test  = align_static(merged_df, kept_test)\n",
        "\n",
        "        assert X_train.shape[0] == Y_train.shape[0] == S_train.shape[0]\n",
        "        assert X_val.shape[0]   == Y_val.shape[0]   == S_val.shape[0]\n",
        "        assert X_test.shape[0]  == S_test.shape[0]\n",
        "\n",
        "        print(f\"Split {split_name} — X_len={X_train.shape[1]} | Y_len={Y_train.shape[1]}\")\n",
        "        print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test inputs: {X_test.shape}\")\n",
        "\n",
        "        # Standardize with train-only stats\n",
        "        x_scaler = StandardScaler()\n",
        "        y_scaler = StandardScaler()\n",
        "        s_scaler = StandardScaler()\n",
        "\n",
        "        X_train_z = x_scaler.fit_transform(X_train)\n",
        "        Y_train_z = y_scaler.fit_transform(Y_train)\n",
        "        S_train_z = s_scaler.fit_transform(S_train)\n",
        "\n",
        "        X_val_z = x_scaler.transform(X_val)\n",
        "        Y_val_z = y_scaler.transform(Y_val)\n",
        "        S_val_z = s_scaler.transform(S_val)\n",
        "\n",
        "        X_test_z = x_scaler.transform(X_test)\n",
        "        S_test_z = s_scaler.transform(S_test)  # keep Y_test on original scale\n",
        "\n",
        "        # Tensors\n",
        "        X_train_t = torch.tensor(X_train_z, dtype=torch.float32, device=device)\n",
        "        Y_train_t = torch.tensor(Y_train_z, dtype=torch.float32, device=device)\n",
        "        S_train_t = torch.tensor(S_train_z, dtype=torch.float32, device=device)\n",
        "\n",
        "        X_val_t = torch.tensor(X_val_z, dtype=torch.float32, device=device)\n",
        "        Y_val_t = torch.tensor(Y_val_z, dtype=torch.float32, device=device)\n",
        "        S_val_t = torch.tensor(S_val_z, dtype=torch.float32, device=device)\n",
        "\n",
        "        X_test_t = torch.tensor(X_test_z, dtype=torch.float32, device=device)\n",
        "        S_test_t = torch.tensor(S_test_z, dtype=torch.float32, device=device)\n",
        "\n",
        "        train_loader = DataLoader(TensorDataset(X_train_t, S_train_t, Y_train_t),\n",
        "                                  batch_size=cfg[\"batch_size\"], shuffle=True, drop_last=False)\n",
        "        val_loader = DataLoader(TensorDataset(X_val_t, S_val_t, Y_val_t),\n",
        "                                batch_size=cfg[\"batch_size\"], shuffle=False, drop_last=False)\n",
        "\n",
        "        # ----- Model -----\n",
        "        model = TVLSSMForecastNet(\n",
        "            input_len=X_train.shape[1],\n",
        "            static_dim=S_train.shape[1],\n",
        "            hidden_dim=cfg[\"hidden_dim\"],\n",
        "            horizon=Y_train.shape[1],\n",
        "            latent_dim=cfg[\"hidden_dim\"],\n",
        "            step_emb_dim=64,\n",
        "            low_rank=cfg[\"low_rank\"],\n",
        "            uv_scale=cfg[\"uv_scale\"],\n",
        "            dropout=cfg[\"dropout\"],\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=cfg[\"learning_rate\"])\n",
        "        criterion = nn.MSELoss()\n",
        "        best_val = math.inf\n",
        "        ckpt_path = os.path.join(cfg[\"checkpoint_dir\"], f\"tvlssm_best_{split_name}.pt\")\n",
        "\n",
        "        print(\"Training started...\")\n",
        "        for epoch in range(cfg[\"epochs\"]):\n",
        "            model.train()\n",
        "            running = 0.0\n",
        "            for Xb, Sb, Yb in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                preds = model(Xb, Sb)                  # [B, H]\n",
        "                loss = criterion(preds, Yb)\n",
        "                if torch.isnan(loss) or torch.isinf(loss):\n",
        "                    print(f\"[Epoch {epoch+1}] NaN/Inf loss. Skipping step.\")\n",
        "                    continue\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), cfg[\"grad_clip\"])\n",
        "                optimizer.step()\n",
        "                running += loss.item()\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for Xb, Sb, Yb in val_loader:\n",
        "                    preds = model(Xb, Sb)\n",
        "                    val_loss += criterion(preds, Yb).item()\n",
        "            val_loss /= max(1, len(val_loader))\n",
        "\n",
        "            if (epoch + 1) % cfg[\"log_every\"] == 0:\n",
        "                trn_loss = running / max(1, len(train_loader))\n",
        "                print(f\"Epoch {epoch+1:3d} | Train {trn_loss:.6f} | Val {val_loss:.6f}\")\n",
        "\n",
        "            if val_loss < best_val and not np.isnan(val_loss):\n",
        "                best_val = val_loss\n",
        "                torch.save(model.state_dict(), ckpt_path)\n",
        "\n",
        "        # ----- Test on original scale -----\n",
        "        model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_pred_z = model(X_test_t, S_test_t).cpu().numpy()\n",
        "        test_pred = y_scaler.inverse_transform(test_pred_z)\n",
        "\n",
        "        # Build true Y_test (on original scale from earlier split)\n",
        "        _, Y_test, _ = extract_X_Y(df_output, kept_test, train_pct)\n",
        "        test_mse = mse_np(test_pred, Y_test)\n",
        "        print(f\"Split {split_name} — Final Test MSE (orig scale): {test_mse:.6e}\")\n",
        "        results.loc[len(results)] = [split_name, test_mse]\n",
        "    return results\n",
        "\n",
        "# --------------------- Run training ---------------------\n",
        "# Expect df_output (file_id, timestep, CO2) and merged_df to be defined already by your unchanged data prep module.\n",
        "# Example:\n",
        "# from data_prep_module import df_output, merged_df\n",
        "DSSM_TV_LSSM_mse = train_all_splits(df_output, merged_df)\n",
        "print(\"\\nAll splits complete:\")\n",
        "print(DSSM_TV_LSSM_mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSS3HBgWU98i",
        "outputId": "ee456add-f37b-443c-ae71-b5945c67571e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Running Split: 20_80 ====\n",
            "Split 20_80 — X_len=20 | Y_len=81\n",
            "Train: (1729, 20), Val: (433, 20), Test inputs: (541, 20)\n",
            "Training started...\n",
            "Epoch  10 | Train 0.060541 | Val 0.076558\n",
            "Epoch  20 | Train 0.036317 | Val 0.049876\n",
            "Epoch  30 | Train 0.017108 | Val 0.031021\n",
            "Epoch  40 | Train 0.022286 | Val 0.036981\n",
            "Epoch  50 | Train 0.008802 | Val 0.025435\n",
            "Epoch  60 | Train 0.006566 | Val 0.020934\n",
            "Epoch  70 | Train 0.006582 | Val 0.022517\n",
            "Epoch  80 | Train 0.005030 | Val 0.018471\n",
            "Epoch  90 | Train 0.009743 | Val 0.026042\n",
            "Epoch 100 | Train 0.004754 | Val 0.019574\n",
            "Epoch 110 | Train 0.004677 | Val 0.020491\n",
            "Epoch 120 | Train 0.003083 | Val 0.019450\n",
            "Epoch 130 | Train 0.002830 | Val 0.015038\n",
            "Epoch 140 | Train 0.003164 | Val 0.016135\n",
            "Epoch 150 | Train 0.003408 | Val 0.015367\n",
            "Epoch 160 | Train 0.002346 | Val 0.015412\n",
            "Epoch 170 | Train 0.002015 | Val 0.017589\n",
            "Epoch 180 | Train 0.001960 | Val 0.017558\n",
            "Epoch 190 | Train 0.002627 | Val 0.018281\n",
            "Epoch 200 | Train 0.007710 | Val 0.026046\n",
            "Epoch 210 | Train 0.003330 | Val 0.013928\n",
            "Epoch 220 | Train 0.001402 | Val 0.012322\n",
            "Epoch 230 | Train 0.001549 | Val 0.012792\n",
            "Epoch 240 | Train 0.001251 | Val 0.014861\n",
            "Epoch 250 | Train 0.001229 | Val 0.012900\n",
            "Epoch 260 | Train 0.002287 | Val 0.015628\n",
            "Epoch 270 | Train 0.001539 | Val 0.012855\n",
            "Epoch 280 | Train 0.001436 | Val 0.013943\n",
            "Epoch 290 | Train 0.001437 | Val 0.013681\n",
            "Epoch 300 | Train 0.001012 | Val 0.012288\n",
            "Epoch 310 | Train 0.002276 | Val 0.012496\n",
            "Epoch 320 | Train 0.001129 | Val 0.013479\n",
            "Epoch 330 | Train 0.001196 | Val 0.014457\n",
            "Epoch 340 | Train 0.001237 | Val 0.014084\n",
            "Epoch 350 | Train 0.006276 | Val 0.019654\n",
            "Epoch 360 | Train 0.006818 | Val 0.016296\n",
            "Epoch 370 | Train 0.001305 | Val 0.010794\n",
            "Epoch 380 | Train 0.000664 | Val 0.011663\n",
            "Epoch 390 | Train 0.000522 | Val 0.011463\n",
            "Epoch 400 | Train 0.000928 | Val 0.012782\n",
            "Epoch 410 | Train 0.003019 | Val 0.012603\n",
            "Epoch 420 | Train 0.000639 | Val 0.010897\n",
            "Epoch 430 | Train 0.000490 | Val 0.011217\n",
            "Epoch 440 | Train 0.001162 | Val 0.010581\n",
            "Epoch 450 | Train 0.003191 | Val 0.014753\n",
            "Epoch 460 | Train 0.000932 | Val 0.011590\n",
            "Epoch 470 | Train 0.000620 | Val 0.011070\n",
            "Epoch 480 | Train 0.000368 | Val 0.011420\n",
            "Epoch 490 | Train 0.000850 | Val 0.010508\n",
            "Epoch 500 | Train 0.001337 | Val 0.010531\n",
            "Split 20_80 — Final Test MSE (orig scale): 1.870665e-03\n",
            "\n",
            "==== Running Split: 10_90 ====\n",
            "Split 10_90 — X_len=10 | Y_len=91\n",
            "Train: (1729, 10), Val: (433, 10), Test inputs: (541, 10)\n",
            "Training started...\n",
            "Epoch  10 | Train 0.087440 | Val 0.126128\n",
            "Epoch  20 | Train 0.050645 | Val 0.077849\n",
            "Epoch  30 | Train 0.037276 | Val 0.064631\n",
            "Epoch  40 | Train 0.021995 | Val 0.054376\n",
            "Epoch  50 | Train 0.019115 | Val 0.047870\n",
            "Epoch  60 | Train 0.015224 | Val 0.048982\n",
            "Epoch  70 | Train 0.011364 | Val 0.039117\n",
            "Epoch  80 | Train 0.009074 | Val 0.044190\n",
            "Epoch  90 | Train 0.009319 | Val 0.050935\n",
            "Epoch 100 | Train 0.008721 | Val 134775.550615\n",
            "Epoch 110 | Train 0.007290 | Val 48582956.036626\n",
            "Epoch 120 | Train 0.006297 | Val 2002224152576.028564\n",
            "Epoch 130 | Train 0.005735 | Val 0.039359\n",
            "Epoch 140 | Train 0.007138 | Val 0.042110\n",
            "Epoch 150 | Train 0.005635 | Val 0.034367\n",
            "Epoch 160 | Train 0.006504 | Val 0.035176\n",
            "Epoch 170 | Train 0.006715 | Val 0.031583\n",
            "Epoch 180 | Train 0.004140 | Val 0.031590\n",
            "Epoch 190 | Train 0.006530 | Val 0.030936\n",
            "Epoch 200 | Train 0.006363 | Val 0.038890\n",
            "Epoch 210 | Train 0.004142 | Val 0.032842\n",
            "Epoch 220 | Train 0.003801 | Val 0.031989\n",
            "Epoch 230 | Train 0.005251 | Val 0.038748\n",
            "Epoch 240 | Train 0.003233 | Val 0.030463\n",
            "Epoch 250 | Train 0.005127 | Val 0.031881\n",
            "Epoch 260 | Train 0.004554 | Val 0.033562\n",
            "Epoch 270 | Train 0.004856 | Val 0.026556\n",
            "Epoch 280 | Train 0.003116 | Val 0.032115\n",
            "Epoch 290 | Train 0.002945 | Val 0.031390\n",
            "Epoch 300 | Train 0.001851 | Val 0.031410\n",
            "Epoch 310 | Train 0.004033 | Val 139899.732889\n",
            "Epoch 320 | Train 0.023787 | Val 0.040395\n",
            "Epoch 330 | Train 0.003149 | Val 0.030621\n",
            "Epoch 340 | Train 0.001675 | Val 0.027857\n",
            "Epoch 350 | Train 0.001962 | Val 0.028564\n",
            "Epoch 360 | Train 0.002414 | Val 0.027531\n",
            "Epoch 370 | Train 0.001361 | Val 0.029058\n",
            "Epoch 380 | Train 0.002443 | Val 0.029371\n",
            "Epoch 390 | Train 0.001603 | Val 0.029990\n",
            "Epoch 400 | Train 0.000983 | Val 0.029559\n",
            "Epoch 410 | Train 0.005844 | Val 0.035322\n",
            "Epoch 420 | Train 0.006089 | Val 0.036811\n",
            "Epoch 430 | Train 0.003160 | Val 0.029199\n",
            "Epoch 440 | Train 0.002619 | Val 0.031806\n",
            "Epoch 450 | Train 0.002029 | Val 0.033683\n",
            "Epoch 460 | Train 0.002413 | Val 0.030074\n",
            "Epoch 470 | Train 0.003853 | Val 0.031958\n",
            "Epoch 480 | Train 0.001403 | Val 0.114510\n",
            "Epoch 490 | Train 0.001128 | Val 1002.866339\n",
            "Epoch 500 | Train 0.001489 | Val 0.108302\n",
            "Split 10_90 — Final Test MSE (orig scale): 3.753888e-03\n",
            "\n",
            "==== Running Split: 5_95 ====\n",
            "Split 5_95 — X_len=5 | Y_len=96\n",
            "Train: (1729, 5), Val: (433, 5), Test inputs: (541, 5)\n",
            "Training started...\n",
            "Epoch  10 | Train 0.123441 | Val 0.155614\n",
            "Epoch  20 | Train 0.082029 | Val 0.119044\n",
            "Epoch  30 | Train 0.050187 | Val 0.099666\n",
            "Epoch  40 | Train 0.052687 | Val 0.102529\n",
            "Epoch  50 | Train 0.043060 | Val 0.083550\n",
            "Epoch  60 | Train 0.034596 | Val 0.088302\n",
            "Epoch  70 | Train 0.031860 | Val 0.081096\n",
            "Epoch  80 | Train 0.028575 | Val 0.080464\n",
            "Epoch  90 | Train 0.031123 | Val 0.083956\n",
            "Epoch 100 | Train 0.022218 | Val 0.070632\n",
            "Epoch 110 | Train 0.021024 | Val 0.074833\n",
            "Epoch 120 | Train 0.020253 | Val 0.072701\n",
            "Epoch 130 | Train 0.025055 | Val 0.085602\n",
            "Epoch 140 | Train 0.017042 | Val 0.075832\n",
            "Epoch 150 | Train 0.013124 | Val 0.068167\n",
            "Epoch 160 | Train 0.018353 | Val 0.077404\n",
            "Epoch 170 | Train 0.014520 | Val 0.069166\n",
            "Epoch 180 | Train 0.017909 | Val 0.072333\n",
            "Epoch 190 | Train 0.014358 | Val 14340.250987\n",
            "Epoch 200 | Train 0.021462 | Val 0.076734\n",
            "Epoch 210 | Train 0.009031 | Val 0.064096\n",
            "Epoch 220 | Train 0.009135 | Val 0.074229\n",
            "Epoch 230 | Train 0.018034 | Val 0.065341\n",
            "Epoch 240 | Train 0.015258 | Val 0.063132\n",
            "Epoch 250 | Train 0.010658 | Val 0.067756\n",
            "Epoch 260 | Train 0.010193 | Val 0.066749\n",
            "Epoch 270 | Train 0.005560 | Val 0.057844\n",
            "Epoch 280 | Train 0.006366 | Val 0.065674\n",
            "Epoch 290 | Train 0.005769 | Val 0.063551\n",
            "Epoch 300 | Train 0.006717 | Val 0.062014\n",
            "Epoch 310 | Train 0.008276 | Val 0.068543\n",
            "Epoch 320 | Train 0.006104 | Val 0.065075\n",
            "Epoch 330 | Train 0.018062 | Val 0.069931\n",
            "Epoch 340 | Train 0.007526 | Val 0.067327\n",
            "Epoch 350 | Train 0.006276 | Val 0.060710\n",
            "Epoch 360 | Train 0.012806 | Val 0.063087\n",
            "Epoch 370 | Train 0.007509 | Val 0.062546\n",
            "Epoch 380 | Train 0.007784 | Val 0.062624\n",
            "Epoch 390 | Train 0.010696 | Val 0.056462\n",
            "Epoch 400 | Train 19.320770 | Val 0.075904\n",
            "Epoch 410 | Train 0.011411 | Val 0.063314\n",
            "Epoch 420 | Train 0.003802 | Val 0.059873\n",
            "Epoch 430 | Train 0.003602 | Val 0.058269\n",
            "Epoch 440 | Train 0.003815 | Val 0.057337\n",
            "Epoch 450 | Train 0.002032 | Val 0.056409\n",
            "Epoch 460 | Train 0.002596 | Val 0.056445\n",
            "Epoch 470 | Train 0.013646 | Val 0.061295\n",
            "Epoch 480 | Train 0.011108 | Val 0.063376\n",
            "Epoch 490 | Train 0.005442 | Val 0.055747\n",
            "Epoch 500 | Train 0.002628 | Val 0.057885\n",
            "Split 5_95 — Final Test MSE (orig scale): 1.076277e-02\n",
            "\n",
            "==== Running Split: 3_97 ====\n",
            "Split 3_97 — X_len=3 | Y_len=98\n",
            "Train: (1729, 3), Val: (433, 3), Test inputs: (541, 3)\n",
            "Training started...\n",
            "Epoch  10 | Train 0.157603 | Val 0.187417\n",
            "Epoch  20 | Train 0.087088 | Val 0.129594\n",
            "Epoch  30 | Train 0.075000 | Val 0.100499\n",
            "Epoch  40 | Train 0.057057 | Val 0.106253\n",
            "Epoch  50 | Train 0.048177 | Val 0.086253\n",
            "Epoch  60 | Train 0.044712 | Val 0.087619\n",
            "Epoch  70 | Train 0.039534 | Val 0.088971\n",
            "Epoch  80 | Train 0.035948 | Val 0.098773\n",
            "Epoch  90 | Train 0.034989 | Val 0.106020\n",
            "Epoch 100 | Train 0.034483 | Val 0.090233\n",
            "Epoch 110 | Train 0.028212 | Val 0.090921\n",
            "Epoch 120 | Train 0.024269 | Val 0.099878\n",
            "Epoch 130 | Train 0.027473 | Val 0.113028\n",
            "Epoch 140 | Train 0.025162 | Val 0.096427\n",
            "Epoch 150 | Train 0.025599 | Val 0.095692\n",
            "Epoch 160 | Train 0.026858 | Val 0.103988\n",
            "Epoch 170 | Train 0.024685 | Val 0.098852\n",
            "Epoch 180 | Train 0.020265 | Val 0.100919\n",
            "Epoch 190 | Train 0.019009 | Val 0.095152\n",
            "Epoch 200 | Train 0.018161 | Val 0.102724\n",
            "Epoch 210 | Train 0.019513 | Val 0.110382\n",
            "Epoch 220 | Train 0.019110 | Val 0.100283\n",
            "Epoch 230 | Train 0.022739 | Val 1.471546\n",
            "Epoch 240 | Train 0.015428 | Val 311883904.091313\n",
            "Epoch 250 | Train 0.016261 | Val 9911613539284418560.000000\n",
            "Epoch 260 | Train 0.020180 | Val 14359362.089354\n",
            "Epoch 270 | Train 0.011556 | Val 2787169892313333760.000000\n",
            "Epoch 280 | Train 0.014425 | Val 10293849520783071641600.000000\n",
            "Epoch 290 | Train 0.018653 | Val 0.096966\n",
            "Epoch 300 | Train 0.010224 | Val 0.100575\n",
            "Epoch 310 | Train 0.011821 | Val 900000.339016\n",
            "Epoch 320 | Train 0.010412 | Val 72283440.084553\n",
            "Epoch 330 | Train 0.009770 | Val 59684551655424.078125\n",
            "Epoch 340 | Train 0.014404 | Val 31073229393107140.000000\n",
            "Epoch 350 | Train 0.014688 | Val 0.123804\n",
            "Epoch 360 | Train 0.007947 | Val 2796.397614\n",
            "Epoch 370 | Train 0.010153 | Val 35504162846747394048.000000\n",
            "Epoch 380 | Train 0.020741 | Val 209649723223572480.000000\n",
            "Epoch 390 | Train 0.009962 | Val 0.119839\n",
            "Epoch 400 | Train 0.004632 | Val 0.102938\n",
            "Epoch 410 | Train 0.004165 | Val 461.988932\n",
            "Epoch 420 | Train 0.004478 | Val 927772.582464\n",
            "Epoch 430 | Train 0.007561 | Val 103871537152.082962\n",
            "Epoch 440 | Train 0.004794 | Val 122682116931584.078125\n",
            "Epoch 450 | Train 0.011382 | Val 86323208192.084488\n",
            "Epoch 460 | Train 0.013940 | Val 15603337735048265728.000000\n",
            "Epoch 470 | Train 0.010613 | Val 0.096341\n",
            "Epoch 480 | Train 0.004837 | Val 0.096869\n",
            "Epoch 490 | Train 0.002887 | Val 0.095583\n",
            "Epoch 500 | Train 0.004188 | Val 0.090967\n",
            "Split 3_97 — Final Test MSE (orig scale): 1.126696e-02\n",
            "\n",
            "==== Running Split: 1_99 ====\n",
            "Split 1_99 — X_len=1 | Y_len=100\n",
            "Train: (1729, 1), Val: (433, 1), Test inputs: (541, 1)\n",
            "Training started...\n",
            "Epoch  10 | Train 0.297256 | Val 0.360920\n",
            "Epoch  20 | Train 0.229939 | Val 0.392254\n",
            "Epoch  30 | Train 0.214904 | Val 0.299759\n",
            "Epoch  40 | Train 0.172416 | Val 0.344216\n",
            "Epoch  50 | Train 0.179547 | Val 0.370206\n",
            "Epoch  60 | Train 0.139298 | Val 0.351194\n",
            "Epoch  70 | Train 0.151239 | Val 0.371306\n",
            "Epoch  80 | Train 0.134610 | Val 0.382480\n",
            "Epoch  90 | Train 0.124395 | Val 0.349984\n",
            "Epoch 100 | Train 0.121714 | Val 0.330687\n",
            "Epoch 110 | Train 0.112501 | Val 0.331618\n",
            "Epoch 120 | Train 0.130879 | Val 0.362202\n",
            "Epoch 130 | Train 0.100073 | Val 0.327468\n",
            "Epoch 140 | Train 0.101563 | Val 0.314191\n",
            "Epoch 150 | Train 0.095120 | Val 0.331977\n",
            "Epoch 160 | Train 0.091352 | Val 0.319407\n",
            "Epoch 170 | Train 0.093235 | Val 0.317029\n",
            "Epoch 180 | Train 0.085311 | Val 0.310647\n",
            "Epoch 190 | Train 0.077866 | Val 0.352230\n",
            "Epoch 200 | Train 0.077977 | Val 0.341180\n",
            "Epoch 210 | Train 0.078647 | Val 0.320424\n",
            "Epoch 220 | Train 0.090904 | Val 0.322754\n",
            "Epoch 230 | Train 0.068961 | Val 0.323049\n",
            "Epoch 240 | Train 0.076329 | Val 0.346708\n",
            "Epoch 250 | Train 0.060647 | Val 0.320036\n",
            "Epoch 260 | Train 0.072310 | Val 0.328419\n",
            "Epoch 270 | Train 0.056819 | Val 0.325963\n",
            "Epoch 280 | Train 0.057117 | Val 0.318314\n",
            "Epoch 290 | Train 0.043453 | Val 0.325638\n",
            "Epoch 300 | Train 0.053968 | Val 0.322954\n",
            "Epoch 310 | Train 0.058546 | Val 0.325645\n",
            "Epoch 320 | Train 0.053420 | Val 0.350063\n",
            "Epoch 330 | Train 0.051152 | Val 0.324181\n",
            "Epoch 340 | Train 0.040852 | Val 0.315889\n",
            "Epoch 350 | Train 0.037744 | Val 0.318219\n",
            "Epoch 360 | Train 0.040579 | Val 0.326144\n",
            "Epoch 370 | Train 0.042521 | Val 0.331661\n",
            "Epoch 380 | Train 0.051309 | Val 0.353098\n",
            "Epoch 390 | Train 0.045453 | Val 0.330927\n",
            "Epoch 400 | Train 0.052077 | Val 0.322708\n",
            "Epoch 410 | Train 0.033947 | Val 0.317260\n",
            "Epoch 420 | Train 0.037397 | Val 0.318510\n",
            "Epoch 430 | Train 0.034864 | Val 0.341363\n",
            "Epoch 440 | Train 0.028371 | Val 0.324318\n",
            "Epoch 450 | Train 0.024618 | Val 0.315105\n",
            "Epoch 460 | Train 0.028724 | Val 0.338121\n",
            "Epoch 470 | Train 0.023611 | Val 0.317287\n",
            "Epoch 480 | Train 0.029846 | Val 0.342208\n",
            "Epoch 490 | Train 0.056377 | Val 0.361863\n",
            "Epoch 500 | Train 0.035767 | Val 0.351930\n",
            "Split 1_99 — Final Test MSE (orig scale): 4.415452e-02\n",
            "\n",
            "==== Running Split: 80_20 ====\n",
            "Split 80_20 — X_len=80 | Y_len=21\n",
            "Train: (1729, 80), Val: (433, 80), Test inputs: (541, 80)\n",
            "Training started...\n",
            "Epoch  10 | Train 0.005679 | Val 0.008522\n",
            "Epoch  20 | Train 0.002954 | Val 0.004163\n",
            "Epoch  30 | Train 0.002204 | Val 0.002901\n",
            "Epoch  40 | Train 0.003030 | Val 0.005299\n",
            "Epoch  50 | Train 0.001510 | Val 0.003376\n",
            "Epoch  60 | Train 0.001324 | Val 0.002021\n",
            "Epoch  70 | Train 0.000889 | Val 0.002109\n",
            "Epoch  80 | Train 0.000767 | Val 0.002124\n",
            "Epoch  90 | Train 0.001763 | Val 0.003013\n",
            "Epoch 100 | Train 0.002290 | Val 0.003076\n",
            "Epoch 110 | Train 0.000600 | Val 0.001161\n",
            "Epoch 120 | Train 0.000367 | Val 0.001103\n",
            "Epoch 130 | Train 0.000509 | Val 0.001242\n",
            "Epoch 140 | Train 0.000501 | Val 0.001259\n",
            "Epoch 150 | Train 0.001802 | Val 0.003716\n",
            "Epoch 160 | Train 0.000395 | Val 0.000734\n",
            "Epoch 170 | Train 0.000226 | Val 0.000680\n",
            "Epoch 180 | Train 0.000151 | Val 0.000509\n",
            "Epoch 190 | Train 0.000217 | Val 0.000639\n",
            "Epoch 200 | Train 0.000318 | Val 0.000668\n",
            "Epoch 210 | Train 0.000202 | Val 0.000741\n",
            "Epoch 220 | Train 0.000241 | Val 0.000524\n",
            "Epoch 230 | Train 0.000319 | Val 0.001099\n",
            "Epoch 240 | Train 0.000188 | Val 0.000466\n",
            "Epoch 250 | Train 0.000076 | Val 0.000469\n",
            "Epoch 260 | Train 0.006002 | Val 0.005279\n",
            "Epoch 270 | Train 0.000885 | Val 0.000759\n",
            "Epoch 280 | Train 0.000156 | Val 0.000455\n",
            "Epoch 290 | Train 0.000115 | Val 0.000399\n",
            "Epoch 300 | Train 0.000107 | Val 0.000390\n",
            "Epoch 310 | Train 0.000140 | Val 0.000413\n",
            "Epoch 320 | Train 0.000108 | Val 0.000450\n",
            "Epoch 330 | Train 0.000104 | Val 0.000393\n",
            "Epoch 340 | Train 0.001239 | Val 0.000730\n",
            "Epoch 350 | Train 0.000229 | Val 0.000593\n",
            "Epoch 360 | Train 0.000214 | Val 0.000494\n",
            "Epoch 370 | Train 0.000233 | Val 0.000621\n",
            "Epoch 380 | Train 0.000104 | Val 0.000457\n",
            "Epoch 390 | Train 0.000669 | Val 0.001077\n",
            "Epoch 400 | Train 0.000255 | Val 0.001007\n",
            "Epoch 410 | Train 0.000282 | Val 0.001827\n",
            "Epoch 420 | Train 0.000086 | Val 0.000422\n",
            "Epoch 430 | Train 0.000128 | Val 0.000410\n",
            "Epoch 440 | Train 0.000082 | Val 0.000428\n",
            "Epoch 450 | Train 0.004088 | Val 0.003037\n",
            "Epoch 460 | Train 0.000172 | Val 0.000373\n",
            "Epoch 470 | Train 0.000140 | Val 0.000335\n",
            "Epoch 480 | Train 0.000089 | Val 0.000308\n",
            "Epoch 490 | Train 0.000086 | Val 0.000318\n",
            "Epoch 500 | Train 0.000078 | Val 0.000304\n",
            "Split 80_20 — Final Test MSE (orig scale): 3.954214e-05\n",
            "\n",
            "==== Running Split: 60_40 ====\n",
            "Split 60_40 — X_len=60 | Y_len=41\n",
            "Train: (1729, 60), Val: (433, 60), Test inputs: (541, 60)\n",
            "Training started...\n",
            "Epoch  10 | Train 0.015679 | Val 0.018157\n",
            "Epoch  20 | Train 0.008723 | Val 0.013356\n",
            "Epoch  30 | Train 0.004272 | Val 0.007133\n",
            "Epoch  40 | Train 0.002993 | Val 0.005550\n",
            "Epoch  50 | Train 0.001229 | Val 0.003752\n",
            "Epoch  60 | Train 0.000789 | Val 0.002668\n",
            "Epoch  70 | Train 0.001255 | Val 0.003132\n",
            "Epoch  80 | Train 0.000811 | Val 0.002467\n",
            "Epoch  90 | Train 0.000636 | Val 0.001733\n",
            "Epoch 100 | Train 0.000533 | Val 0.001932\n",
            "Epoch 110 | Train 0.000616 | Val 0.001790\n",
            "Epoch 120 | Train 0.000469 | Val 0.001719\n",
            "Epoch 130 | Train 0.000670 | Val 0.002214\n",
            "Epoch 140 | Train 0.000924 | Val 0.001883\n",
            "Epoch 150 | Train 0.000862 | Val 0.001658\n",
            "Epoch 160 | Train 0.000664 | Val 0.001650\n",
            "Epoch 170 | Train 0.003991 | Val 0.004115\n",
            "Epoch 180 | Train 0.002778 | Val 0.003265\n",
            "Epoch 190 | Train 0.000464 | Val 0.001384\n",
            "Epoch 200 | Train 0.000360 | Val 0.001185\n",
            "Epoch 210 | Train 0.000399 | Val 0.001580\n",
            "Epoch 220 | Train 0.000451 | Val 0.001605\n",
            "Epoch 230 | Train 0.000283 | Val 0.001248\n",
            "Epoch 240 | Train 0.000352 | Val 0.001748\n",
            "Epoch 250 | Train 0.000411 | Val 0.001246\n",
            "Epoch 260 | Train 0.000303 | Val 0.001177\n",
            "Epoch 270 | Train 0.000459 | Val 0.001831\n",
            "Epoch 280 | Train 0.000333 | Val 0.001404\n",
            "Epoch 290 | Train 0.001257 | Val 0.002275\n",
            "Epoch 300 | Train 0.000423 | Val 0.001612\n",
            "Epoch 310 | Train 0.000395 | Val 0.001434\n",
            "Epoch 320 | Train 0.001588 | Val 0.003340\n",
            "Epoch 330 | Train 0.000393 | Val 0.001462\n",
            "Epoch 340 | Train 0.000323 | Val 0.001248\n",
            "Epoch 350 | Train 0.000244 | Val 0.001240\n",
            "Epoch 360 | Train 0.000836 | Val 0.001396\n",
            "Epoch 370 | Train 0.001728 | Val 0.003907\n",
            "Epoch 380 | Train 0.001480 | Val 0.002788\n",
            "Epoch 390 | Train 0.000200 | Val 0.000958\n",
            "Epoch 400 | Train 0.000180 | Val 0.000988\n",
            "Epoch 410 | Train 0.000177 | Val 0.000991\n",
            "Epoch 420 | Train 0.000393 | Val 0.001143\n",
            "Epoch 430 | Train 0.000202 | Val 0.000958\n",
            "Epoch 440 | Train 0.000346 | Val 0.001002\n",
            "Epoch 450 | Train 0.000229 | Val 0.001037\n",
            "Epoch 460 | Train 0.000160 | Val 0.000825\n",
            "Epoch 470 | Train 0.000156 | Val 0.000826\n",
            "Epoch 480 | Train 0.000517 | Val 0.001503\n",
            "Epoch 490 | Train 0.000481 | Val 0.001022\n",
            "Epoch 500 | Train 0.000258 | Val 0.000754\n",
            "Split 60_40 — Final Test MSE (orig scale): 1.228897e-04\n",
            "\n",
            "==== Running Split: 50_50 ====\n",
            "Split 50_50 — X_len=50 | Y_len=51\n",
            "Train: (1729, 50), Val: (433, 50), Test inputs: (541, 50)\n",
            "Training started...\n",
            "Epoch  10 | Train 0.027936 | Val 0.034094\n",
            "Epoch  20 | Train 0.018126 | Val 0.023553\n",
            "Epoch  30 | Train 0.009824 | Val 0.014173\n",
            "Epoch  40 | Train 0.003449 | Val 0.006572\n",
            "Epoch  50 | Train 0.002015 | Val 0.005242\n",
            "Epoch  60 | Train 0.001406 | Val 0.003807\n",
            "Epoch  70 | Train 0.001206 | Val 0.003978\n",
            "Epoch  80 | Train 0.001079 | Val 0.004148\n",
            "Epoch  90 | Train 0.001203 | Val 0.003733\n",
            "Epoch 100 | Train 0.000986 | Val 0.003431\n",
            "Epoch 110 | Train 0.001268 | Val 0.003827\n",
            "Epoch 120 | Train 0.001323 | Val 0.004732\n",
            "Epoch 130 | Train 0.001060 | Val 0.003540\n",
            "Epoch 140 | Train 0.000847 | Val 0.003717\n",
            "Epoch 150 | Train 0.000885 | Val 0.003384\n",
            "Epoch 160 | Train 0.000792 | Val 0.003608\n",
            "Epoch 170 | Train 0.000923 | Val 0.003683\n",
            "Epoch 180 | Train 0.002900 | Val 0.005128\n",
            "Epoch 190 | Train 0.000784 | Val 0.003039\n",
            "Epoch 200 | Train 0.000578 | Val 0.003035\n",
            "Epoch 210 | Train 0.000999 | Val 0.003235\n",
            "Epoch 220 | Train 0.000525 | Val 0.002897\n",
            "Epoch 230 | Train 0.000479 | Val 0.003603\n",
            "Epoch 240 | Train 0.000965 | Val 0.003310\n",
            "Epoch 250 | Train 0.000597 | Val 0.003129\n",
            "Epoch 260 | Train 0.004835 | Val 0.007051\n",
            "Epoch 270 | Train 0.003994 | Val 0.006030\n",
            "Epoch 280 | Train 0.000652 | Val 0.002481\n",
            "Epoch 290 | Train 0.000482 | Val 0.002084\n",
            "Epoch 300 | Train 0.000371 | Val 0.001888\n",
            "Epoch 310 | Train 0.000349 | Val 0.001946\n",
            "Epoch 320 | Train 0.000440 | Val 0.002116\n",
            "Epoch 330 | Train 0.000528 | Val 0.002452\n",
            "Epoch 340 | Train 0.000804 | Val 0.002195\n",
            "Epoch 350 | Train 0.000458 | Val 0.002249\n",
            "Epoch 360 | Train 0.000400 | Val 0.001799\n",
            "Epoch 370 | Train 0.000312 | Val 0.002382\n",
            "Epoch 380 | Train 0.000570 | Val 0.002131\n",
            "Epoch 390 | Train 0.000377 | Val 0.001977\n",
            "Epoch 400 | Train 0.000244 | Val 0.001754\n",
            "Epoch 410 | Train 0.000925 | Val 0.003906\n",
            "Epoch 420 | Train 0.000325 | Val 0.002117\n",
            "Epoch 430 | Train 0.000660 | Val 0.002768\n",
            "Epoch 440 | Train 0.000666 | Val 0.002322\n",
            "Epoch 450 | Train 0.000229 | Val 0.001853\n",
            "Epoch 460 | Train 0.000143 | Val 0.001925\n",
            "Epoch 470 | Train 0.000523 | Val 0.002354\n",
            "Epoch 480 | Train 0.000524 | Val 0.001944\n",
            "Epoch 490 | Train 0.000223 | Val 0.001804\n",
            "Epoch 500 | Train 0.000212 | Val 0.002141\n",
            "Split 50_50 — Final Test MSE (orig scale): 3.288808e-04\n",
            "\n",
            "==== Running Split: 40_60 ====\n",
            "Split 40_60 — X_len=40 | Y_len=61\n",
            "Train: (1729, 40), Val: (433, 40), Test inputs: (541, 40)\n",
            "Training started...\n",
            "Epoch  10 | Train 0.034063 | Val 0.040335\n",
            "Epoch  20 | Train 0.008163 | Val 0.012863\n",
            "Epoch  30 | Train 0.004748 | Val 0.011325\n",
            "Epoch  40 | Train 0.003276 | Val 0.006514\n",
            "Epoch  50 | Train 0.003432 | Val 0.006395\n",
            "Epoch  60 | Train 0.002058 | Val 0.007203\n",
            "Epoch  70 | Train 0.002383 | Val 0.008127\n",
            "Epoch  80 | Train 0.001300 | Val 0.005212\n",
            "Epoch  90 | Train 0.002641 | Val 0.006275\n",
            "Epoch 100 | Train 0.003217 | Val 0.007972\n",
            "Epoch 110 | Train 0.001108 | Val 0.005963\n",
            "Epoch 120 | Train 0.000886 | Val 0.004731\n",
            "Epoch 130 | Train 0.000864 | Val 0.004782\n",
            "Epoch 140 | Train 0.001684 | Val 0.006742\n",
            "Epoch 150 | Train 0.001944 | Val 0.007203\n",
            "Epoch 160 | Train 0.001310 | Val 0.004420\n",
            "Epoch 170 | Train 0.000786 | Val 0.005245\n",
            "Epoch 180 | Train 0.001573 | Val 0.005725\n",
            "Epoch 190 | Train 0.002069 | Val 0.005818\n",
            "Epoch 200 | Train 0.001377 | Val 0.004800\n",
            "Epoch 210 | Train 0.000614 | Val 0.004687\n",
            "Epoch 220 | Train 0.000967 | Val 0.004894\n",
            "Epoch 230 | Train 0.001368 | Val 0.006910\n",
            "Epoch 240 | Train 0.001292 | Val 0.006737\n",
            "Epoch 250 | Train 0.002564 | Val 0.004844\n",
            "Epoch 260 | Train 0.001612 | Val 0.006426\n",
            "Epoch 270 | Train 0.000937 | Val 0.004373\n",
            "Epoch 280 | Train 0.000630 | Val 0.004232\n",
            "Epoch 290 | Train 0.000520 | Val 0.004212\n",
            "Epoch 300 | Train 0.000700 | Val 0.004170\n",
            "Epoch 310 | Train 0.001063 | Val 0.004516\n",
            "Epoch 320 | Train 0.000426 | Val 0.004408\n",
            "Epoch 330 | Train 0.000476 | Val 0.004447\n",
            "Epoch 340 | Train 0.000617 | Val 0.004273\n",
            "Epoch 350 | Train 0.001166 | Val 0.004946\n",
            "Epoch 360 | Train 0.000456 | Val 0.004479\n",
            "Epoch 370 | Train 0.000882 | Val 0.004678\n",
            "Epoch 380 | Train 0.002245 | Val 0.007027\n",
            "Epoch 390 | Train 0.000506 | Val 0.004426\n",
            "Epoch 400 | Train 0.000643 | Val 0.004178\n",
            "Epoch 410 | Train 0.000496 | Val 0.004211\n",
            "Epoch 420 | Train 0.000374 | Val 0.004226\n",
            "Epoch 430 | Train 0.000318 | Val 0.004273\n",
            "Epoch 440 | Train 0.000617 | Val 0.004083\n",
            "Epoch 450 | Train 0.000469 | Val 0.004653\n",
            "Epoch 460 | Train 0.000993 | Val 0.004260\n",
            "Epoch 470 | Train 0.002191 | Val 0.006175\n",
            "Epoch 480 | Train 0.000347 | Val 0.003803\n",
            "Epoch 490 | Train 0.000452 | Val 0.005387\n",
            "Epoch 500 | Train 0.000864 | Val 0.004296\n",
            "Split 40_60 — Final Test MSE (orig scale): 9.031050e-04\n",
            "\n",
            "All splits complete:\n",
            "   Split  Test_MSE\n",
            "0  20_80  0.001871\n",
            "1  10_90  0.003754\n",
            "2   5_95  0.010763\n",
            "3   3_97  0.011267\n",
            "4   1_99  0.044155\n",
            "5  80_20  0.000040\n",
            "6  60_40  0.000123\n",
            "7  50_50  0.000329\n",
            "8  40_60  0.000903\n"
          ]
        }
      ]
    }
  ]
}